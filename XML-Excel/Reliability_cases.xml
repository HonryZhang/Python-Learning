<?xml version="1.0" encoding="UTF-8"?>
<data>
<testsuite id="" name="ThunderBD 可靠性测试" >
<node_order><![CDATA[]]></node_order>
<details><![CDATA[]]></details>
<testsuite id="3" name="网络故障注入" >
<node_order><![CDATA[1]]></node_order>
<details><![CDATA[<p>
	网络故障注入测试用例，分为静态测试，和动态测试。静态测试无IO下发，检查ceph集群的状态，动态测试有IO下发，检查ceph集群的状态，并检查数据一致性</p>
]]></details>

<testcase internalid="9" name="客户端与Public Switch间物理闪断">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[1]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动快速拔插客户端与Public Switch间物理链路，5秒内插回；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路恢复，io归零在30秒内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	总共重复3次操作。</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="19" name="服务器前端与Public Switch间物理闪断">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[3]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动快速拔插服务器前端与Public Switch间物理链路，5秒内插回；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，数据一致性正常，链路插回后正常，io归零30秒以内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="25" name="服务器后端与Cluster Switch间物理闪断">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[4]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动快速拔插服务器后端与Cluster Switch间物理链路，5秒内插回；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，数据一致性正常，链路插回后正常，io归零30秒以内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="32" name="主Monitor与Public Switch间物理闪断">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[5]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动快速拔插主Monitor与Public Switch间物理链路，5秒内插回；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，数据一致性正常，链路插回后正常，io归零在30秒内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="38" name="非主Monitor与Public Switch间物理闪断">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[6]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动快速拔插非主Monitor与Public Switch间物理链路，5秒内插回；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，数据一致性正常，链路插回后正常，io归零在30秒内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="44" name="客户端与Public Switch间逻辑闪断">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[7]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ifdown/ifup模拟客户端与Public Switch间逻辑链路闪断，间隔5s；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，数据一致性正常，链路正常，io归零在30秒以内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="50" name="服务器前端与Public Switch间逻辑闪断">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[8]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ifdown/ifup模拟服务器前端与Public Switch间逻辑链路闪断，间隔5s；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	&nbsp;</div>
<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，数据一致性正常，链路正常，io归零在30秒内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="56" name="服务器后端与Cluster Switch间逻辑闪断">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[9]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ifdown/ifup模拟服务器后端与Cluster Switch间逻辑链路闪断，间隔5s；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，数据一致性正常，链路正常，io归零在30秒内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="62" name="主Monitor与Public Switch间逻辑闪断">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[10]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ifdown/ifup模拟主Monitor与Public Switch间逻辑链路闪断，间隔5s；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，数据一致性正常，链路正常，io归零在30秒内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="68" name="非主Monitor与Public Switch间逻辑闪断">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[11]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ifdown/ifup模拟非主Monitor与Public Switch间逻辑链路闪断，间隔5s；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，数据一致性正常，链路正常，io归零在30秒内</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="74" name="服务器前端与Public Switch间长时间物理断开">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[12]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动断开服务器前端与Public Switch间物理链路，60秒后插回；查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	线缆拔出后，io归零30秒以内，osd进程没有异常退出，插回后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

	<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	链路插回后，看到有PG的迁移，在迁移过程中，将链路长时间中断，60秒后插回，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	线缆拔出后，io归零30秒以内，osd进程没有异常退出，插回后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	链路插回后，待PG迁移完成后，将链路长时间中断，400秒后插回，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	线缆拔出后，io归零30秒以内，osd进程没有异常退出，插回后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	重复步骤2到步骤4，做3遍</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="81" name="服务器后端与Cluster Switch间长时间物理断开">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[13]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动断开服务器后端与Cluster Switch间物理链路，60秒后插回；查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	线缆拔出后，io归零30秒以内，osd进程没有异常退出，插回后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	链路插回后，看到有PG的迁移，在迁移过程中，将链路长时间中断，60秒后插回，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	线缆拔出后，io归零30秒以内，osd进程没有异常退出，插回后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	链路插回后，待PG迁移完成后，将链路长时间中断，400秒后插回，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	线缆拔出后，io归零30秒以内，osd进程没有异常退出，插回后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	重复步骤2到步骤4，做3遍</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="89" name="主Monitor与Public Switch间长时间物理断开">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[14]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动断开主Monitor与Public Switch间物理链路，60秒后插回；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="95" name="非主Monitor与Public Switch间长时间物理断开">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[15]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动断开非主Monitor与Public Switch间物理链路，60秒后插回；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="101" name="服务器前端与Public Switch间长时间逻辑断开">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[16]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	ifdown端口断开服务器前端与Public Switch间物理链路，60秒后ifup端口；查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口down后，io归零30秒以内，osd进程没有异常退出，端口up后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	ifup端口后，看到有PG的迁移，在迁移过程中，ifdown端口，60秒后ifup端口，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口down后，io归零30秒以内，osd进程没有异常退出，端口up后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	ifup端口后，待PG迁移完成后，ifdown端口，400秒后ifup端口，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口down后，io归零30秒以内，osd进程没有异常退出，端口up后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	重复步骤2到步骤4，做3遍</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="108" name="服务器后端与Cluster Switch间长时间逻辑断开">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[17]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	ifdown端口断开服务器前端与Cluster Switch间物理链路，60秒后ifup端口；查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口down后，io归零30秒以内，osd进程没有异常退出，端口up后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	ifup端口后，看到有PG的迁移，在迁移过程中，ifdown端口，60秒后ifup端口，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口down后，io归零30秒以内，osd进程没有异常退出，端口up后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	ifup端口后，待PG迁移完成后，ifdown端口，400秒后ifup端口，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口down后，io归零30秒以内，osd进程没有异常退出，端口up后，连接恢复，集群状态恢复为OK，io读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	重复步骤2到步骤4，做3遍</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="117" name="主Monitor与Public Switch间长时间逻辑断开">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[18]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ifdown/ifup模拟主Monitor与Public Switch间逻辑链路断开，间隔60s；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="123" name="非主Monitor与Public Switch间长时间逻辑断开">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[19]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ifdown/ifup模拟非主Monitor与Public Switch间逻辑链路断开，间隔60s；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="129" name="客户端MTU设置9000">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[20]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在客户端设置MTU为9000；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态。</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常，IO性能有波动。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="134" name="客户端MTU设置1200">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[21]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在客户端设置MTU为1200；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态。</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常，IO性能有波动。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="139" name="OSD节点MTU设置9000">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[22]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在所有OSD节点Public网络设置MTU为9000；</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常，IO性能有波动。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	在所有OSD节点Cluster网络设置MTU为9000</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常，IO性能有波动。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<div>
	将所有OSD的public网络和cluster网络都设置成MTU 1500</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="147" name="OSD节点MTU设置1200">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[23]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在所有OSD节点Public网络设置MTU为1200；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常，IO性能有波动。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	在所有OSD节点Cluster网络设置MTU为1200</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常，IO性能有波动。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<div>
	将所有OSD的public网络和cluster网络都设置成MTU 1500</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常，IO性能有波动。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="156" name="在单Monitor情况下修改Monitor IP">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[24]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在只有一个Mon情况下，修改Mon的Public IP，5分钟后改回原来的IP；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对第二个image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态。</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改IP后，IO归零时间在30秒内，集群读写不出现数据不一致；IP改回之后，集群状态可以恢复正常，且IO可进行正常读写，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="162" name="在多Monitor情况下修改leader Monitor IP">
	<node_order><![CDATA[23]]></node_order>
	<externalid><![CDATA[25]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在有三个Mon情况下，修改主Mon的Public IP，5分钟后改回原来的IP；（包含两种情况，修改成同一网段ip，修改成不同网段的ip）</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对第二个image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态。</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改IP后，IO归零时间在30秒内，集群读写不出现数据不一致；IP改回之后，集群状态可以恢复正常，且IO可进行正常读写，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="227" name="在多Monitor情况下修改非leader Monitor IP">
	<node_order><![CDATA[24]]></node_order>
	<externalid><![CDATA[35]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在有三个Mon情况下，修改非leader mon Public IP，5分钟后改回原来的IP；（包含两种情况，修改成同一网段ip，修改成不同网段的ip）</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对第二个image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态。</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改IP后，IO归零时间在30秒内，集群读写不出现数据不一致；IP改回之后，集群状态可以恢复正常，且IO可进行正常读写，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="168" name="修改OSD IP">
	<node_order><![CDATA[25]]></node_order>
	<externalid><![CDATA[26]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	修改一个OSD节点的Public IP，5分钟后改回原来的IP；（包含两种情况，修改成同一网段ip，修改成不同网段的ip）</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改IP后，IO归零时间在30秒内，集群读写不出现数据不一致；IP改回之后，集群状态可以恢复正常，且IO可进行正常读写，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	修改一个OSD节点的Cluster IP，5分钟后改回原来的ip；（包含两种情况，修改成同一网段ip，修改成不同网段的ip）</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	看集群状态，IO状态，链路状态。</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改IP后，IO归零时间在30秒内，集群读写不出现数据不一致；IP改回之后，集群状态可以恢复正常，且IO可进行正常读写，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="175" name="主Monitor与Public Switch间物理闪断（mon与osd共存）">
	<node_order><![CDATA[26]]></node_order>
	<externalid><![CDATA[27]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动快速拔插主Monitor与Public Switch间物理链路，5秒内插回；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="181" name="非主Monitor与Public Switch间物理闪断（mon与osd共存）">
	<node_order><![CDATA[27]]></node_order>
	<externalid><![CDATA[28]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动快速拔插非主Monitor与Public Switch间物理链路，5秒内插回；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="187" name="主Monitor与Public Switch间逻辑闪断（mon与osd共存）">
	<node_order><![CDATA[28]]></node_order>
	<externalid><![CDATA[29]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ifdown/ifup模拟主Monitor与Public Switch间逻辑链路闪断，间隔5s；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="193" name="非主Monitor与Public Switch间逻辑闪断（mon与osd共存）">
	<node_order><![CDATA[29]]></node_order>
	<externalid><![CDATA[30]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	<div>
		通过ifdown/ifup模拟主Monitor与Public Switch间逻辑链路闪断，间隔5s；</div>
	<div>
		&nbsp;</div>
</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据一致性正常，链路正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	总共重复3次操作。</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="199" name="主Monitor与Public Switch间长时间物理断开（mon与osd共存）">
	<node_order><![CDATA[30]]></node_order>
	<externalid><![CDATA[31]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动断开服务器前端与Public Switch间物理链路，60秒后插回；查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	将线缆拔出后，IO归零时间在30秒内；集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	链路插回后，看到有PG的迁移，在迁移过程中，将链路长时间中断，60秒后插回，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	将线缆拔出后，IO归零时间在30秒内；集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	链路插回后，待PG迁移完成后，将链路长时间中断，400秒后插回，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	将线缆拔出后，IO归零时间在30秒内；集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	重复步骤2到步骤4，做3遍</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="206" name="非主Monitor与Public Switch间长时间物理断开（mon与osd共存）">
	<node_order><![CDATA[31]]></node_order>
	<externalid><![CDATA[32]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	手动断开服务器前端与Public Switch间物理链路，60秒后插回；查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	将线缆拔出后，IO归零时间在30秒内，集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	链路插回后，看到有PG的迁移，在迁移过程中，将链路长时间中断，60秒后插回，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	将线缆拔出后，IO归零时间在30秒内，集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	链路插回后，待PG迁移完成后，将链路长时间中断，400秒后插回，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	将线缆拔出后，IO归零时间在30秒内，集群状态正常，IO读写正常，数据一致性正常，链路插回后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	重复步骤2到步骤4，做3遍</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="213" name="主Monitor与Public Switch间长时间逻辑断开（mon与osd共存）">
	<node_order><![CDATA[32]]></node_order>
	<externalid><![CDATA[33]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	ifdown断开服务器前端与Public Switch间物理链路，60秒后ifup；查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	端口ifdown后，IO归零时间在30秒内，集群状态正常，IO读写正常，数据一致性正常，端口ifup后正常。</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	端口ifup后，看到有PG的迁移，在迁移过程中，将端口ipdown，60秒后ifup，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口ifdown后，IO归零时间在30秒内，集群状态正常，IO读写正常，数据一致性正常，端口ifup后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	端口ifup后，待PG迁移完成后，将端口ifdown，400秒后端口ifup，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口ifdown后，IO归零时间在30秒内，集群状态正常，IO读写正常，数据一致性正常，端口ifup后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	重复步骤2到步骤4，做3遍</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="220" name="非主Monitor与Public Switch间长时间逻辑断开（mon与osd共存）">
	<node_order><![CDATA[33]]></node_order>
	<externalid><![CDATA[34]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	ifdown端口断开服务器前端与Public Switch间物理链路，60秒后端口ifup；查看集群状态，IO状态，链路状态；</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	端口ifdown后，IO归零时间在30秒内，集群状态正常，IO读写正常，数据一致性正常，端口ifup后正常。</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	端口ifup后，看到有PG的迁移，在迁移过程中，端口ifdown中断，60秒后ifup，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口ifdown后，IO归零时间在30秒内，集群状态正常，IO读写正常，数据一致性正常，端口ifup后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	端口ifup后，待PG迁移完成后，将端口ifdown中断，400秒后ifup，查看集群状态，IO状态，链路状态。</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	端口ifdown后，IO归零时间在30秒内，集群状态正常，IO读写正常，数据一致性正常，端口ifup后正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	重复步骤2到步骤4，做3遍</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="233" name="在多Monitor情况下修改leader Monitor IP（mon与osd共存）">
	<node_order><![CDATA[34]]></node_order>
	<externalid><![CDATA[36]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在有三个Mon情况下，修改主Mon的Public IP，5分钟后改回原来的IP；（包含两种情况，修改成同一网段ip，修改成不同网段的ip）</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒以内</p>
<p>
	修改成同网段的ip，osd进程会正常退出，不是异常退出</p>
<p>
	修改成不同网段的ip，osd进程不退出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对第二个image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群仲裁成功后，fio下发io正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	5分钟后，修改回原来的ip，启动osd进程，查看集群状态，IO状态，链路状态。</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio读写正常，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="239" name="在多Monitor情况下修改非leader Monitor IP（mon与osd共存）">
	<node_order><![CDATA[35]]></node_order>
	<externalid><![CDATA[37]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，同时node1，node2，node3也为mon节点，3个mon的仲裁，leader mon位于node3节点上</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在有三个Mon情况下，修改非leader mon Public IP，5分钟后改回原来的IP；（包含两种情况，修改成同一网段ip，修改成不同网段的ip）</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒以内</p>
<p>
	修改成同网段的ip，osd进程会正常退出，不是异常退出</p>
<p>
	修改成不同网段的ip，osd进程不退出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对第二个image进行随机写，带一致性校验；</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群形成新的仲裁，leader mon不变，fio下发io成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	5分钟后，修改回原来的ip，启动osd进程，查看集群状态，IO状态，链路状态。</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio读写正常，数据一致性正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="596" name="OSD public 网络全部物理中断">
	<node_order><![CDATA[36]]></node_order>
	<externalid><![CDATA[103]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node4为mon主机，创建了pool，双副本</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建rbd，启动fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，fio读写无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2，node3的public网络全部中断，等待20分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	15分钟后集群有告警，fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2，node3的public网络恢复，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群恢复为OK状态，fio继续下发io，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="601" name="OSD cluster 网络全部物理中断">
	<node_order><![CDATA[37]]></node_order>
	<externalid><![CDATA[104]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	<span style="font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node4为mon主机，创建了pool，双副本</span></p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建rbd，启动fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，fio读写无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2，node3的cluster网络中断，等待20分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群有告警，fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	恢复node1，node2，node3的cluster网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群恢复为OK状态，fio集群下发，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="606" name="三个mon网络全部物理中断">
	<node_order><![CDATA[38]]></node_order>
	<externalid><![CDATA[105]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3主机为osd主机，node4，node5，node6为mon主机，集群状态正常，创建了pool，双副本</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建rbd，启动fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，fio读写无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node4，node5，node6的public网络全部中断，查看集群状态，对另外一个rbd启动读写，观察io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态无法查看，新的fio无法下发io，原来的fio读写不受影响</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	等待5分钟后，将node4，node5，node6的public网络恢复，查看集群状态，对另外一个rbd启动读写，观察io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群恢复为OK，新的fio可以下发io，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="611" name="OSD public 网络全部逻辑中断">
	<node_order><![CDATA[39]]></node_order>
	<externalid><![CDATA[106]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	<span style="font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node4为mon主机，创建了pool，双副本</span></p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建rbd，启动fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，fio读写无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2，node3的public网络，用命令ifconfig down中断网口，等待20分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	15分钟后，集群有告警，fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2，node3的public网络，用命令ifconfig up恢复，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群恢复为OK状态，fio继续下发，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="616" name="OSD cluster 网络全部逻辑中断">
	<node_order><![CDATA[40]]></node_order>
	<externalid><![CDATA[107]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	<span style="font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node4为mon主机，创建了pool，双副本</span></p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建rbd，启动fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，fio读写无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2，node3的cluster网络，用命令idconfig down网口中断，等待20分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群有告警，fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2，node3的cluster网络，用命令ifconfig up网口恢复，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群恢复为OK状态，fio继续下发，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="621" name="三个mon网络全部逻辑中断">
	<node_order><![CDATA[41]]></node_order>
	<externalid><![CDATA[108]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	<span style="font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">node1，node2，node3主机为osd主机，node4，node5，node6为mon主机，集群状态正常，创建了pool，双副本</span></p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建rbd，启动fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建rbd成功，fio读写无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node4，node5，node6的public网络用命令ifconfig down网口中断，观察集群状态，对另外一个rbd启动读写，观察io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不能查看状态，新的fio读写不能下发，原来的fio读写不受影响</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	等待5分钟后，用命令ifconfig up网口，恢复node4，node5，node6的网络，观察集群状态，对另外一个rbd启动fio读写，观察io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态为OK，新的fio可以下发，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="626" name="public网络全部物理中断（mon与osd共存）">
	<node_order><![CDATA[42]]></node_order>
	<externalid><![CDATA[109]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，也是mon主机，3个mon，集群状态为OK，创建了rbd pool，双副本</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建rbd，启动fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，fio读写无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2，node3的public网络网线全部拔出，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态无法查看，fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，恢复node1，node2，node3的网络连接，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态为OK，fio继续下发，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="5" name="服务故障注入" >
<node_order><![CDATA[2]]></node_order>
<details><![CDATA[<p>
	主要是osd服务，mon服务进行故障注入，检查集群状态和IO一致性。</p>
]]></details>

<testcase internalid="246" name="osd脱离、加入集群（IO）">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[38]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程从集群脱离，观察io写</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.x</div>
<div>
	&nbsp; &nbsp;ceph -w</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd进程脱离集群时，io写没有一致性问题</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	该osd服务下的PG被迁出后，将osd服务停止，观察io写</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=x或者kill pid</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
<p>
	ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	将osd服务重新开启，观察io写</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或者ceph-osd -i osdid</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	将osd进程加入集群，平衡数据，观察io写</div>
<div>
	&nbsp; &nbsp;ceph osd in osd.x</div>
<div>
	&nbsp; &nbsp;ceph -w</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd进程加入集群时，io写没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<div>
	数据平衡完成后，检查集群状态为OK</div>
<div>
	&nbsp; &nbsp;ceph -s</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd加入集群后，状态为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7948" name="osd out/in 交替">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[538]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	启动fio对image进行读写带一致性校验，在读写过程中，将node1上的osd.0进行out</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd.0 out过程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	15秒后，在osd.0还在out的过程中，将osd.0 in</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd.0 in过程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	15秒后，在osd.0还在in的过程中，将osd.0 out</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd.0 out过程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2,3进行5遍</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复OK后，在node2，node3上针对某一个osd，重复步骤1,2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="254" name="单节点关闭一个osd进程（IO）">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[39]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程关闭，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=x或kill osdid</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭一个osd进程后，io写没有一致性问题</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	将osd服务重新开启，观察io写，观察集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i osdid</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	重新开启osd服务后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="259" name="单节点kill一个osd进程（IO）">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[40]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程kill，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	kill一个osd进程后，io写没有一致性问题</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	将osd服务重新开启，观察io写，观察集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i osdid</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	重新开启osd服务后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="264" name="单节点关闭两个osd进程（IO）">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[41]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2，副本按照host分布</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程关闭，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=x或kill osdpid</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭一个osd进程后，io写没有一致性问题</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	将node1上另外一个osd进程关闭，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=y或kill osdpid</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭第二个osd进程后，is写没有一致性问题</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	将两个osd进程重新开启，观察写io，集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i x</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=y或ceph-osd -i y</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	重新开启两个osd服务后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="270" name="单节点kill两个osd进程（IO）">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[42]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点,将一个osd进程kill，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	kill一个osd进程后，io写没有一致性问题</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	将node1上另外一个osd进程kill，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	kill第二个osd进程后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	将两个osd进程重新开启，观察写io，集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i x</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=y或ceph-osd -i y</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	重新开启两个osd服务后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="276" name="单节点关闭三个osd进程（IO）">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[43]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程关闭，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=x或kill osdpid</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭一个osd进程后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	将node1上第二个osd进程关闭，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=y或kill osdpid</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭第二个osd进程后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	将node1上第三个osd进程关闭，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=z或kill osdpid</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭第三个osd进程后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	将三个osd进程重新开启，观察写io，集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i x</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=y或ceph-osd -i y</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=z或ceph-osd -i z</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	重新开启三个osd服务后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="283" name="单节点kill三个osd进程（IO）">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[44]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程kill，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	kill一个osd进程后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	将node1上第二个osd进程kill，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	kill第二个osd进程后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	将node1上第三个osd进程kill，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	kill第三个osd进程后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	将两个osd进程重新开启，观察写io，集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i x</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=y或ceph-osd -i y</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=z或ceph-osd -i z</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	重新开启三个osd服务后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="290" name="两个节点都关闭一个osd进程（IO）">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[45]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程关闭，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=x或kill osdpid</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node1节点osd进程关闭后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	集群状态恢复为OK后，登录node2节点，将一个osd进程关闭，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=y或kill osdpid</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node2节点osd进程关闭后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	在两个节点上，分别将osd进程启动，观察写io，集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i x</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=y或ceph-osd -i y</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	两个osd进程重新开启后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="296" name="两个节点都kill一个osd进程（IO）">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[46]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程kill，然后out出集群，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
<div>
	&nbsp; &nbsp;ceph osd out x</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node1节点osd进程kill后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	集群恢复为OK后，登录node2节点，将一个osd进程kill，然后将osd out出集群，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
<div>
	&nbsp; &nbsp;ceph osd out y</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node2节点osd进程kill后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	在两个节点上，分别将osd进程启动，并加入集群，观察写io，集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i x</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=y或ceph-osd -i y</div>
<div>
	&nbsp; &nbsp;ceph osd in osd.x</div>
<div>
	&nbsp; &nbsp;ceph osd in osd.y</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	两个osd进程重新开启后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="377" name="三个节点都关闭一个osd进程">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[58]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程关闭，并out出集群，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=x或kill osdpid</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.x</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node1节点osd进程关闭后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	待集群恢复为OK状态后，登录node2节点，将一个osd进程关闭，并out出集群，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=y或kill osdpid</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.y</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node2节点osd进程关闭后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	待集群恢复为OK状态后，登录node3节点，将一个osd进程关闭，并out出集群，观察写io</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=z或kill osdpid</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.z</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node3节点osd进程关闭后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	&nbsp;</div>
<div>
	在三个节点上，分别将osd进程启动，加入集群，观察写io，集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i x</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=y或ceph-osd -i y</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=z或ceph-osd -i z</div>
<div>
	&nbsp; &nbsp;ceph osd in osd.x</div>
<div>
	&nbsp; &nbsp;ceph osd in osd.y</div>
<div>
	&nbsp; &nbsp;ceph osd in osd.z</div>
<div>
	&nbsp; &nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	三个osd进程重新开启加入集群后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="384" name="三个节点都kill一个osd进程">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[59]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将一个osd进程kill，并out出集群，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 osdpid</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.x</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node1节点osd进程kill后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	待集群恢复为OK后，登录node2节点，将一个osd进程kill，并out出集群，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 osdpid</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.y</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node2节点osd进程kill后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	待集群恢复为OK后，登录node3节点，将一个osd进程kill，并out出集群，观察写io</div>
<div>
	&nbsp; &nbsp;kill -9 osdpid</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.z</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node3节点osd进程kill后，io写没有一致性问题</div>
<div>
	&nbsp;</div>
<div>
	<p>
		osd服务停止后，该osd进程的内存被释放，查看osd进程占用的内存：</p>
	<p>
		ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep ceph</p>
</div>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	在三个节点上，分别将osd进程启动，加入集群，观察写io，集群状态</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=x或ceph-osd -i x</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=y或ceph-osd -i y</div>
<div>
	&nbsp; &nbsp;start ceph-osd id=z或ceph-osd -i z</div>
<div>
	&nbsp; &nbsp;ceph osd in osd.x</div>
<div>
	&nbsp; &nbsp;ceph osd in osd.y</div>
<div>
	&nbsp; &nbsp;ceph osd in osd.z</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	三个osd进程重新开启并加入集群后，io写没有一致性问题，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="302" name="在node1节点上添加一个osd进程（IO)">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[47]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点有3个osd服务进程，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，创建osd进程，使用磁盘/dev/nvme1n1,osd id为9</div>
<div>
	对磁盘分区</div>
<div>
	&nbsp; &nbsp;parted -s -a optimal /dev/nvme1n1 mkpart head-reverse-part 0M 100M</div>
<div>
	&nbsp; &nbsp;parted -s -a optimal /dev/nvme1n1 mkpart osd-device-9-wal 1G 3G</div>
<div>
	&nbsp; &nbsp;parted -s -a optimal /dev/nvme1n1 mkpart osd-device-9-db 3G 11G</div>
<div>
	&nbsp; &nbsp;parted -s -a optimal /dev/nvme1n1 mkpart osd-device-9-block 12G 650G</div>
<div>
	创建osd目录</div>
<div>
	&nbsp; &nbsp;mkdir -p /var/lib/ceph/osd/osd-device-9-data</div>
<div>
	修改配置文件/etc/ceph/ceph.conf，添加如下内容</div>
<div>
	&nbsp; &nbsp;[osd.9]</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; host = node1</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; osd data = /var/lib/ceph/osd/osd-device-9-data</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; bluestore block wal path = /dev/disk/by-partlabel/osd-device-9-wal</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; bluestore block db path = /dev/disk/by-partlabel/osd-device-9-db</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; bluestore block path = /dev/disk/by-partlabel/osd-device-9-block</div>
<div>
	同步配置文件到其他集群节点</div>
<div>
	&nbsp; &nbsp; scp /etc/ceph/ceph.conf &nbsp;node2:/etc/ceph</div>
<div>
	&nbsp; &nbsp; scp /etc/ceph/ceph.conf &nbsp;node3:/etc/ceph</div>
<div>
	&nbsp; &nbsp; scp /etc/ceph/ceph.conf &nbsp;node4:/etc/ceph</div>
<div>
	创建osd服务进程</div>
<div>
	&nbsp; &nbsp; ceph-osd -i 9 --mkfs --mkkey</div>
<div>
	&nbsp; &nbsp; ceph auth add osd.9 osd &#39;allow *&#39; mon &#39;allow profile osd&#39; -i /var/lib/ceph/osd/osd-device-9-data/keyring</div>
<div>
	&nbsp; &nbsp; ceph osd crush add osd.9 1.0 host=node1</div>
<div>
	&nbsp; &nbsp; touch /var/lib/ceph/osd/osd-device-9-data/upstart</div>
<div>
	&nbsp; &nbsp; initctl emit ceph-osd cluster=ceph id=9</div>
<div>
	查看osd tree</div>
<div>
	&nbsp; &nbsp; ceph osd tree</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	在node1上添加osd进程成功,在添加过程中io写没有一致性问题</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，查看io写状态</div>
<div>
	&nbsp; &nbsp; ceph -w</div>
<div>
	&nbsp; &nbsp; ceph -s</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd加入集群后，数据平衡后，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="307" name="在新节点node5上创建一个osd进程（IO）">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[48]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p3.PNG" style="width: 494px; height: 268px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，node2和node3 osd节点有3个osd服务进程，node1 osd节点有4个osd进程</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
<div>
	4，node5主机节点为新节点，已经安装ceph软件，见组网图二</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node5节点，创建osd进程，使用磁盘/dev/nvme1n1,osd id为10</div>
<div>
	对磁盘分区</div>
<div>
	&nbsp; &nbsp;parted -s -a optimal /dev/nvme1n1 mkpart head-reverse-part 0M 100M</div>
<div>
	&nbsp; &nbsp;parted -s -a optimal /dev/nvme1n1 mkpart osd-device-10-wal 1G 3G</div>
<div>
	&nbsp; &nbsp;parted -s -a optimal /dev/nvme1n1 mkpart osd-device-10-db 3G 11G</div>
<div>
	&nbsp; &nbsp;parted -s -a optimal /dev/nvme1n1 mkpart osd-device-10-block 12G 650G</div>
<div>
	创建osd目录</div>
<div>
	&nbsp; &nbsp;mkdir -p /var/lib/ceph/osd/osd-device-10-data</div>
<div>
	在node1节点上修改配置文件/etc/ceph/ceph.conf，添加如下内容</div>
<div>
	&nbsp; &nbsp;[osd.10]</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; host = node5</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; osd data = /var/lib/ceph/osd/osd-device-10-data</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; bluestore block wal path = /dev/disk/by-partlabel/osd-device-10-wal</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; bluestore block db path = /dev/disk/by-partlabel/osd-device-10-db</div>
<div>
	&nbsp; &nbsp; &nbsp; &nbsp; bluestore block path = /dev/disk/by-partlabel/osd-device-10-block</div>
<div>
	在node1节点上同步配置文件到其他集群节点</div>
<div>
	&nbsp; &nbsp; scp /etc/ceph/ceph.conf &nbsp;node2:/etc/ceph</div>
<div>
	&nbsp; &nbsp; scp /etc/ceph/ceph.conf &nbsp;node3:/etc/ceph</div>
<div>
	&nbsp; &nbsp; scp /etc/ceph/ceph.conf &nbsp;node4:/etc/ceph</div>
<div>
	&nbsp; &nbsp; scp /etc/ceph/ceph.conf &nbsp;node5:/etc/ceph</div>
<div>
	在node5节点上创建osd服务进程</div>
<div>
	&nbsp; &nbsp; ceph-osd -i 10 --mkfs --mkkey</div>
<div>
	&nbsp; &nbsp; ceph auth add osd.10 osd &#39;allow *&#39; mon &#39;allow profile osd&#39; -i /var/lib/ceph/osd/osd-device-10-data/keyring</div>
<div>
	&nbsp; &nbsp; ceph osd crush add-bucket node5 host</div>
<div>
	&nbsp; &nbsp; ceph osd crush move node5 root=default</div>
<div>
	&nbsp; &nbsp; ceph osd crush add osd.10 1.0 host=node1</div>
<div>
	&nbsp; &nbsp; touch /var/lib/ceph/osd/osd-device-10-data/upstart</div>
<div>
	&nbsp; &nbsp; initctl emit ceph-osd cluster=ceph id=10</div>
<div>
	查看osd tree</div>
<div>
	&nbsp; &nbsp; ceph osd tree</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	在node5上添加osd进程成功,在添加过程中io写没有一致性问题</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，查看io写状态</div>
<div>
	&nbsp; &nbsp; ceph -w</div>
<div>
	&nbsp; &nbsp; ceph -s</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd加入集群后，数据平衡后，集群状态恢复为OK</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="312" name="在node1节点删除第一个osd进程（IO)">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[49]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p3.PNG" style="width: 494px; height: 268px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5一共5个主机节点，node4为mon节点，其余为osd节点，node2和node3 osd节点有3个osd服务进程，node1 osd节点有4个osd进程，node5 osd节点有一个osd进程，见组网图二</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1主机节点，删除osd进程，id为x</div>
<div>
	&nbsp; &nbsp;将osd out集群</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.x</div>
<div>
	&nbsp; &nbsp;ceph -w</div>
<div>
	&nbsp; &nbsp;ceph -s</div>
<div>
	&nbsp; &nbsp;停止osd x服务</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=x</div>
<div>
	&nbsp; &nbsp;initctl list | grep ceph</div>
<div>
	&nbsp; &nbsp;从crush中删除osd.x</div>
<div>
	&nbsp; &nbsp;ceph osd crush rm osd.x</div>
<div>
	&nbsp; &nbsp;ceph auth del osd.x</div>
<div>
	&nbsp; &nbsp;ceph osd rm osd.x</div>
<div>
	&nbsp; &nbsp;查看osd tree</div>
<div>
	&nbsp; &nbsp;ceph osd tree</div>
<div>
	&nbsp; &nbsp;ceph -s</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io状态</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd.x out集群时，数据迁移正常，FIO写一致性没有问题；从crush中删除osd.x正常，osd tree更新正常；删除osd.x后，集群状态正常，FIO写一致性没有问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="317" name="在node1节点删除第二个osd进程（IO)">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[50]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p3.PNG" style="width: 494px; height: 268px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5一共5个主机节点，node4为mon节点，其余为osd节点，node1、node2、node3 osd节点有3个osd服务进程，node5 osd节点有一个osd进程，见组网图二</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1主机节点，删除第二个osd进程，id为y</div>
<div>
	&nbsp; &nbsp;将osd out集群</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.y</div>
<div>
	&nbsp; &nbsp;ceph -w</div>
<div>
	&nbsp; &nbsp;ceph -s</div>
<div>
	&nbsp; &nbsp;停止osd y服务</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=y</div>
<div>
	&nbsp; &nbsp;initctl list | grep ceph</div>
<div>
	&nbsp; &nbsp;从crush中删除osd.y</div>
<div>
	&nbsp; &nbsp;ceph osd crush rm osd.y</div>
<div>
	&nbsp; &nbsp;ceph auth del osd.y</div>
<div>
	&nbsp; &nbsp;ceph osd rm osd.y</div>
<div>
	&nbsp; &nbsp;查看osd tree</div>
<div>
	&nbsp; &nbsp;ceph osd tree</div>
<div>
	&nbsp; &nbsp;ceph -s</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io状态</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd.y out集群时，数据迁移正常，FIO写一致性没有问题；从crush中删除osd.y正常，osd tree更新正常；删除osd.y后，集群状态正常，FIO写一致性没有问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="322" name="在node1节点删除第三个osd进程（IO)">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[51]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p3.PNG" style="width: 494px; height: 268px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5一共5个主机节点，node4为mon节点，其余为osd节点，node2、node3 osd节点有3个osd服务进程，node1 osd节点有两个osd服务进程，node5 osd节点有一个osd进程，见组网图二</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1主机节点，删除第三个osd进程，id为z</div>
<div>
	&nbsp; &nbsp;将osd out集群</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.z</div>
<div>
	&nbsp; &nbsp;ceph -w</div>
<div>
	&nbsp; &nbsp;ceph -s</div>
<div>
	&nbsp; &nbsp;停止osd y服务</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=z</div>
<div>
	&nbsp; &nbsp;initctl list | grep ceph</div>
<div>
	&nbsp; &nbsp;从crush中删除osd.z</div>
<div>
	&nbsp; &nbsp;ceph osd crush rm osd.z</div>
<div>
	&nbsp; &nbsp;ceph auth del osd.z</div>
<div>
	&nbsp; &nbsp;ceph osd rm osd.z</div>
<div>
	&nbsp; &nbsp;查看osd tree</div>
<div>
	&nbsp; &nbsp;ceph osd tree</div>
<div>
	&nbsp; &nbsp;ceph -s</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io状态</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd.z out集群时，数据迁移正常，FIO写一致性没有问题；从crush中删除osd.z正常，osd tree更新正常；删除osd.z后，集群状态正常，FIO写一致性没有问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="327" name="两个osd节点各删除一个osd进程（IO）">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[52]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p3.PNG" style="width: 494px; height: 268px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5一共5个主机节点，node4为mon节点，其余为osd节点，node2、node3 osd节点有3个osd服务进程，node1、node5 osd节点有一个osd服务进程，见组网图二</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将osd.m和osd.n out出集群，osd.m属于node1，osd.n属于node2</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.m</div>
<div>
	&nbsp; &nbsp;ceph osd out osd.n</div>
<div>
	&nbsp; &nbsp;ceph -w</div>
<div>
	&nbsp; &nbsp;ceph -s</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd.m，osd.n out出集群时，数据迁移正常，FIO没有写一致性问题</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	停止osd服务</div>
<div>
	&nbsp; &nbsp;登录node1，停止osd.m</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=m</div>
<div>
	&nbsp; &nbsp;登录node2，停止osd.n</div>
<div>
	&nbsp; &nbsp;stop ceph-osd id=n</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	从crush中删除osd.m，osd.n，node1主机</div>
<div>
	&nbsp; &nbsp;ceph osd crush rm osd.m</div>
<div>
	&nbsp; &nbsp;ceph osd crush rm osd.n</div>
<div>
	&nbsp; &nbsp;ceph auth del osd.m</div>
<div>
	&nbsp; &nbsp;ceph auth del osd.n</div>
<div>
	&nbsp; &nbsp;ceph osd crush rm node1</div>
<div>
	&nbsp; &nbsp;ceph osd rm osd.m</div>
<div>
	&nbsp; &nbsp;ceph osd rm osd.n</div>
<div>
	&nbsp; &nbsp;ceph osd tree</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd.m，osd.n，node1从crush中删除正常，osd tree显示正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	检查集群状态，检查io</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	删除osd后，集群状态正常，FIO写正常，没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="334" name="单节点关闭mon服务（IO）">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[53]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node4节点，将mon服务关闭</div>
<div>
	&nbsp; &nbsp;stop ceph-mon id=node4或kill monpid</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	mon服务停止后，集群状态无法查看，原来的fio读写不受影响</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io，对另外一个rbd起fio读写</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	mon服务停止后，集群状态无法查看，新的fio读写无法下发</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	登录node4节点，将mon服务开启</div>
<div>
	&nbsp; &nbsp;start ceph-mon id=node4或ceph-mon -i node4</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，查看io状况</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	mon服务开启后，集群状态为OK，新的FIO开始读写，没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="341" name="单节点kill mon服务（IO）">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[54]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node4节点，将mon服务kill</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	mon服务kill，集群状态无法查看，原来的FIO读写不受影响</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io，对另外一个rbd起fio读写</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	mon服务kill，集群状态无法查看，新的FIO不能读写</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	登录node4节点，将mon服务开启</div>
<div>
	&nbsp; &nbsp;start ceph-mon id=node4或ceph-mon -i node4</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，重新用FIO对image随机写，带一致性校验，查看io状况</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	mon服务开启后，集群状态为OK，继续下发FIO写，没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="348" name="多节点kill mon服务（leader mon，IO）">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[55]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上，见组网图三</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node4节点，将leader mon服务kill</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io，查看集群mon新的仲裁状</div>
<div>
	ceph quorum_status --format json-pretty</div>
<div>
	对另外一个rbd启动fio读写</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	leader mon服务kill后，集群状态正常，FIO写没有一致性问题，集群形成新的mon仲裁，产生新的leader mon，新的fio读写没有问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	登录node4节点，将mon服务开启</div>
<div>
	&nbsp; &nbsp;start ceph-mon id=node4或ceph-mon -i node4</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io，查看集群mon的仲裁状态</div>
<div>
	ceph quorum_status --format json-pretty</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node4上mon服务重新开启后，可以正常加入mon仲裁，集群状态正常，FIO写没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="355" name="多节点kill mon服务（非leader mon，IO）">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[56]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上，见组网三</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node5节点，将非leader mon服务kill</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io，查看集群mon的仲裁状态</div>
<div>
	&nbsp; &nbsp; ceph quorum_status --format json-pretty</div>
<div>
	对另外一个rbd进行FIO读写</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	非leader mon服务kill后，集群状态正常，FIO写没有一致性问题，leader mon不变，新的FIO可以读写</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	登录node5节点，将mon服务开启</div>
<div>
	&nbsp; &nbsp;start ceph-mon id=node5或ceph-mon -i node5</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io，查看集群mon的仲裁状态</div>
<div>
	&nbsp; &nbsp;ceph quorum_status --format json-pretty</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	node5上mon服务重新开启后，可以正常加入mon仲裁，集群状态正常，FIO写没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="362" name="多节点kill两个mon服务（IO）">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[57]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4，node5，node6一共6个主机节点，node1，node2，node3为osd节点，每个osd节点上有3个osd进程服务，node4，node5，node6为mon节点，3个mon的仲裁，leader mon位于node4节点上，见组网图三</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	启动FIO对image进行随机写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node4节点，将leader mon服务kill</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io，查看集群mon的仲裁状态</div>
<div>
	&nbsp; &nbsp;ceph quorum_status --format json-pretty</div>
<div>
	对另外一个rbd进行fio读写</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	kill node4节点的mon后，mon leader切换，集群状态正常，FIO没有写一致性问题，新起的fio可以正常读写</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	登录node5节点，mon服务kill</div>
<div>
	&nbsp; &nbsp;kill -9 xxxx</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io，查看集群mon的仲裁状态</div>
<div>
	&nbsp; &nbsp;ceph quorum_status --format json-pretty</div>
<div>
	对另外一个rbd启动fio读写</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	kill node5节点的mon后，集群只剩一个mon服务存活，仲裁失败，集群不能提供服务，不能查询集群信息，原来已经启动的fio读写不受影响，新起的fio不能读写</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<div>
	登录node4节点，将mon服务开启</div>
<div>
	&nbsp; &nbsp;start ceph-mon id=node4或ceph-mon -i node4</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，FIO重新对image随机写，带一致性校验，检查IO，查看mon仲裁状态</div>
<div>
	&nbsp; &nbsp;ceph quorum_status --format json-pretty</div>
<div>
	对另外一个rbd启动fio读写</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	将node4的mon服务开启后，mon服务存活数量为2，仲裁成功，继续下发FIO写成功，原来的fio读写不受影响，没有一致性问题，集群状态正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<div>
	登录node5节点，将mon服务开启</div>
<div>
	&nbsp; &nbsp;start ceph-mon id=node5或ceph-mon -i node5</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查IO，查看mon仲裁状态</div>
<div>
	&nbsp; &nbsp;ceph quorum_status --format json-pretty</div>
<div>
	对另外一个rbd启动fio读写</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	将node5的mon服务开启后，加入仲裁成功，集群状态正常，FIO写没有一致性问题，原来的fio不受影响，新的fio可以正常读写</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="391" name="暂停集群osd（IO）">
	<node_order><![CDATA[23]]></node_order>
	<externalid><![CDATA[60]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3，node4一共4个主机节点，node4为mon节点，其余为osd节点，每个osd节点上有3个osd进程服务，见组网图一</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image</div>
<div>
	3，副本数为2</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	启动FIO对image进行随机写，带一致性校验</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，将集群暂停</div>
<div>
	&nbsp; &nbsp;ceph osd set pause</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	查看集群状态，检查io</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	暂停集群后，FIO写中断，处于暂停状态，不接受读写IO</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	登录node1节点，取消集群暂停</div>
<div>
	&nbsp; &nbsp;ceph osd unset pause</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	查看集群状态，检查io</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	取消集群暂停后，恢复IO写，FIO写没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="834" name="强制删除osd进程">
	<node_order><![CDATA[24]]></node_order>
	<externalid><![CDATA[156]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，node4为mon主机，每个osd主机上有若干个osd进程，创建了pool，创建了rbd，副本数为2，集群状态正常</p>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对两个rbd起fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	读写正常，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录node1节点，kill -9进程osd.0,osd.1，将osd.0 osd.1从crush tree上删除，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd.0 osd.1从crush tree上成功删除</p>
<p>
	fio归零30秒以内，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="838" name="kill所有进程">
	<node_order><![CDATA[25]]></node_order>
	<externalid><![CDATA[157]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，node4为mon主机，每个osd主机上有若干个osd进程，创建了pool，创建了rbd，副本数为2，集群状态正常</p>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对两个rbd读写，带fio校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	读写正常，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将所有osd，mon进程全部kill -9</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起</p>
<p>
	集群无法查看状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	启动所有osd，mon进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，集群恢复正常</p>
<p>
	fio恢复读写，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="6" name="节点故障注入" >
<node_order><![CDATA[3]]></node_order>
<details><![CDATA[<p>
	节点故障注入，针对主机，操作系统进行重启，异常掉电，开关机，os panic等操作，检查ceph集群状态，IO一致性。</p>
]]></details>

<testcase internalid="398" name="一个OSD节点重启">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[61]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	在一个osd节点上执行reboot,</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	重启后ceph -s 和ceph osd tree查看集群状态</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	reboot 过程中 Fio 读写正常；重启后集群状态正常；FIO 数据一致性正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将该节点的osd服务启动，加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd服务启动正常，加入集群正常，io一致性正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="402" name="一个OSD节点掉电">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[62]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	拔掉一个osd节点电源，</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	2mins后给osd 上电，</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	断电过程中Fio正常；上电后集群状态正常；数据一致性正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="406" name="一个OSD节点关机/开机">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[63]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	在一个osd节点上执行power off,</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	等待关机后再执行power on，ceph -s 和ceph osd tree查看集群状态</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	power off后Fio 读写正常；power on 后集群状态正常；数据一致性正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="410" name="一个OSD节点Panic">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[64]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	一个OSD 节点panic</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	重启OSD节点，启动osd服务，加入集群，观察集群状态，检查io状态</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd服务启动正常，加入集群成功，集群状态为OK，io读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="414" name="两个OSD节点分别重启">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[65]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在一个osd节点上执行reboot重启，手动将该主机上的所有osd进程out出集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io归零30秒内，out过程正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，登录第二个osd主机，执行reboot重启，手动将该主机上的所有osd进程out出集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io归零30秒内，没有数据一致性问题，out过程正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将两个osd主机的osd服务都开启，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd服务开启正常，加入集群正常，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="420" name="两个OSD节点分别掉电">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[66]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	拔掉node1 osd节点电源，将node1上的所有osd服务都out出集群，观察集群状态，io状态<br />
	&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1掉电后，io归零30秒以内，数据一致性正常，out迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，拔掉node2 osd节点电源，将node2上的所有osd服务都out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node2掉电后，io归零30秒以内，数据一致性正常，out迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node1，node2上电，将osd服务开启，并加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd服务启动正常，加入集群成功，状态恢复为OK，读写正常，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="425" name="两个OSD节点分别关机">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[67]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	将node1 osd主机shutdown关机，将node1上的osd服务out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1关机时，io归零30秒以内，数据一致性正常，out流程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待集群重新恢复为OK后，将node2 osd主机shutdown关机，将node2上的ost服务out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node2关机时，io归零30秒以内，数据一致性正常，out流程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2主机开机，将osd服务都开启，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd服务开启成功，osd加入集群成功，状态为OK，io读写正常，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="430" name="两个OSD节点分别Panic">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[68]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	将node1 osd主机os panic，将该主机上的所有osd服务out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	os panic时，io归零30秒以内，out流程正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待集群重新恢复为OK后，将node2 osd主机os panic，将该主机上的所有osd服务out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	os panic时，io归零30秒以内，out流程正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node1，node2主机上线后，开启osd服务，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd服务开启成功，加入集群成功，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="435" name="两个OSD节点同时重启">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[69]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在2个OSD节点上执行reboot<br />
	&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	重启后，开启osd主机上的osd服务，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	两个节点重启，fio读写挂起，主机重启成功后，开启osd服务正常，加入集群正常，状态为OK，fio继续读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="439" name="两个OSD节点同时掉电">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[70]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	给2个OSD节点电源断电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	断电后再给2个节点上电，开启osd服务，加入集群，查看集群状态，查看io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	两个osd主机断电后，fio挂起，主机上电开机后，启动osd服务成功，加入集群成功，fio继续下发io，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="443" name="两个OSD节点同时关机/开机">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[71]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在2个OSD节点上执行power off</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	关机后，再执行power on,开启osd服务，加入集群，查看集群状态，观察io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	两个osd主机同时关机后，fio读写挂起，开机后，开启osd服务成功，加入集群成功，fio继续下发读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="447" name="两个OSD节点同时panic">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[72]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	2个OSD节点Panic</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	重启这2个OSD节点，开启osd服务，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	两个osd主机panic时，fio读写挂起，主机重启后，开启osd服务成功，加入集群成功，fio继续下发读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="451" name="全部OSD节点重启">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[73]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在3个osd节点上执行reboot</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	重启后，开启osd服务，加入集群，ceph -s 和ceph osd tree查看集群状态，查看io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有osd主机重启时，fio读写挂起，主机重启后，开启osd服务成功，加入集群成功，fio继续下发，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="455" name="全部OSD节点掉电">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[74]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在3个osd节点上拔掉电源</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	等掉电后重新上电，开启osd服务，加入集群，ceph -s ，ceph osd tree查看集群状态，查看io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有osd主机掉电后，fio读写挂起，上电后，开启osd服务成功，osd加入集群成功，fio继续读写，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="459" name="全部OSD节点关机/开机">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[75]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在3个osd节点上执行power off</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	等关机后再执行power on，开启osd服务，加入集群，ceph -s ceph osd tree查看集群状态，查看io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有osd主机关机后，fio读写挂起，主机开机后，开启osd服务成功，加入集群成功，fio继续下发读写，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="463" name="全部OSD节点Panic">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[76]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	全部OSD节点Panic</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	重启全部OSD节点，开启osd服务，加入集群，查看集群状态，查看io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有osd节点panic时，fio读写挂起，开机后，osd服务开启成功，加入集群成功，状态为OK，fio继续读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="467" name="一个monitor节点重启">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[77]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在monitor节点上执行reboot，对另外一个rbd启动fio读写，查看集群状态，io状态<br />
	&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	monitor重启时，原来的fio读写不受影响，新下发的fio不能读写，集群状态不能查看</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	等待重启后，启动mon服务，查看集群状态，对另外一个rbd启动fio读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon服务启动成功，可以查看集群状态，状态为OK，新启动的fio可以读写，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="471" name="一个monitor节点掉电">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[78]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在monitor节点上断开电源，对另外一个rbd启动fio读写，检查io，检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon节点掉电后，不能查看集群状态，原来的fio读写不受影响，新启动的fio不能读写</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	断电后重新上电 ，启动mon服务，对另外一个rbd启动fio读写，查看集群状态，检查io</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon服务启动成功，可以查询集群状态，新启动的fio可以正常读写，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="475" name="一个monitor节点关机/开机">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[79]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在monitor节点上执行power off，对另外一个rbd启动fio读写，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon关机后，原来的fio读写不受影响，新启动的fio不能读写，不能查看集群状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	关机后再power on，启动mon服务，对另外一个rbd启动fio读写，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon服务启动成功，新启动的fio可以读写，没有数据一致性问题，可以查看集群状态，为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="479" name="一个monitor节点Panic">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[80]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	一个monitor节点Panic，对另外一个rbd启动fio读写，检查io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon节点panic后，新启动的fio不能读写，原来的fio读写不受影响，不能查看集群状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	重启该monitor节点，启动mon服务，对另外一个rbd启动fio读写，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon服务启动成功，新启动的fio可以读写，没有数据一致性问题，可以查看集群状态，为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="483" name="添加第二个monitor">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[81]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p4.PNG" style="width: 452px; height: 285px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，1个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端随机写，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在node5主机上添加第二个monitor，对另外一个rbd启动读写，检查集群状态和数据一致性，ceph quorum_status --format json-pretty</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	在node5上创建mon成功，fio读写没有问题，集群仲裁没有问题，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="486" name="添加第三个monitor">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[82]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，2个Monitor,3个OSDs,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在node6主机上添加第三个monitor,对另外一个rbd启动读写，检查集群状态和数据一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建第三个mon成功，fio读写没有问题，集群仲裁成功，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="489" name="三个Mon中将Leader Mon掉电">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[83]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，3个Monitor，3个OSDs,保证双副本,一个client，Fio 正在随机写，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	将三个monitor中leader monitor异常掉电，对另外一个rbd启动读写，查看集群状态，查看io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群仲裁成功，选出新的leader mon，fio读写没有问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1 min后上电，启动mon服务，对另外一个rbd启动读写，查看集群状态和io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon服务正常开启，重启加入集群，变为leader mon，集群状态为ok，fio读写没有问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="493" name="三个Mon中将非 Leader Mon掉电">
	<node_order><![CDATA[23]]></node_order>
	<externalid><![CDATA[84]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，3个Monitor，3个OSDs,保证双副本,一个client，Fio 正在随机写，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	将三个monitor中非leader monitor异常掉电，对另外一个rbd启动读写，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，集群仲裁正常，leader mon不变</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1 min后上电，启动mon服务，查看集群状态和数据一致性状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon服务启动正常，加入集群后，仲裁成功，leader mon不变，fio读写没有问题，集群状态ok</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="497" name="三个Mon中将2个Mon掉电">
	<node_order><![CDATA[24]]></node_order>
	<externalid><![CDATA[85]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，3个Monitor3个OSDs,保证双副本,一个client，Fio 正在随机写，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	将三个monitor中2个monitor异常掉电，对另外一个rbd进行fio读写，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	新起的fio不能读写，原来的fio不受影响，集群状态不能查看</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1 min后上电，启动mon服务，加入集群，对另外一个rbd进行fio读写，查看集群状态和数据一致性状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon服务开启成功，形成仲裁，leader mon不变，集群状态为ok，新起的fio可以读写，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="501" name="Leader Mon 非NTP同步">
	<node_order><![CDATA[25]]></node_order>
	<externalid><![CDATA[86]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，3个Monitor3个OSDs,保证双副本,一个client，Fio 正在随机写，带Fio校验，配置NTP 服务</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	将leader monitor 设置为与其他2个monitor 非NTP 时钟同步，包含两种情况，一种比集群的时间慢5分钟，另外一种比集群的时间快5分钟</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	集群有告警信息，IO读写正常，数据一致性正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	10min后设置为时钟同步，观察集群状态，io状态</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	告警信息消失，集群状态正常，IO读写正常，数据一致性正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="505" name="非leader Mon 非NTP 同步">
	<node_order><![CDATA[26]]></node_order>
	<externalid><![CDATA[87]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，3个Monitor3个OSDs,保证双副本,一个client，Fio 正在随机写，带Fio校验，配置NTP 服务</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	将非 leader monitor 设置为与其他2个monitor 非NTP 时钟同步，包含另种情况，一种比集群的时间慢5分钟，另外一种比集群的时间快5分钟</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	集群有告警信息，IO读写正常，数据一致性正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	10min后设置为时钟同步，观察集群状态，io状态</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	告警信息消失，集群状态正常，IO读写正常，数据一致性正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="528" name="OSD 非NTP同步">
	<node_order><![CDATA[27]]></node_order>
	<externalid><![CDATA[90]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/.PNG" style="width: 368px; height: 234px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	集群状态正常，node1，node2，node3为osd主机，mon为mon主机，副本数为2, 4台主机已经配置了NTP时钟同步</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	1，创建pool，创建两种格式的rbd，启动fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，rbd创建成功，读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	登陆node1节点，取消NTP配置，使用命令date修改node1主机的时间，比现在的时间快5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd主机时间比集群快后，该主机上的osd会正常退出，io归零在30秒内，crush tree更新状态，标记osd为down并out</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登录node1节点，启动osd服务，观察集群状态和io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	观察900秒的时间，osd进程会再次退出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	登录node1，配置NTP，使得该主机的时间与集群时间一致，相差在0.05秒以内，启动该主机的osd服务，观察集群状态和io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd服务启动成功，加入集群成功，集群为OK状态，io正常，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	登录node1节点，取消NTP配置，使用命令date修改node1主机的时间，比现在的时间慢5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	900秒超时后，crush tree上将这些osd标记为down，并out出集群。</p>
<p>
	fio读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	登录node1节点，配置NTP，使得该主机时间与集群时间一致，相差在0.05秒以内，重启所有osd进程，观察集群状态和io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd重启后，可以加入集群，pg迁移成功，状态为OK</p>
<p>
	fio读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="509" name="同时创建3个monitor">
	<node_order><![CDATA[28]]></node_order>
	<externalid><![CDATA[88]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	机器已经安装ceph软件，public网络，cluster网络已经配置好，物理链路正常，node1，node2，node3为osd主机，node4，node5，node6为mon主机</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	使用脚本，创建集群，在创建过程中，指定3个monitor，3个monitor跟osd主机不是相同物理机</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	创建pool，创建rbd，包含格式1和格式2，启动FIO对两种格式的rbd进行随机写，带一致性校验</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	在写的过程中，将leader mon重启，观察集群状态和IO</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	leader mon重启后，集群会选出新的leader mon，读写不受影响</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	状态恢复后，将非leader的一个mon异常掉电，观察集群状态和IO</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	非leader的mon掉电后，leader mon不变，读写不受影响</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	状态恢复后，将非leader的另外一个mon panic重启，观察集群状态和IO</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	非leader的mon重启后，leader mon不变，读写不受影响</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="517" name="同时创建3个monitor（mon与osd共存）">
	<node_order><![CDATA[29]]></node_order>
	<externalid><![CDATA[89]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	机器已经安装ceph软件，public网络，cluster网络已经配置好，物理链路正常，node1，node2，node3主机为osd主机也为mon主机</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	使用脚本，创建集群，在创建过程中，指定3个monitor，3个monitor与3个osd主机为相同的主机</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	创建pool，创建rbd，包含格式1和格式2，启动FIO对两种格式的rbd进行随机写，带一致性校验</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	在写的过程中，将leader mon关机，观察集群状态和IO</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	1，leader mon重启后，集群会选出新的leader mon，该主机的osd服务会down，io归零在30秒以内，没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	leader mon开机后，启动mon服务，状态恢复后，将非leader的一个mon异常掉电，观察集群状态和IO</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	非leader的mon掉电后，leader mon不变，该主机的osd服务会down，io归零30秒以内，没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	非leader mon开机后，启动mon服务，状态恢复后，将非leader的另外一个mon panic重启，观察集群状态和IO</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	非leader的mon重启后，leader mon不变，该主机的osd服务会down，io归零30秒以内，没有一致性问题</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	状态恢复为OK后，在leader mon上取消NTP设置，使用命令date修改主机的时间，包含两种情况，一种比集群的时间快5分钟，另外一种比集群的时间慢5分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间后，集群有告警，该主机上的osd服务会正常退出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	修改leader mon的时间，设置NTP时钟同步，启动该主机的osd服务，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd服务启动成功，加入集群成功，状态为OK，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在非leader mon主机上重复步骤6和7</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="537" name="删除monitor">
	<node_order><![CDATA[30]]></node_order>
	<externalid><![CDATA[91]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	集群含有3个mon，状态正常，node1，node2，node3为osd主机，也是mon主机，已经创建了pool，创建了2种格式的rbd，启动FIO读写</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	删除node1上的mon（leader），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除mon成功，集群形成新的仲裁，状态OK，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	删除node2上的mon（leader），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除mon成功，集群只有一个mon，状态OK，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="7" name="磁盘故障注入" >
<node_order><![CDATA[4]]></node_order>
<details><![CDATA[<p>
	针对SSD盘进行的故障注入，例如慢盘，坏盘，异常拔出磁盘等，检查ceph集群状态,IO读写一致性</p>
]]></details>

<testcase internalid="541" name="物理拔盘-单主机拔一块盘(IO)">
	<node_order><![CDATA[1000]]></node_order>
	<externalid><![CDATA[92]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，node1，node2，node3为osd主机也是mon主机，三个mon,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	随机选取一台主机，手动拔掉一块创建了OSD的盘</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	ceph -s 和 ceph osd tree查看集群状态</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘拔出后，该硬盘上的osd进程退出，io归零30秒恢复，5分钟后，该硬盘上的osd out出集群，PG迁移正常，集群重新恢复为OK状态，io读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，插回硬盘，启动osd服务，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘插回后，主机正常识别，启动osd服务成功，加入集群成功，集群状态为OK，fio继续下发，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="545" name="物理拔盘-单主机拔两块盘(IO)">
	<node_order><![CDATA[1001]]></node_order>
	<externalid><![CDATA[93]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，node1，node2，node3为osd主机，也是mon，3个mon,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	随机选取一台主机，手动拔掉两块创建了OSD的盘</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	ceph -s 和 ceph osd tree查看集群状态</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘拔出后，该osd进程退出，io归零30秒恢复，5分钟后，该硬盘上的osd out出集群，PG迁移正常，集群重新恢复为OK状态，io读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，插回硬盘，启动osd服务，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘插回后，主机正常识别，启动osd服务成功，加入集群成功，集群状态为OK，fio继续下发，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="549" name="物理拔盘-单主机拔三块盘(IO)">
	<node_order><![CDATA[1002]]></node_order>
	<externalid><![CDATA[94]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，node1，node2，node3为osd主机，也是mon，3个mon,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	随机选取一台主机，手动拔掉三块创建了OSD的盘</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	ceph -s 和 ceph osd tree查看集群状态</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘拔出后，该osd进程退出，io归零30秒恢复，5分钟后，该硬盘上的osd out出集群，PG迁移正常，集群重新恢复为OK状态，io读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，插回硬盘，启动osd服务，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘插回后，主机正常识别，启动osd服务成功，加入集群成功，集群状态为OK，fio继续下发，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="553" name="物理拔盘-双主机各拔一块盘(IO)">
	<node_order><![CDATA[1003]]></node_order>
	<externalid><![CDATA[95]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，node1，node2，node3为osd主机，也是mon,保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	随机选取两台主机，手动拔掉一块创建了OSD的盘</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	ceph -s 和ceph osd tree查看集群状态</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群显示有pg是旧状态，不能更新，fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将硬盘插回，启动osd服务，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘插回后，主机正常识别，启动osd服务成功，加入集群成功，集群状态为OK，fio继续下发，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="561" name="逻辑拔盘-单主机拔一块盘(IO)">
	<node_order><![CDATA[1004]]></node_order>
	<externalid><![CDATA[96]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，node1，node2，node3为osd主机，也是mon，3个mon，保证双副本,一个client，Fio 正在client端运行，带Fio校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	随机选取一台主机，用scsi命令模拟拔掉一块创建了OSD的盘</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	ceph -s 和 ceph osd tree查看集群状态</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘拔出后，该硬盘上的osd进程退出，io归零30秒恢复，5分钟后，该硬盘上的osd out出集群，PG迁移正常，集群重新恢复为OK状态，io读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，将盘扫出，启动osd进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘扫出后，主机正常识别，启动osd服务成功，加入集群成功，集群状态为OK，fio继续下发，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="566" name="rbd失效(IO)">
	<node_order><![CDATA[1005]]></node_order>
	<externalid><![CDATA[97]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	部署Ceph Cluster, 状态为health HEALTH_OK，node1，node2，node3为osd主机，也是mon，3个mon，每个OSD主机有3个osd服务，保证双副本,一个client</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	创建集群，配置crush算法，使得指定osd来创建pool。创建两个osd域osddomain1，osddomain2，每个osd域都跨所有的OSD主机，副本在host之间分布</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	创建pool1，指定使用osddomain1的osd，创建pool2，指定使用osddomain2的osd</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	在pool1中创建rbd1，格式为1，创建rbd2格式为2，在pool2中创建rbd3，格式为1，创建rbd4，格式为2</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建rbd成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	启动FIO，对rbd1，rbd2，rbd3，rbd4进行随机写，带校验，在写的过程中，将pool2中的硬盘都踢出，观察IO</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool2里面的磁盘被全部scsi踢出后，相关的osd进程退出，rbd3，rbd4的fio读写挂起，rbd1，rbd2的fio读写不受影响。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	将pool2中的硬盘都加载上，启动osd服务，观察IO</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘加载后，启动osd服务成功，加入集群成功，rbd3，rbd4的fio读写继续下发，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<div>
	在写的过程中，将pool1中的硬盘都踢出，观察IO</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool1里面的磁盘被全部scsi踢出后，相关的osd进程退出，rbd1，rbd2的fio读写挂起，rbd3，rbd4的fio读写不受影响。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<div>
	将pool1中的硬盘都加载上，启动osd服务，观察IO</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘加载后，启动osd服务成功，加入集群成功，rbd1，rbd2的fio读写继续下发，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="8" name="压力和长时间测试" >
<node_order><![CDATA[5]]></node_order>
<details><![CDATA[<p>
	压力测试，即保证一定的读写压力，使得存储集群处于一定的繁忙状态，检查集群状态，IO一致性，以及在此基础上做一些配置或者管理操作，检查集群对这些管理平面的需求的响应时间和效率。</p>
<p>
	长时间测试，模拟客户实际使用场景，启动数据模型进行长时间读写运行，查看集群状态，IO一致性，有无节点宕机，服务异常退出等</p>
]]></details>

<testcase internalid="575" name="压力测试">
	<node_order><![CDATA[1000]]></node_order>
	<externalid><![CDATA[98]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，也是mon主机，3个mon，每个osd主机上有多个osd进程，双副本</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建若干个pool，在每个pool中创建若干个rbd，大小100G左右</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，rbd创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	在Client端对RBD下发IO，IO模式为8K随机读，随机写或混合读写，iodepth为1，观察集群每台server的CPU，内存，以及OSD磁盘的利用率，保证监控指标值达到80%左右，如果压力未达到，可以增加客户端，创建RBD并下发IO</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	IO下发正常，CPU,内存，磁盘利用率显示正常，没有明显的较大的波动</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在压力保证的情况下，反复创建删除新的Pool/RBD,或者对pool/RBD做Snapshot等操作，运行半小时观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	IO下发正常，CPU,内存，磁盘利用率显示正常，没有明显的较大的波动，保证长时间情况下，系统不会Panic，产品不会宕机，业务运行正常，IO不会中断，OSD和Mon不会Down，内存不会泄露，数据一致性正常。</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将iodepth分别修改为4,8,16,32,64,128,256，重复步骤2和步骤3，运行半小时，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	IO下发正常，CPU,内存，磁盘利用率显示正常，没有明显的较大的波动，保证长时间情况下，系统不会Panic，产品不会宕机，业务运行正常，IO不会中断，OSD和Mon不会Down，内存不会泄露，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="583" name="长时间测试-基准测试">
	<node_order><![CDATA[1001]]></node_order>
	<externalid><![CDATA[100]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3主机为osd主机，也是mon主机，每个osd主机上有若干osd进程，集群状态OK，副本数为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建2个pool，每个pool中2个rbd，每个rbd 200G左右</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，rbd成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在Client端对4个RBD下发IO，iodepth 128，采用5种IO模型（随机读写，顺序读写，随机混合读写7：3），每种IO模式运行时间2小时，记录每种模型下，服务器的CPU，内存以及磁盘利用率</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无告警，CPU，内存，磁盘利用率显示正常，没有明显的较大的波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="587" name="长时间测试-长时间读写">
	<node_order><![CDATA[1002]]></node_order>
	<externalid><![CDATA[101]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，也是mon，3个mon，集群状态正常，双副本，每个osd主机上有若干个osd进程，已经做过长时间测试的基准测试</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在总结了每种IO模型下的性能指标值后，调用脚本（包含5种IO模型）对集群进行长时间IO测试，持续循环进行ceph 增删查改等基本功能性操作。测试时间为一个测试周期。此时不关注性能，不对产品做任何故障操作。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	验证产品在不同IO模型的切换下，主机的CPU，内存，磁盘利用率和基准测试保持一致，同时在长时间测试下，系统不会Panic，产品不会宕机，业务运行正常，IO不会中断，OSD和Mon服务可用，节点可访问，内存不会泄露，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="590" name="极限测试">
	<node_order><![CDATA[1003]]></node_order>
	<externalid><![CDATA[102]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，也是mon主机，3个mon，集群状态正常，每个osd主机有若干个osd进程，双副本</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	将产品放置于零下5℃环境下，创建Pool,创建了RBD后，在客户端采取任何一种IO模式对产品下发IO</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	集群状态正常，pool 和RBD都能正常创建</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	观察服务器的性能指标，如CPU,内存，磁盘利用率。</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	服务器性能指标正常，没有出现较大的波动</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	观察业务性能，IOPS，带宽，延迟等信息</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	业务性能正常，没有出现较大的波动，IO数据一致，集群工作正常，服务器工作正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将产品放置于高温环境下（70℃以上），重复步骤2和步骤3</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	集群状态正常，pool 和RBD都能正常创建</div>
<div>
	服务器性能指标正常，没有出现较大的波动</div>
<div>
	业务性能正常，没有出现较大的波动</div>
<div>
	IO数据一致，集群工作正常，服务器工作正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="636" name="PG/Crush故障注入" >
<node_order><![CDATA[6]]></node_order>
<details><![CDATA[<p>
	Crush的不同配置下的fio读写，故障注入</p>
<p>
	PG属性的修改，PG状态的改变下的故障注入</p>
]]></details>

<testcase internalid="637" name="创建pool，使用不同数量的pg">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[111]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有3个osd进程，node1，node2，node3也是mon主机，三个mon，集群状态正常，双副本</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建poo1，pg数量：pg 32，pgp 32，在pool1中创建rbd1，启动fio读写，带一致性校验，运行5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool1，rbd1创建成功，fio读写正常，没有大范围的波动，没有数据一致性问题，osd进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建poo2，pg数量：pg 2048，pgp 2048，在pool2中创建rbd2，启动fio读写，带一致性校验，运行5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool2，rbd2创建成功，fio读写正常，没有大范围波动，没有数据一致性问题，osd进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	创建pool3，pg数量：pg 729，pgp 729，在pool3中创建rbd3，启动fio读写，带一致性校验，运行5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool3，rbd3创建成功，fio读写正常，没有大范围波动，没有数据一致性问题，osd进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	创建pool4，pg数量：pg 500000，pgp 500000</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool4无法创建成功，提示每个osd上最多分布32768个pg，该参数是默认值</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="642" name="创建大量pool">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[112]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上3个osd进程，同时node1，node2，node3主机也是mon，三个mon，双副本，集群状态正常</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建30个pool，每个pool中pg 数量动态计算（合理pg数），每个pool中创建一个rbd 大小50G</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool成功，创建rbd成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	fio对每个rbd进行读写，每个rbd都写入30G的数据进行预埋</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	对其中两个rbd进行fio随机写，带一致性校验，在读写过程中，kill -9一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	kill进程后，io归零30秒恢复，集群有告警，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将kill的osd进程，out出集群(<span style="background-color: rgb(255, 255, 255); font-family: Arial; font-size: 14px;">ceph osd out osd.x</span>)，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	out操作成功，集群有pg迁移，最后变为OK状态，在迁移初期有短暂的性能降低，然后恢复到较高的水平，fio没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	启动osd进程(ceph-osd -i osdid)，并添加回集群(<span style="background-color: rgb(255, 255, 255); font-family: Arial; font-size: 14px;">ceph osd in osd.x</span>)，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd进程成功，加入集群成功，有pg的迁移，在迁移初期有短暂的性能降低，然后恢复到较高水平，fio没有数据一致性问题，迁移完成后，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在有FIO读写，带一致性校验的情况下，将其中一台osd主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	掉电后，io归零在30秒以内，集群有告警，fio无数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	6分钟后，将该主机上电，启动osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	5分钟后，集群自动将该主机上的osd进程out出集群，fio性能短暂下降，PG迁移完成后，集群恢复到OK状态。主机上电后，启动osd进程成功，加入集群成功，PG有迁移恢复，最后为OK状态，fio无数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="651" name="在线扩展pg数量">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[113]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上若干个osd进程，node1，node2，node3也为mon主机，3个mon主机 ，创建rbd pool，双副本，集群状态正常</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个rbd，fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，fio读写无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，增加1，观察集群状态，io状态，重复5次，每次增加1。(ceph osd pool set poolname pg_num value，value是修改后的值)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，增加20，观察集群状态，io状态，重复5次，每次增加20。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，增加100，观察集群状态，io状态，重复3次，每次增加100。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，修改pgp数量，增加1，观察集群状态，io状态，重复5次，每次增加1。(ceph osd pool set poolname pgp_num value，value是修改后的值)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，有数据迁移，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，修改pgp数量，增加20，观察集群状态，io状态，重复5次，每次增加20。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，有数据迁移，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，修改pgp数量，增加100，观察集群状态，io状态，重复3次，每次增加100。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，fio正常，都没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="660" name="在线减少pg数量">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[114]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	<span style="font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">node1，node2，node3为osd主机，每个osd主机上3个osd进程，node1，node2，node3也为mon主机，3个mon主机 ，创建rbd pool，pg 1024，pgp 1024，双副本，集群状态正常</span></p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个rbd，fio读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，减少1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	不能减少pg，提示错误</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，减少20，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	不能减少pg，提示错误</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，减少100，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	不能减少pg，提示错误</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pgp数量，减少1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态有告警，提示pgp比pg少</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pgp数量，减少20，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态有告警，提示pgp比pg少</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pgp数量，减少100，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态有告警，提示pgp比pg少</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="669" name="在线增加副本数">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[115]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，同时node1，node2，node3也是mon，3个mon，创建了两个pool，pool1副本数为1，pool2副本数为2，每个pool中创建了一个rbd</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	启动fio，对两个rbd分别读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，对pool1设置副本数为2，对pool2设置副本数为3，观察集群状态，观察io状态，查看pg分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	副本增加后，有pg的分配和数据的迁移，fio性能短暂下降然后恢复到较高水平，集群恢复为OK状态，PG数量有增加，分布在不同的osd上，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="671" name="在线减少副本数">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[116]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，同时node1，node2，node3也是mon，3个mon，创建了两个pool，副本数为3，每个pool中创建了一个rbd</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	启动fio对两个rbd读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，设置两个pool的副本数为2，查看集群状态，io状态，pg分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	减少副本数后，pg数量减少，集群有再平衡，性能有短暂降低后恢复到稳定的高水平，比原来三副本的性能要高，集群状态为OK，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群状态为OK时，在读写过程中，设置两个pool的副本数为1，查看集群状态，io状态，pg分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	减少副本数后，pg数量减少，集群有再平衡，性能有短暂降低后恢复到稳定的高水平，比原来两副本的性能要高，集群状态为OK，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="889" name="在线修改pool的crush rule">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[158]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，rule规则为1，在domain2上创建pool3，pool4，rule规则为2。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在fio读写过程中，手动修改pool1的crush rule规则，由rule 1设置为rule 2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain2的osd上，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待pool1的pg全部迁移到osddomain2的osd后，在读写过程中，手动修改pool1的crush rule规则，由rule 2设置为rule 1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain1的osd上，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在fio读写过程中，手动修改pool3的crush rule规则，由rule 2设置为rule 1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain1的osd上，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待pool3的pg全部迁移到osddomain1的osd后，在读写过程中，手动修改pool3的crush rule规则，由rule 1设置为rule 2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain2的osd上，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在fio读写过程中，手动修改pool1的crush rule规则，由rule 1设置为rule 2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain2的osd上，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="678" name="PG迁移-kill 进程">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[117]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将osd.0,osd.1进程kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题，集群有告警，有PG的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动osd.0,osd.1进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，fio归零30秒以内，没有数据一致性问题，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio恢复到稳定状态后，将node1上的另外一个osd.2进程kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后启动osd.2进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，fio归零30秒以内，没有数据一致性问题，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待fio恢复到稳定状态后，在node2上kill -9一个osd进程，一分钟后启动该进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒内，没有数据一致性问题，有pg的迁移。若是负责迁移的osd被kill，则集群会有pg卡住，io挂起，该osd恢复后，pg恢复正常，io恢复。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	待fio恢复到稳定状态后，在node3上kill -9一个osd进程，一分钟后启动该进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒内，没有数据一致性问题，有pg的迁移。</p>
<p>
	若是负责迁移的osd被kill，则集群会有pg卡住，io挂起，该osd恢复后，pg恢复正常，io恢复。</p>
<p>
	最后pg迁移完成，集群状态为OK。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="680" name="PG迁移-主机异常掉电">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[118]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将node1主机异常掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有一致性问题，集群有告警信息，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node1节点上电，启动该节点上的所有osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd启动成功，加入集群成功，性能短暂降低后恢复到较高水平并稳定，读写没有一致性问题，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio读写稳定后，将node2主机异常掉电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将node2节点上电，启动该节点上的所有osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd启动成功，加入集群成功，性能短暂降低后恢复到较高水平并稳定，读写没有一致性问题，有pg的迁移，最后迁移全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="682" name="PG迁移-网络中断">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[119]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断node1节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断node1节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在node2节点上重复操作步骤1和步骤2，短时间中断网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断node1节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断node1节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在node2节点上重复操作步骤4和步骤5，长时间中断网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	长时间中断后，有pg卡住，出现stuck inactive的状态，io挂起</p>
<p>
	连接恢复后，io恢复，没有数据一致性问题，pg状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的public网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的cluster网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，pg迁移正常</p>
<p>
	最后pg迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="684" name="PG迁移-IP变更">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[120]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，修改node1主机上public网络的ip（包含两种情况，修改成同网段的ip，修改成不同网段的ip）</p>
<p>
	观察集群状态，io状态</p>
<p>
	1分钟后，修改回原来的ip，启动该节点的osd进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node1主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，修改node1主机上的cluster网络ip，重复步骤1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node1主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，修改node2主机上的public、cluster网络ip，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，node2主机上osd进程正常退出，部分pg卡住，stuck inactive状态，io挂起</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
<p>
	最后迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="686" name="PG迁移-NTP时钟不同步">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[121]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，设置node1主机非NTP同步时钟，使用命令date修改时间，包含两种情况，比集群时间晚5分钟，比集群时间快5分钟。</p>
<p>
	观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	比集群时间快5分钟，该主机上的osd进程正常退出（有的不退出，需要重启才能重新加入集群），fio归零30秒以内，没有数据一致性问题，集群有告警</p>
<p>
	比集群时间慢5分钟，fio归零30秒以内，900秒后，该osd上的服务在crush上被标记为down并out（需要重启osd进程才能再次加入集群）</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	设置node1主机时间NTP时钟同步，和集群时间相差在0.05秒以内</p>
<p>
	重启该节点所有osd进程，加入集群</p>
<p>
	观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间成功，NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登陆node2，修改时间，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间后，该主机上的osd进程正常退出（部分不退出，需要重启），部分pg卡住，fio挂起</p>
<p>
	NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg迁移正常</p>
<p>
	最终pg迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="688" name="PG迁移-磁盘故障">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[122]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，登陆node1节点，将osd.0 osd.1所在的磁盘通过UI进行下电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	osd.0,osd.1进程正常退出，集群有告警信息</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，通过UI对这两个磁盘上电，启动osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio稳定后，在node1上，使用命令踢盘的方式删除osd.0 osd.1的磁盘，再用命令扫出磁盘，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	踢盘时：</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	osd.0,osd.1进程正常退出，集群有告警信息</p>
<p>
	扫盘后：</p>
<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待fio稳定后，在node2上，使用命令和UI下电磁盘两种方式故障一个磁盘，重复步骤1到步骤3</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	磁盘故障时：</p>
<p>
	fio归零30秒以内，没有数据一致性问题；若是负责迁移的osd被终止，则集群会有pg卡住，io挂起，该osd恢复后，pg恢复正常，io恢复</p>
<p>
	osd进程正常退出，集群有告警信息</p>
<p>
	磁盘恢复后：</p>
<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg迁移正常，最后迁移全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="690" name="PG迁移-IO压力增加">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[123]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，新增加10条流的读写（或增加client），iodepth 256，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群整体的iops，带宽有提升，osd进程正常，集群状态正常，pg迁移正常，性能稳定无较大的波动</p>
<p>
	最终pg迁移全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="692" name="PG迁移-管理平面操作">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[124]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，用脚本大批量创建，删除pool、rbd、快照并修改属性，用脚本反复查看集群状态、osd tree等信息，做大量管理平面的操作，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写无数据一致性问题</p>
<p>
	性能稍有衰减（iops衰减1k-2k），没有大范围波动</p>
<p>
	集群状态正常，osd进程正常</p>
<p>
	pg迁移过程最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="694" name="PG迁移-暂停集群">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[125]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，暂停osd，观察集群状态，io状态</p>
<p style="margin: 0px; padding: 0px; font-family: Arial; font-size: 14px; background-color: rgb(255, 255, 255);">
	暂停osd （暂停后整个集群不再接收数据）</p>
<p style="margin: 0px; padding: 0px; font-family: Arial; font-size: 14px; background-color: rgb(255, 255, 255);">
	[root@admin ~]# ceph osd pause</p>
<p style="margin: 0px; padding: 0px; font-family: Arial; font-size: 14px; background-color: rgb(255, 255, 255);">
	set pauserd,pausewr &nbsp; &nbsp; &nbsp;</p>
<p style="margin: 0px; padding: 0px; font-family: Arial; font-size: 14px; background-color: rgb(255, 255, 255);">
	&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	暂停成功，fio读写挂起</p>
<p>
	osd进程正常，迁移正常进行，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，取消暂停，观察集群状态，io状态</p>
<p style="margin: 0px; padding: 0px; font-family: Arial; font-size: 14px; background-color: rgb(255, 255, 255);">
	开启osd （开启后再次接收数据）&nbsp;</p>
<p style="margin: 0px; padding: 0px; font-family: Arial; font-size: 14px; background-color: rgb(255, 255, 255);">
	[root@admin ~]# ceph osd unpause</p>
<p style="margin: 0px; padding: 0px; font-family: Arial; font-size: 14px; background-color: rgb(255, 255, 255);">
	unset pauserd,pausewr</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	取消暂停成功，fio继续下发读写，没有一致性问题</p>
<p>
	osd进程正常，pg迁移正常，集群状态正常</p>
<p>
	pg迁移最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="696" name="PG迁移-添加进程">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[126]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1是mon主机，1个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，在node1节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，在node2节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG迁移完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，在node2和node3上创建mon进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程创建成功，形成新的集群仲裁</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG迁移完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="698" name="PG迁移-删除进程">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[127]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，删除osd.0进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node1节点上没有out的osd进程删除（osd.2）,观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node2上的一个osd进程删除，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，删除两个mon进程（leader mon和非leader mon），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功，集群形成新的仲裁</p>
<p>
	fio不归零，没有数据一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="700" name="PG迁移-rbd失效">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[128]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /><img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，在domain2上创建pool3，pool4。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0（osddomain1） osd.1（osddomain2） out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将osddomain2中的硬盘全部拔出，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘拔出后，相应的osd进程正常退出，osddomain2中的rbd读写挂起，迁移停止，集群有告警信息</p>
<p>
	osddomain1中的rbd读写不受影响，没有数据一致性问题，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将osddomain2中的硬盘插回，启动osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd进程成功，加入集群成功，osddomain2中rbd恢复读写，没有一致性问题，迁移开始，最后迁移完成</p>
<p>
	osddomain1中的rbd读写不受影响，没有数据一致性问题，pg迁移正常，最后迁移完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将osddomain1中的磁盘全部拔出，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osddomain2中的rbd读写不受影响，没有数据一致性问题，pg迁移正常</p>
<p>
	磁盘拔出时：</p>
<p>
	osddomain1上的osd进程正常退出，集群有告警信息</p>
<p>
	rbd读写挂起，pg迁移暂停</p>
<p>
	磁盘插回后：</p>
<p>
	osddomain1上的osd进程启动成功，加入集群成功</p>
<p>
	rbd读写恢复，没有数据一致性问题，pg迁移开始</p>
<p>
	最终两个osddomain上的pg迁移全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="702" name="PG迁移-修改pool的crush rule">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[129]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，rule规则为1，在domain2上创建pool3，pool4，rule规则为2。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录node1节点，将该节点的osd.0（osddomain1） osd.1（osddomain2）out出集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	out操作成功，有pg的迁移</p>
<p>
	fio读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，手动修改pool1的crush rule规则，由rule2设置为rule 1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain1的osd上</p>
<p>
	最终pg迁移全部完成，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录node1节点，将该节点的osd.0 osd.1 重新加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	in操作成功，有pg的迁移</p>
<p>
	fio读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，手动修改pool1的crush rule规则，由rule 1设置为rule 2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain2的osd上</p>
<p>
	最终pg迁移全部完成，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中修改pool3的crush rule规则，由rule2设置为rule1，重复步骤1到步骤4</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rule 2设置为rule 1：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain1的osd上，集群状态OK</p>
<p>
	rule 1设置为rule 2：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain2的osd上</p>
<p>
	最终pg迁移全部完成，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="704" name="PG恢复-kill进程">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[130]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，将node1节点的osd.2进程kill -9，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒后恢复，没有数据一致性问题</p>
<p>
	集群有告警信息，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将osd.2进程启动，加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	进程启动成功，加入集群成功，集群状态正常</p>
<p>
	io性能短暂降低后恢复到较高水平并稳定，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node2主机，对一个osd进程kill -9，重复步骤1和步骤2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒内，没有数据一致性问题，有pg的迁移。</p>
<p>
	若是负责迁移的osd被kill，则集群会有pg卡住，io挂起，该osd恢复后，pg恢复正常，io恢复。</p>
<p>
	最后pg迁移完成，集群状态为OK。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="706" name="PG恢复-主机异常掉电">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[131]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，将node2主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动node2主机，启动osd进程，加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，没有数据一致性问题</p>
<p>
	性能短暂下降后恢复到较高水平并稳定，没有数据一致性问题，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node1主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	node1主机上的osd进程正常退出，集群有告警信息，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	启动node1主机，启动osd进程（不启动osd.0,osd.1）,查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，没有数据一致性问题</p>
<p>
	性能短暂下降后恢复到较高水平并稳定，没有数据一致性问题，pg恢复正常，最后pg恢复完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="708" name="PG恢复-网络中断">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[132]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，短时间中断node1的public网络，拔出插回在5秒内，观察集群状态，io状态，重复3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	短时间中断后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，短时间中断node1的cluster网络，拔出插回在5秒内，观察集群状态，io状态，重复3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	短时间中断后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在node2上短时间中断public、cluster网络，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	短时间中断后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在node1上，长时间中断public网络，拔出插回在2分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	插回线缆后，连接恢复，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在node1上，长时间中断cluster网络，拔出插回间隔2分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	插回线缆后，连接恢复，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在node2上长时间中断public、cluster网络，重复步骤4和步骤5</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	长时间中断后，有pg卡住，出现stuck inactive的状态，io挂起</p>
<p>
	连接恢复后，io恢复，没有数据一致性问题，pg状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将所有node的public网络长时间中断，拔出插回间隔2分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，fio读写挂起，集群有告警信息</p>
<p>
	插回线缆后，连接恢复，fio读写恢复，没有数据一致性问题，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	将所有node的cluster网络长时间中断，拔出插回间隔2分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，fio读写挂起，集群有告警信息</p>
<p>
	插回线缆后，连接恢复，fio读写恢复，没有数据一致性问题，集群状态正常，pg恢复正常，最后pg恢复完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="710" name="PG恢复-IP变更">
	<node_order><![CDATA[23]]></node_order>
	<externalid><![CDATA[133]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，修改node1主机上public网络的ip（包含两种情况，修改成同网段的ip，修改成不同网段的ip）</p>
<p>
	观察集群状态，io状态</p>
<p>
	1分钟后，修改回原来的ip，启动该节点的osd进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node1主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，修改node1主机上的cluster网络ip，重复步骤1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node1主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，修改node2主机上的public、cluster网络ip，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，node2主机上osd进程正常退出，部分pg卡住，stuck inactive状态，io挂起</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
<p>
	最后迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="712" name="PG恢复-NTP时钟不同步">
	<node_order><![CDATA[24]]></node_order>
	<externalid><![CDATA[134]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，设置node1主机非NTP同步时钟，使用命令date修改时间，包含两种情况，比集群时间晚5分钟，比集群时间快5分钟。</p>
<p>
	观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间后，该主机上的osd进程全部正常退出，fio归零30秒以内，没有数据一致性问题，集群有告警</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，设置node1主机时间NTP时钟同步，和集群时间相差在0.05秒以内</p>
<p>
	启动osd进程，加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间成功，NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登陆node2，修改时间，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间后，该主机上的osd进程正常退出（部分不退出，需要重启），部分pg卡住，fio挂起</p>
<p>
	NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg迁移正常</p>
<p>
	最终pg迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="714" name="PG恢复-磁盘故障">
	<node_order><![CDATA[25]]></node_order>
	<externalid><![CDATA[135]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，将node2上的一个硬盘通过UI下电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题；若是负责迁移的osd被kill，则集群会有pg卡住，io挂起</p>
<p>
	osd进程正常退出，集群有告警信息，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，通过UI界面对硬盘上电，启动osd进程，加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	io性能短暂降低后恢复到较高水平并稳定，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，在node2上使用命令方式删除另外一个磁盘，重复步骤1和2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除磁盘时：</p>
<p>
	fio归零30秒以内，没有数据一致性问题；若是负责迁移的osd被kill，则集群会有pg卡住，io挂起，该osd恢复后，pg恢复正常，io恢复</p>
<p>
	osd进程正常退出，集群有告警信息，pg恢复正常</p>
<p>
	扫出磁盘后：</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	io性能在短暂降低后恢复到较高水平并稳定，集群状态正常，pg恢复正常，最后pg恢复全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="716" name="PG恢复-IO压力增加">
	<node_order><![CDATA[26]]></node_order>
	<externalid><![CDATA[136]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，新增加10条流的读写（或增加client），iodepth 256，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群整体的iops，带宽有提升，osd进程正常，集群状态正常，pg恢复正常，性能稳定无较大的波动</p>
<p>
	最终pg恢复全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="718" name="PG恢复-管理平面操作">
	<node_order><![CDATA[27]]></node_order>
	<externalid><![CDATA[137]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，用脚本大批量创建，删除pool、rbd、快照并修改属性，用脚本反复查看集群状态、osd tree等信息，做大量管理平面的操作，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写无数据一致性问题</p>
<p>
	性能稍有衰减（iops衰减1k-2k），没有大范围波动</p>
<p>
	集群状态正常，osd进程正常</p>
<p>
	pg恢复最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="720" name="PG恢复-暂停集群">
	<node_order><![CDATA[28]]></node_order>
	<externalid><![CDATA[138]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，暂停osd，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	暂停成功，fio读写挂起</p>
<p>
	osd进程正常，pg恢复暂停，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，取消暂停，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	取消暂停成功，fio继续下发读写，没有一致性问题</p>
<p>
	osd进程正常，pg恢复正常，集群状态正常</p>
<p>
	pg最终全部恢复完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="722" name="PG恢复-添加进程">
	<node_order><![CDATA[29]]></node_order>
	<externalid><![CDATA[139]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1是mon主机，1个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，在node1节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，在node2节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG恢复完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，在node2和node3上创建mon进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程创建成功，形成新的集群仲裁</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG恢复完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="724" name="PG恢复-删除进程">
	<node_order><![CDATA[30]]></node_order>
	<externalid><![CDATA[140]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。集群状态正常，创建了5个pool，每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd进行随机写，带fio校验，在读写过程中，登录node1节点，通过UI将两块硬盘下电，对应的osd.0 osd.1 手动out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，删除osd.0进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，有pg的恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node1节点上没有out的osd进程删除（osd.2）,观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node2上的一个osd进程删除，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，删除两个mon进程（leader mon和非leader mon），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功，集群形成新的仲裁</p>
<p>
	fio不归零，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="726" name="PG恢复-rbd失效">
	<node_order><![CDATA[31]]></node_order>
	<externalid><![CDATA[141]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，在domain2上创建pool3，pool4。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验，在读写过程中，登录node1节点，将osddomain1，osddomain2各通过UI进行下电一个磁盘，将相应的osd.0 osd.1手动out出集群</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复的过程中，将osddomain1中的硬盘全部拔出，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osddomain1上的rbd读写挂起，osd进程正常退出，pg恢复暂停，集群有告警</p>
<p>
	osddomain2上的rbd读写不受影响，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将osddomain1上的硬盘全部插回，启动osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，成功加入集群，osddomain1上的rbd恢复读写，没有一致性问题，pg恢复正常，最终恢复成功</p>
<p>
	osddomain2上的rbd读写不受影响，没有数据一致性问题，pg恢复正常，最终恢复完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，将osddomain2中的硬盘全部拔出，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osddomain1中的rbd读写不受影响，没有数据一致性问题，pg恢复正常</p>
<p>
	磁盘拔出时：</p>
<p>
	osddomain2上的osd进程正常退出，集群有告警信息</p>
<p>
	rbd读写挂起，pg恢复暂停</p>
<p>
	磁盘插回后：</p>
<p>
	osddomain2上的osd进程启动成功，加入集群成功</p>
<p>
	rbd读写恢复，没有数据一致性问题，pg恢复开始</p>
<p>
	最终两个osddomain上的pg恢复全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="728" name="PG恢复-修改pool的crush rule">
	<node_order><![CDATA[32]]></node_order>
	<externalid><![CDATA[142]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，crush rule 1，在domain2上创建pool3，pool4，crush rule 2。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验，在读写过程中，登录node1节点，将osddomain1，osddomain2各通过UI下电一个硬盘，将相应的osd.0 osd.1手动out出集群</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，手动修改pool1的crush rule规则，由rule 1设置为rule 2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain2的osd上</p>
<p>
	pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待pool1的pg全部迁移到osddomain2的osd上后，在pg恢复过程中，手动修改pool1的crush rule规则，由rule 2设置为rule 1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain1的osd上</p>
<p>
	pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，手动设置pool3的crush rule规则，重复步骤1步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rule 2设置为rule 1：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain1的osd上</p>
<p>
	pg恢复正常</p>
<p>
	rule 1设置为rule 2：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain2的osd上</p>
<p>
	pg恢复正常，最终pg恢复全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="730" name="PG平衡-kill进程">
	<node_order><![CDATA[33]]></node_order>
	<externalid><![CDATA[143]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg平衡过程中，在node2上kill -9两个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将杀死的的osd进程启动，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂降低后恢复到较高水平并稳定，没有数据一致性问题</p>
<p>
	pg平衡正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio读写稳定后，登陆node3节点，将新创建的osd进程全部kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将node3的kill的osd进程全部启动，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂降低后恢复到较高水平并稳定，没有数据一致性问题</p>
<p>
	pg平衡正常，最后平衡完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="732" name="PG平衡-主机异常掉电">
	<node_order><![CDATA[34]]></node_order>
	<externalid><![CDATA[144]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg平衡过程中，将node2主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动node2主机，启动osd进程，加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，没有数据一致性问题</p>
<p>
	性能短暂下降后恢复到较高水平并稳定，没有数据一致性问题，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node3主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	启动node3主机，启动osd进程,查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，没有数据一致性问题</p>
<p>
	性能短暂下降后恢复到较高水平并稳定，没有数据一致性问题，pg平衡正常，最后pg平衡完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="734" name="PG平衡-网络中断">
	<node_order><![CDATA[35]]></node_order>
	<externalid><![CDATA[145]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断node3节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断node3节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在node2节点上重复操作步骤1和步骤2，短时间中断网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断node3节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断node3节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在node2节点上重复操作步骤4和步骤5，长时间中断网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio挂起，pg卡住。</p>
<p>
	插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的public网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的cluster网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，pg平衡正常</p>
<p>
	最终pg平衡全部完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="736" name="PG平衡-IP变更">
	<node_order><![CDATA[36]]></node_order>
	<externalid><![CDATA[146]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，修改node3主机上public网络的ip（包含两种情况，修改成同网段的ip，修改成不同网段的ip）</p>
<p>
	观察集群状态，io状态</p>
<p>
	1分钟后，修改回原来的ip，启动该节点的osd进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node3主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，修改node3主机上的cluster网络ip，重复步骤1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node3主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，修改node2主机上的public、cluster网络ip，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，node2主机上osd进程正常退出，有部分pg出现stuck inactive的状态，io挂起</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
<p>
	最后平衡全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="738" name="PG平衡-NTP时钟不同步">
	<node_order><![CDATA[37]]></node_order>
	<externalid><![CDATA[147]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，设置node3主机非NTP同步时钟，使用命令date修改时间，包含两种情况，比集群时间晚5分钟，比集群时间快5分钟。</p>
<p>
	观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	比集群时间快5分钟，该主机上的osd进程正常退出（有的不退出，需要重启才能重新加入集群），fio归零30秒以内，没有数据一致性问题，集群有告警</p>
<p>
	比集群时间慢5分钟，fio归零30秒以内，900秒后，该osd上的服务在crush上被标记为down并out（需要重启osd进程才能再次加入集群）</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，设置node3主机时间NTP时钟同步，和集群时间相差在0.05秒以内</p>
<p>
	启动osd进程，加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间成功，NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登陆node2，修改时间，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间后，该主机上的osd进程正常退出（部分不退出，需要重启），部分pg卡住，fio挂起</p>
<p>
	NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg迁移正常</p>
<p>
	最终pg迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="740" name="PG平衡-磁盘故障">
	<node_order><![CDATA[38]]></node_order>
	<externalid><![CDATA[148]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，登陆node3节点，将新添加的3个osd进程所在的磁盘通过UI下电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	osd进程正常退出，集群有告警信息</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将三个物理磁盘通过UI上电，启动osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio稳定后，在node3上，使用命令踢盘的方式删除新添加的osd进程的磁盘，再用命令扫出磁盘，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	踢盘时：</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	osd进程正常退出，集群有告警信息</p>
<p>
	扫盘后：</p>
<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待fio稳定后，在node2上，使用命令和UI下电磁盘两种方式故障一个磁盘，重复步骤1到步骤3</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	磁盘故障时：</p>
<p>
	osd进程正常退出，集群有告警信息</p>
<p>
	有部分pg卡住，fio挂起</p>
<p>
	磁盘恢复后：</p>
<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio读写恢复，没有一致性问题</p>
<p>
	集群状态正常，pg平衡正常，最后pg平衡全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="742" name="PG平衡-IO压力增加">
	<node_order><![CDATA[39]]></node_order>
	<externalid><![CDATA[149]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，新增加10条流的读写（或增加client），iodepth 256，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群整体的iops，带宽有提升，osd进程正常，集群状态正常，pg平衡正常，性能稳定无较大的波动</p>
<p>
	最终pg平衡全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="744" name="PG平衡-管理平面操作">
	<node_order><![CDATA[40]]></node_order>
	<externalid><![CDATA[150]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，用脚本大批量创建，删除pool、rbd、快照并修改属性，用脚本反复查看集群状态、osd tree等信息，做大量管理平面的操作，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写无数据一致性问题</p>
<p>
	性能稍有衰减（iops衰减1k-2k），没有大范围波动</p>
<p>
	集群状态正常，osd进程正常</p>
<p>
	pg平衡过程最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="746" name="PG平衡-暂停集群">
	<node_order><![CDATA[41]]></node_order>
	<externalid><![CDATA[151]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，暂停osd，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	暂停成功，fio读写挂起</p>
<p>
	osd进程正常，平衡继续进行，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，取消暂停，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	取消暂停成功，fio继续下发读写，没有一致性问题</p>
<p>
	osd进程正常，pg平衡正常，集群状态正常</p>
<p>
	pg平衡最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="748" name="PG平衡-添加进程">
	<node_order><![CDATA[42]]></node_order>
	<externalid><![CDATA[152]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1为mon主机，1个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，在node1节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，在node2节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG平衡完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，在node2和node3上创建mon进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程创建成功，形成新的集群仲裁</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG平衡完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="750" name="PG平衡-删除进程">
	<node_order><![CDATA[43]]></node_order>
	<externalid><![CDATA[153]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon，创建5个pool，每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd启动fio读写，带一致性校验，在写的过程中，在node3上新添加3个osd进程到集群，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，登录node3节点，删除一个在平衡过程中的osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群有告警信息，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node1节点，删除一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群有告警信息，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node2节点，删除一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群有告警信息，pg平衡正常</p>
<p>
	最终pg平衡完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，删除两个mon进程（leader mon和非leader mon），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功，集群形成新的仲裁</p>
<p>
	fio不归零，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="752" name="PG平衡-rbd失效">
	<node_order><![CDATA[44]]></node_order>
	<externalid><![CDATA[154]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon。配置crush rule，创建两个osddomain，osddomain1和osddomain2上的osd在node1和node2上都有分布，osddomain1上创建pool1，pool2，crush rule 1，osddomain2上创建pool3，pool4，crush rule 2。每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd（rbd1属于pool1，rbd3属于pool3）启动fio读写，带一致性校验，在写的过程中，在node3上新添加2个osd到osddomain1，新添加另外两个osd到osddomain2，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，将osddomain1中的磁盘全部拔出，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	磁盘拔出后，相应的osd进程正常退出，集群有告警信息</p>
<p>
	osddomain1中的rbd读写挂起，pg平衡暂停</p>
<p>
	osddomain2中的rbd读写不受影响，没有数据一致性问题，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将osddomain1中的磁盘全部插回，启动osd进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	磁盘插回后，启动osd进程成功，加入集群成功</p>
<p>
	osddomain1中的rbd读写恢复，没有数据一致性问题，pg平衡开始</p>
<p>
	osddomain2中的rbd读写不受影响，没有数据一致性问题，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，将osddomain2中的磁盘全部拔出，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osddomain1中的rbd读写不受影响，没有数据一致性问题，pg平衡正常</p>
<p>
	磁盘拔出时：</p>
<p>
	osddomain2上的osd进程正常退出，集群有告警信息</p>
<p>
	rbd读写挂起，pg平衡暂停</p>
<p>
	磁盘插回后：</p>
<p>
	osddomain2上的osd进程启动成功，加入集群成功</p>
<p>
	rbd读写恢复，没有数据一致性问题，pg平衡开始</p>
<p>
	最终两个osddomain上的pg平衡全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="754" name="PG平衡-修改pool的crush rule">
	<node_order><![CDATA[45]]></node_order>
	<externalid><![CDATA[155]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，使用node1，node2，node3做mon主机，三个mon。配置crush rule，创建两个osddomain，osddomain1和osddomain2上的osd在node1和node2上都有分布，osddomain1上创建pool1，pool2，crush rule 1，osddomain2上创建pool3，pool4，crush rule 2。每个pool若干个rbd，对每个rbd 8k随机写，使得每个osd下使用100G的容量左右。对其中两个rbd（rbd1属于pool1，rbd3属于pool3）启动fio读写，带一致性校验，在写的过程中，在node3上新添加2个osd到osddomain1，新添加另外两个osd到osddomain2，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg平衡过程中，手动修改pool1的crush rule规则，由rule 1设置为rule 2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain2的osd上</p>
<p>
	pg平衡过程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待pool1的pg全部迁移到osddomain2的osd上后，在pg平衡过程中，手动修改pool1的crush rule规则，由rule 2设置为rule 1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain1的osd上</p>
<p>
	pg平衡过程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pg平衡过程中，设置pool3的crush rule规则，重复步骤1到2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	由rule 2设置为rule 1：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain1的osd上</p>
<p>
	pg平衡过程正常</p>
<p>
	由rule 1设置为rule 2：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain2的osd上</p>
<p>
	pg平衡过程正常，最终pg平衡全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="756" name="大规模" >
<node_order><![CDATA[7]]></node_order>
<details><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
<p>
	使用两台tahoe组成大规模集群进行测试，其中副本是按照tahoe来分布或者按host分布（2副本）。</p>
<p>
	所有的image均使用nbd方式导出，使用多个client进行测试。</p>
<p>
	由于要组织crush tree，UI暂不支持，创建完集群后，需要手动调整crush tree的组织形式。</p>
<p>
	两台tahoe的硬盘最好是平均分配，这样更好的平衡数据。</p>
]]></details>
<testsuite id="7445" name="副本按host分布" >
<node_order><![CDATA[1]]></node_order>
<details><![CDATA[<p>
	两个副本，副本按照host来分布，有可能同一个数据的两个副本在同一个tahoe上</p>
]]></details>

<testcase internalid="7467" name="创建集群-含两个tahoe">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[464]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	使用脚本创建集群，一次创建集群osd节点包含两台tahoe 6个host，mon节点选取其中5个host</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，使用nbd方式导出，在client端创建文件系统或裸盘，使用fio进行读写10分钟，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写没有问题，无一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将集群删除，使用脚本创建集群，使用tahoe1a,tahoe2b,tahoe2c节点创建osd节点, 使用tahoe1b,tahoe1c,tahoe2a节点创建mon节点</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，使用nbd方式导出，在client端创建文件系统或裸盘，使用fio进行读写10分钟，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写没有问题，无一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将集群删除，使用脚本创建集群，使用tahoe1的三个节点创建osd节点，使用tahoe2的三个节点创建mon节点</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，使用nbd方式导出，在client端创建文件系统或裸盘，使用fio进行读写10分钟，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写没有问题，无一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将集群删除，使用UI界面创建集群，包含两个tahoe1，tahoe2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，使用nbd方式导出，在client端创建文件系统或裸盘，使用fio进行读写10分钟，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写没有问题，无一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7469" name="创建集群-扩展到两个tahoe">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[465]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1上已经创建了集群，包括三个osd节点，三个mon节点</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，nbd方式导出到client端，创建文件系统和裸盘，使用fio进行读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，image，nbd成功</p>
<p>
	fio读写无一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	调整pool的pg，pgp数量（根据要增加的osd数量来增加），待集群恢复OK后，通过脚本，将tahoe2a节点创建osd后添加到集群，使得osd节点有4个，每个节点上的osd数量基本一致</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	增加pg，pgp成功</p>
<p>
	添加新的osd节点成功</p>
<p>
	有数据的迁移，fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复OK后，调整pool的pg，pgp数量（根据要增加的osd数量来增加），待集群恢复OK后，通过脚本，将tahoe2b节点创建osd后添加到集群，使得osd节点有5个，每个节点上的osd数量基本一致</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	增加pg，pgp成功</p>
<p>
	添加新的osd节点成功</p>
<p>
	有数据的迁移，fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复OK后，调整pool的pg，pgp数量（根据要增加的osd数量来增加），待集群恢复OK后，通过脚本，将tahoe2c节点创建osd后添加到集群，使得osd节点有5个，每个节点上的osd数量基本一致</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	增加pg，pgp成功</p>
<p>
	添加新的osd节点成功</p>
<p>
	有数据的迁移，fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	使用UI来操作步骤2到4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7471" name="增加monitor">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[466]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，三个mon节点全部都在tahoe1上</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，nbd导出到client端，创建文件系统和裸盘，用fio进行读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，image，nbd导出成功</p>
<p>
	fio读写无一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在tahoe2a，tahoe2b节点上创建mon，加入集群，使得集群有5个mon</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建mon成功，集群仲裁成功</p>
<p>
	fio读写无一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在UI上重新操作步骤2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7473" name="删除monitor">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[467]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，5个mon节点</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，nbd导出到client端，创建文件系统和裸盘，用fio进行读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，image，nbd导出成功</p>
<p>
	fio读写无一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，删除leader mon（tahoe1a例如）</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除mon成功</p>
<p>
	fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在tahoe2上选择一个mon节点删除</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除mon成功，节点只有3个mon，集群仲裁正常</p>
<p>
	fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在UI界面上重复步骤2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7475" name="public网络故障">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[468]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，物理拔出tahoe1a的public线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1a的osd，mon进程不退出</p>
<p>
	tahoe1a的osd在crush上标记为down，leader mon切换</p>
<p>
	fio归零在30秒以内恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，插回线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1a的osd重新标记为up，该节点重新变为leader mon</p>
<p>
	fio读写正常，集群有数据的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe1a上通过ifdown，ifup命令中断恢复public线缆，重做步骤1到2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	注意：osd，mon进程是不退出的。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，物理拔出tahoe2的所有节点的public线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的三个节点的所有osd，mon进程不退出</p>
<p>
	有pg卡住，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，插回tahoe2的所有public线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd重新标记为up，mon为三个，形成仲裁</p>
<p>
	pg状态恢复，集群为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	用ifdown，ifup命令重做步骤4到5</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	注意：osd，mon进程是不退出的</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，物理拔出tahoe1，tahoe2的所有public线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1，tahoe2上的osd，mon进程不退出</p>
<p>
	集群状态查询不到，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，插回tahoe1，tahoe2的所有public线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复为OK</p>
<p>
	fio恢复读写，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	用ifdown，ifup命令重做步骤7,8</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	注意：osd，mon进程不退出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7477" name="cluster网络故障">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[469]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程，物理拔出tahoe1a的cluster线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1a的osd，mon进程不退出</p>
<p>
	tahoe1a的osd在crush上标记为down，leader mon不切换</p>
<p>
	fio归零在30秒以内恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，插回线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1a的osd重新标记为up</p>
<p>
	fio读写正常，集群有数据的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe1a上通过ifdown，ifup命令中断恢复cluster线缆，重做步骤1到2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	注意：osd，mon进程是不退出的。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，物理拔出tahoe2的所有节点的cluster线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的三个节点的所有osd，mon进程不退出</p>
<p>
	有pg卡住，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，插回tahoe2的所有cluster线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd重新标记为up</p>
<p>
	pg状态恢复，集群为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	用ifdown，ifup命令重做步骤4到5</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	注意：osd，mon进程是不退出的</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，物理拔出tahoe1，tahoe2的所有cluster线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1，tahoe2上的osd，mon进程不退出</p>
<p>
	集群状态查询不到，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，插回tahoe1，tahoe2的所有cluster线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复为OK</p>
<p>
	fio恢复读写，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	用ifdown，ifup命令重做步骤7,8</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	注意：osd，mon进程不退出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7479" name="全部网络故障">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[470]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，物理拔出tahoe1，tahoe2上的所有节点的public网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有osd，mon进程不退出</p>
<p>
	集群不可用，无法查询到状态</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，恢复tahoe1，tahoe2上的所有节点的public网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	使用ifdown，ifup命令方式断开所有节点的public网口，重复步骤1，2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	注意：osd，mon进程不退出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复OK后，物理拔出tahoe1，tahoe2上的所有节点的cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有osd，mon进程不退出</p>
<p>
	集群不可用</p>
<p>
	fio挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，恢复tahoe1，tahoe2上的所有节点的cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，使用ifdown，ifup命令方式断开所有节点的cluester网口，重复步骤4,5</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	注意：osd，mon进程不退出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，物理拔出tahoe1，tahoe2上所有节点的public，cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有osd，mon进程不退出</p>
<p>
	集群不可用</p>
<p>
	fio挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，恢复所有网络</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	通过掉电public、cluster网络交换机来重复上面网络故障步骤</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7483" name="节点重启">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[472]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1a节点reboot</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1a上的osd标记为down，该节点的mon退出仲裁，集群选举出新的leader mon</p>
<p>
	fio读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复OK后，将tahoe1a，tahoe1b，tahoe1c三个节点同时reboot</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	tahoe1a，tahoe1b，tahoe1c 系统恢复后，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程自动启动</p>
<p>
	集群恢复状态为OK</p>
<p>
	fio读写继续下发，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，将tahoe1，tahoe2上的所有节点同时reboot</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	tahoe1，tahoe2启动后，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程自动启动</p>
<p>
	集群为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7485" name="节点panic">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[473]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe2b节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2b节点的osd标记为down</p>
<p>
	fio读写短暂归零（30秒内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	tahoe2b启动后，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd自动启动，加入集群</p>
<p>
	集群恢复为OK</p>
<p>
	fio读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复OK后，将tahoe2的三个节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用，为ERR状态，有pg卡住</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	tahoe2的三个节点启动后，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程自动启动，加入集群</p>
<p>
	集群恢复为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，将tahoe1，tahoe2的所有节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	tahoe1，tahoe2节点恢复后，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程自动启动</p>
<p>
	集群恢复为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7487" name="tahoe整体掉电">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[474]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将两台tahoe都拔出电源</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	fio挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将两台tahoe同时上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	上电后，每个节点的osd，mon进程自动启动</p>
<p>
	集群恢复为OK状态</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，将两台tahoe都拔出电源</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	fio挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将tahoe1上电，10分钟后，将tahoe2上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1上电后，自动启动所有osd，mon进程，集群状态不可用，fio挂起</p>
<p>
	tahoe2上电后，自动启动所有osd，mon进程，集群状态恢复为OK，fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7489" name="kill 进程">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[475]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7491" name="压力和长时间测试">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[476]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7497" name="高负载">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[478]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7493" name="crush rule 变换">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[477]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7525" name="PG迁移-网络中断">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[480]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7527" name="PG迁移-主机panic">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[481]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7529" name="PG迁移-磁盘故障">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[482]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7531" name="PG恢复-网络中断">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[483]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7535" name="PG恢复-主机panic">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[484]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7538" name="PG恢复-磁盘故障">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[485]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7540" name="PG平衡-网络中断">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[486]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7543" name="PG平衡-主机panic">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[487]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7545" name="PG平衡-磁盘故障">
	<node_order><![CDATA[23]]></node_order>
	<externalid><![CDATA[488]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>

<testcase internalid="7986" name="tahoe磁盘不均匀">
	<node_order><![CDATA[24]]></node_order>
	<externalid><![CDATA[540]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>

</testcase>
</testsuite><testsuite id="7615" name="副本按tahoe分布" >
<node_order><![CDATA[1]]></node_order>
<details><![CDATA[<p>
	两个副本，副本按照tahoe来分布，同一个tahoe上只保留数据的单个副本</p>
]]></details>

<testcase internalid="7616" name="创建集群-含两个tahoe">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[513]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上有三个host，每个host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建集群，包括3个mon节点，6个osd节点，双副本，副本按照tahoe分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群创建成功，crush tree组织正常，osd权重正常，集群可用容量正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，nbd方式导出到client端，创建文件系统和裸盘，进行文件系统和裸盘读写10分钟，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，image成功</p>
<p>
	nbd读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7618" name="创建集群-扩展到两个tahoe">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[514]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上有三个host，每个host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	初始化集群，创建集群的时候使用tahoe1的三个节点做mon和osd，副本按照host分布，创建pool，创建imge，nbd导出后，进行读写，待一致性校验，每个osd写入30G左右的数据量</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建集群成功，写入数据正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，添加tahoe2的三个节点做为osd主机到集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	添加tahoe2的节点成功</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	集群有数据的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复OK后，在读写过程中，将所有pool的crush配置成副本按tahoe分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有数据的迁移，最后集群恢复为OK</p>
<p>
	读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7620" name="增加monitor">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[515]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，三个mon节点全部都在tahoe1上</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在tahoe2a上新创建一个mon，加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建mon成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，tahoe2b上新创建一个mon，加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建mon成功，集群有5个mon</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将这5个mon 进程，kill -9杀死任意的两个</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群中有mon down，存活3个mon，集群仲裁正常</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将杀死的mon进程启动</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程启动后，加入集群成功，集群仲裁正常</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7622" name="删除monitor">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[516]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，6个osd节点，5个mon节点：tahoe1a,tahoe1b,tahoe1c,tahoe2a,tahoe2b</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1a（leader mon）mon删除</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon删除成功，集群选出新的leader mon，tahoe1b</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1b（leader mon）mon删除</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon删除成功，集群选出新的leader mon，tahoe1c，集群有三个mon</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe2b的mon进程stop</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群有mon down，形成新的仲裁</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将tahoe2b的mon进程启动</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7624" name="public网络故障">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[517]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe2的所有节点的public线缆拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2上的所有osd标记为down，tahoe2a的mon退出集群仲裁</p>
<p>
	io短暂归零（30秒以内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，把tahoe2的public线缆插回</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2上的所有osd，mon加入集群，集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	用ifdown，ifup的方式断开、连接public网络，重复步骤1，2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1，tahoe2的所有节点的的public线缆拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将tahoe1，tahoe2的所有节点的public线缆插回</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群恢复OK状态</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	用ifdown，ifup的方式断开、连接public网络，重复步骤4,5</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7626" name="cluster网络故障">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[518]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe2的所有节点的cluster线缆拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd进程标记为down，mon正常</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将tahoe的所有节点的cluster线缆插回</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2上的所有osd加入集群，集群状态OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	用ifdown、ifup方式中断、连接cluster网络，重复步骤1,2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1，tahoe2的所有节点的cluster线缆拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将tahoe1，tahoe2的所有节点的cluster线缆插回</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态为OK</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	用ifdown、ifup方式中断、连接cluster网络，重复步骤4,5</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7628" name="全部网络故障">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[519]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1，tahoe2的所有public，cluster线缆物理拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，将tahoe1，tahoe2的所有public，cluster线缆插回</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群恢复为OK</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	用ifdown、ifup的方式中断、连接tahoe1，tahoe2的public、cluster网络，重复步骤1,2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7630" name="节点重启">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[520]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe2的三个节点同时reboot</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2上的所有osd标记为down，tahoe2a的mon退出集群仲裁</p>
<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待三个节点恢复后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd、mon进程自动启动，加入集群成功，集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1，tahoe2的所有节点同时reboot</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待6个节点恢复后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群osd、mon进程自动启动，集群状态为OK</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7632" name="节点panic">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[521]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe2的三个节点同时panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd标记为down，tahoe2a的mon退出集群仲裁</p>
<p>
	io读写短暂归零（30秒以内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待tahoe2的三个节点恢复后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd、mon进程自动启动，加入集群成功，状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1，tahoe2的所有节点同时panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待tahoe1，tahoe2的节点都恢复后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复为OK</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7634" name="tahoe整体掉电">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[522]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe2的电源拔出，整体掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd标记为down，tahoe2a的mon退出集群</p>
<p>
	io短暂归零（30秒以内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe2的电源插回重新上电，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd、mon自动启动，加入集群成功，状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1和tahoe2的电源都拔出，所有tahoe掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe1和tahoe2的电源插回重新上电，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1，tahoe2的osd、mon自动启动，集群状态为OK</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7636" name="kill 进程">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[523]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中将tahoe2上三个节点的所有osd、mon进程关闭</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2上的osd标记为down，tahoe2a的mon退出集群</p>
<p>
	io短暂归零（30秒以内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将tahoe2上的所有osd、mon进程开启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2上的osd、mon进程启动后加入集群，集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe1，tahoe2上的所有osd、mon进程关闭</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将tahoe1，tahoe2上的所有osd、mon进程开启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群恢复OK</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7638" name="压力和长时间测试">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[524]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	接入多台client，每个client端都有若干个nbd，创建文件系统或者裸盘，进行随机读写或者顺序读写等数据模型。进行长时间测试，使得server端的压力cpu空闲30%-40%左右</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	没有os重启，死机的现象</p>
<p>
	BMC串口没有打印异常</p>
<p>
	没有温度告警</p>
<p>
	osd、mon进程没有崩溃</p>
<p>
	没有数据不一致</p>
<p>
	没有内存溢出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7642" name="高负载">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[526]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	接入多台client，每个client端都有若干个nbd，创建文件系统或者裸盘，进行随机读写或者顺序读写等数据模型。进行一晚上12个小时，使得server端的cpu几乎没有什么空闲状态，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	没有os重启，死机的现象</p>
<p>
	BMC串口没有打印异常</p>
<p>
	没有温度告警</p>
<p>
	osd、mon进程没有崩溃</p>
<p>
	没有数据不一致</p>
<p>
	没有内存溢出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7644" name="crush rule 变换">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[527]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，设置两种crush rule规则，rule1，副本按照tahoe来分布，rule2，副本按照host来分布</p>
<p>
	6，创建pool，2副本，副本按照tahoe来分布，创建image，用nbd导出到client端，创建文件系统和裸盘，进行fio读写，带一致性校验，每个osd上大概有100G的数据量分布</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，设置所有pool的crush rule规则为副本按照host来分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有数据的迁移，最后集群变为OK状态</p>
<p>
	io读写有性能短暂下降，然后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，在读写过程中，设置所有的pool的crush rule规则为副本按照tahoe来分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有数据的迁移，最后集群变为OK状态</p>
<p>
	io读写有性能短暂下降，然后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7646" name="PG迁移-网络中断">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[528]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，2副本，副本按照tahoe来分布，创建image，启动fio写数据进行预埋，每个osd上大概有100G的数据量分布</p>
<p>
	6，将image用nbd方式导出到client端，创建文件系统或者裸盘，启动读写，带一致性校验，在读写过程中，将tahoe2b上的所有osd进程out，有数据的迁移</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移的过程中，拔出tahoe2b节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
<p>
	tahoe2b上的所有osd被标记为down</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe2b上的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2b上的osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe2c节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd标记为down</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe2c上的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe1c节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd标记为down</p>
<p>
	io读写挂起，数据迁移挂起，有pg卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe1c节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd标记为up</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe2的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd标记为down，tahoe2a的mon退出集群仲裁</p>
<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性</p>
<p>
	数据迁移卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe2的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的osd标记为up，tahoe2a的mon加入集群仲裁</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe1的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe1的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe1，tahoe2的所有节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe1，tahoe2的所有节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常，最后全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7648" name="PG迁移-主机panic">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[529]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，2副本，副本按照tahoe来分布，创建image，启动fio写数据进行预埋，每个osd上大概有100G的数据量分布</p>
<p>
	6，将image用nbd方式导出到client端，创建文件系统或者裸盘，启动读写，带一致性校验，在读写过程中，将tahoe2b上的所有osd进程out，有数据的迁移</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移的过程中，将tahoe2b节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
<p>
	tahoe2b上的所有osd被标记为down</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	tahoe2b节点启动后，将tahoe2b节点的所有osd进行out，有数据的迁移</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2b上的osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe2c节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd标记为down</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	tahoe2c节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd自动启动，并标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe1c节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd标记为down</p>
<p>
	io读写挂起，数据迁移挂起，有pg卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	tahoe1c节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd自动启动，并标记为up</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe2的三个节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd标记为down，tahoe2a的mon退出集群仲裁</p>
<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性</p>
<p>
	数据迁移卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	tahoe2的节点启动后，将tahoe2b上的osd手动out，有数据的迁移</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的osd、mon自动启动，并标记为up，tahoe2a的mon加入集群仲裁</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe1的三个节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	tahoe1的节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	tahoe1，tahoe2的所有节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	tahoe1，tahoe2的节点启动后，将tahoe2b的osd手动out，有数据的迁移</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常，最后全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7650" name="PG迁移-磁盘故障">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[530]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，2副本，副本按照tahoe来分布，创建image，启动fio写数据进行预埋，每个osd上大概有100G的数据量分布</p>
<p>
	6，将image用nbd方式导出到client端，创建文件系统或者裸盘，启动读写，带一致性校验，在读写过程中，将tahoe2b上的所有osd进程out，有数据的迁移</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，在tahoe2b节点上逻辑踢出5个磁盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd进程退出，标记为down</p>
<p>
	io短暂归零（30秒内）后恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe2b的盘重新扫出，启动这5个osd进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，在tahoe2c节点上逻辑踢出5个磁盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd进程退出，标记为down</p>
<p>
	io短暂归零（30秒内）后恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe2c的盘重新扫出，启动这5个osd进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移的过程中，在tahoe1c节点上逻辑踢出5个磁盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd进程退出，标记为down</p>
<p>
	io读写挂起，数据迁移挂起，有pg卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe1c节点的盘重新扫出，启动这5个osd进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常，最后全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7652" name="PG恢复-网络中断">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[531]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，2副本，副本按照tahoe来分布，创建image，启动fio写数据进行预埋，每个osd上大概有100G的数据量分布</p>
<p>
	6，将image用nbd方式导出到client端，创建文件系统或者裸盘，启动读写，带一致性校验，在读写过程中，登录UI界面将tahoe2b的所有盘都下电，并且手动将tahoe2b上的osd全部out，有数据的迁移</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移的过程中，拔出tahoe2b节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后插回tahoe2b节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe2c节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd标记为down</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe2c上的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe1c节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd标记为down</p>
<p>
	io读写挂起，数据迁移挂起，有pg卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe1c节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd标记为up</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe2的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd标记为down，tahoe2a的mon退出集群仲裁</p>
<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性</p>
<p>
	数据迁移卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe2的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的osd标记为up，tahoe2a的mon加入集群仲裁</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe1的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe1的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe1，tahoe2的所有节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe1，tahoe2的所有节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常，最后全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7654" name="PG恢复-主机panic">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[532]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，2副本，副本按照tahoe来分布，创建image，启动fio写数据进行预埋，每个osd上大概有100G的数据量分布</p>
<p>
	6，将image用nbd方式导出到client端，创建文件系统或者裸盘，启动读写，带一致性校验，在读写过程中，登录UI界面将tahoe2b的所有盘都下电，并且手动将tahoe2b上的osd全部out，有数据的迁移</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移的过程中，将tahoe2b节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	tahoe2b节点启动后，将tahoe2b节点的所有盘都下电，手动将所有osd进行out，有数据的迁移</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2b上的osd标记为down</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe2c节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd标记为down</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	tahoe2c节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd自动启动，并标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe1c节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd标记为down</p>
<p>
	io读写挂起，数据迁移挂起，有pg卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	tahoe1c节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd自动启动，并标记为up</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe2的三个节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd标记为down，tahoe2a的mon退出集群仲裁</p>
<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性</p>
<p>
	数据迁移卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	tahoe2的节点启动后，将tahoe2b上的所有SSD下电，osd手动out，有数据的迁移</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的osd、mon自动启动，并标记为up，tahoe2a的mon加入集群仲裁</p>
<p>
	tahoe2b的SSD下电后，tahoe2b上的osd进程退出，标记为down</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe1的三个节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	tahoe1的节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	tahoe1，tahoe2的所有节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	tahoe1，tahoe2的节点启动后，将tahoe2b的osd手动out，有数据的迁移</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常，最后全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7656" name="PG恢复-磁盘故障">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[533]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a）</p>
<p>
	5，创建pool，2副本，副本按照tahoe来分布，创建image，启动fio写数据进行预埋，每个osd上大概有100G的数据量分布</p>
<p>
	6，将image用nbd方式导出到client端，创建文件系统或者裸盘，启动读写，带一致性校验，在读写过程中，登录UI界面将tahoe2b的所有盘都下电，并且手动将tahoe2b上的osd全部out，有数据的迁移</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，在tahoe2c节点上逻辑踢出5个磁盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd进程退出，标记为down</p>
<p>
	io短暂归零（30秒内）后恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe2c的盘重新扫出，启动这5个osd进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移的过程中，在tahoe1c节点上逻辑踢出5个磁盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd进程退出，标记为down</p>
<p>
	io读写挂起，数据迁移挂起，有pg卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe1c节点的盘重新扫出，启动这5个osd进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常，最后全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7658" name="PG平衡-网络中断">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[534]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a），tahoe2b节点的osd已经out完毕</p>
<p>
	5，创建pool，2副本，副本按照tahoe来分布，创建image，启动fio写数据进行预埋，每个osd上大概有100G的数据量分布</p>
<p>
	6，将image用nbd方式导出到client端，创建文件系统或者裸盘，启动读写，带一致性校验，在读写过程中，将tahoe2b上的所有osd in回集群，有数据的迁移</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移的过程中，拔出tahoe2b节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性问题</p>
<p>
	数据迁移卡住</p>
<p>
	tahoe2b上的所有osd被标记为down</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe2b上的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2b上的osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe2c节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd标记为down</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移卡住</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe2c上的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe1c节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd标记为down</p>
<p>
	io读写挂起，数据迁移挂起，有pg卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe1c节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd标记为up</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe2的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd标记为down，tahoe2a的mon退出集群仲裁</p>
<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性</p>
<p>
	数据迁移卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe2的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的osd标记为up，tahoe2a的mon加入集群仲裁</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe1的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe1的三个节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，拔出tahoe1，tahoe2的所有节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，插回tahoe1，tahoe2的所有节点的public网线和cluster网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常，最后全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7660" name="PG平衡-主机panic">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[535]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a），tahoe2b节点的osd已经out完毕</p>
<p>
	5，创建pool，2副本，副本按照tahoe来分布，创建image，启动fio写数据进行预埋，每个osd上大概有100G的数据量分布</p>
<p>
	6，将image用nbd方式导出到client端，创建文件系统或者裸盘，启动读写，带一致性校验，在读写过程中，将tahoe2b上的所有osd in回集群，有数据的迁移</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移的过程中，将tahoe2b节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
<p>
	tahoe2b上的所有osd被标记为down</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	tahoe2b节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2b上的osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe2c节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd标记为down</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	tahoe2c节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2c上的osd自动启动，并标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe1c节点panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd标记为down</p>
<p>
	io读写挂起，数据迁移挂起，有pg卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	tahoe1c节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe1c上的osd自动启动，并标记为up</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe2的三个节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的所有osd标记为down，tahoe2a的mon退出集群仲裁</p>
<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性</p>
<p>
	数据迁移卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	tahoe2的节点启动后，将tahoe2b上的osd手动out，有数据的迁移</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	tahoe2的osd、mon自动启动，并标记为up，tahoe2a的mon加入集群仲裁</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，将tahoe1的三个节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	tahoe1的节点启动后，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	tahoe1，tahoe2的所有节点都panic</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	数据迁移挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	tahoe1，tahoe2的节点启动后，将tahoe2b的osd手动out，有数据的迁移</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可用</p>
<p>
	io读写恢复，没有数据一致性问题</p>
<p>
	数据迁移正常，最后全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7662" name="PG平衡-磁盘故障">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[536]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/-2%20tahoe.PNG" style="width: 496px; height: 252px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，两台tahoe通过40G 交换机组成public，cluster网络环境</p>
<p>
	2，每个tahoe上的host的SSD盘均匀</p>
<p>
	3，两个tahoe上ceph版本一致</p>
<p>
	4，在tahoe1，tahoe2上已经创建了集群，包括6个osd节点，3个mon节点（tahoe1a（leader mon），tahoe1b，tahoe2a），tahoe2b节点的osd已经out完毕</p>
<p>
	5，创建pool，2副本，副本按照tahoe来分布，创建image，启动fio写数据进行预埋，每个osd上大概有100G的数据量分布</p>
<p>
	6，将image用nbd方式导出到client端，创建文件系统或者裸盘，启动读写，带一致性校验，在读写过程中，将tahoe2b上的所有osd in回集群，有数据的迁移</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，在tahoe2b节点上逻辑踢出5个磁盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd进程退出，标记为down</p>
<p>
	io短暂归零（30秒内）后恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe2b的盘重新扫出，启动这5个osd进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中，在tahoe2c节点上逻辑踢出5个磁盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd进程退出，标记为down</p>
<p>
	io短暂归零（30秒内）后恢复，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe2c的盘重新扫出，启动这5个osd进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移的过程中，在tahoe1c节点上逻辑踢出5个磁盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd进程退出，标记为down</p>
<p>
	io读写挂起，数据迁移挂起，有pg卡住，需要osd恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，tahoe1c节点的盘重新扫出，启动这5个osd进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd标记为up</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	数据迁移正常，最后全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite></testsuite><testsuite id="757" name="边界值" >
<node_order><![CDATA[8]]></node_order>
<details><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
<p>
	主要关注在cpu，内存，网络带宽，pool，rbd等规格测试，以及接近物理极限时的集群响应</p>
]]></details>

<testcase internalid="915" name="创建极限数量osd">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[164]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，同时这三个主机也是mon主机，3个mon。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	三台主机均插满硬盘，按照每个硬盘创建5个osd的标准，3个mon，创建集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建集群成功，创建osd成功，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建pool，2副本，创建rbd，fio读写30分钟，带一致性校验，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写没有数据一致性问题，性能稳定，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，长时间中断node1的public网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down，选出新的leader mon</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，长时间中断node1的cluster网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，将主机node1异常掉电，然后再上电，启动所有osd进程，mon进程，观察集群，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1掉电后，io能够在30秒内恢复，该节点上的osd全部标记为down，集群仲裁选出新的leader mon</p>
<p>
	node1上电后，启动osd进程mon进程成功，osd全部标记为up，该mon重新变为leader mon，io短暂下降后恢复正常。</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="905" name="创建极限数量pool">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[159]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，数量为32个，副本数为2（pg根据osd数量计算）</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool创建成功，pg分布成功</p>
<p>
	ceph osd df</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在pool上面随机创建若干个rbd（选择id最小的pool，id最大的pool，id中间的pool），大小在10G到2T之间随机分配，启动fio对这些rbd读写，带一致性校验，运行半小时，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，fio读写没有问题，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，长时间中断node1的public网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down，选出新的leader mon</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，长时间中断node1的cluster网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down，选出新的leader mon</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，将主机node1异常掉电，然后再上电，启动所有osd进程，mon进程，观察集群，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1掉电后，io能够在30秒内恢复，该节点上的osd全部标记为down，集群仲裁选出新的leader mon。</p>
<p>
	node1上电后，启动osd进程mon进程成功，osd全部标记为up，该mon重新变为leader mon，io短暂下降后恢复正常。</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="907" name="创建极限数量image">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[160]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个pool，pool1，pool2，双副本，每个pool的pg根据osd数量计算，在每个pool中创建1024个rbd，一共2048个rbd ，随机对其中的10个rbd进行读写（包含最先创建的rbd，最后创建的rbd，中间创建的rbd），读写带一致性校验，运行半小时，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建rbd成功，fio读写没有数据一致性问题，集群状态OK，无告警</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，长时间中断node1的public网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down，选出新的leader mon</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，长时间中断node1的cluster网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，将主机node1异常掉电，然后再上电，启动所有osd进程，mon进程，观察集群，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1掉电后，io能够在30秒内恢复，该节点上的osd全部标记为down，集群仲裁选出新的leader mon</p>
<p>
	node1上电后，启动osd进程mon进程成功，osd全部标记为up，该mon重新变为leader mon，io短暂下降后恢复正常。</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="909" name="物理cpu极限">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[161]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建若干个pool，每个pool中创建若干个image，启动fio,对这些image进行8k随机写，带校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool、images成功，fio读写没有问题，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	继续创建pool，images，增加client端，对新增加的images进行8k随机写，不带校验。直到server端的cpu利用率达到接近100%。运行半小时，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool、images成功，fio读写没有问题，server端cpu利用率达到100%后，运行fio写没有异常，os无异常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="911" name="物理带宽极限">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[162]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建30个pool，每个pool中创建1个image</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool、image创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在单个client端，使用fio同时对30个image，进行1M（或4M）大小的顺序写，带一致性校验，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	client端下发io正常，没有数据一致性问题</p>
<p>
	client物理链路达到理论带宽，性能无较大波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	增加一个client端，两个client端同时起多条流，进行1M（或4M）大小的顺序写，带一致性校验，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	两个client端下发io正常，没有数据一致性问题</p>
<p>
	client物理链路达到理论带宽，性能无较大波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	继续创建pool，images，继续增加client端，每个clietn端起多条流，进行1M（或4M）大小的顺序写，带一致性校验，将server端的物理链路带宽跑满为止，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有client端下发io正常，没有数据一致性问题</p>
<p>
	server物理链路达到理论带宽，性能无较大波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2到步骤4，将顺序写换成顺序读，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="913" name="物理内存极限">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[163]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。每个主机的内存按osd标配来配置（每个osd进程配置2G内存预算）</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建30个pool，每个pool中创建1个image</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool、image创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动fio，对每个image进行8K随机读写，带一致性校验，观察内存，观察io状态，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	内存使用持续增长，达到一定程度后，系统会自动释放一定空间，不会将所有内存全部耗尽，不存在内存泄漏</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="917" name="物理容量极限">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[165]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机上一个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建osd时指定block的使用容量，每个osd使用10G左右的容量，创建集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群创建成功，查看osd的可用容量正确</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，对该image，启动fio随机写，写的数据容量，超过pool的可用容量，观察io状态，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool、image创建成功</p>
<p>
	fio随机写，写到默认的阈值后，fio无法继续再下发io，集群有告警。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="919" name="创建删除超大image">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[166]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，双副本，在该pool中创建2T的rbd，创建完成后，对该rbd进行读写，带一致性校验，运行15分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，rbd创建成功，io正常，没有数据一致性问题</p>
<p>
	集群状态为OK，cpu，内存使用范围正常，性能稳定没有明显波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建4T的rbd，创建完成后，对该rbd进行读写，带一致性校验，运行15分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，io正常，没有数据一致性问题</p>
<p>
	集群状态为OK，cpu，内存使用范围正常，性能稳定没有明显波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	创建10T的rbd，创建完成后，对该rbd进行读写，带一致性校验，运行15分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功，io正常，没有数据一致性问题</p>
<p>
	集群状态为OK，cpu，内存使用范围正常，性能稳定没有明显波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	对2T,4T的image进行读写，10T的image停止读写，将10T的image删除，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	10T的image删除正常，删除完成后pool的可用容量释放10T左右</p>
<p>
	2T,4T的image读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="921" name="创建删除小容量image">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[167]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，双副本，在该pool中创建rbd1，大小为1G，对rbd进行读写，带一致性校验，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，rbd创建成功，读写没有问题，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在pool中创建rbd2，大小为100M，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pool中创建rbd3，大小为1M，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将rbd1，rbd2，rbd3删除，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除rbd成功，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="923" name="client端使用超大块读写">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[168]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建3个pool,副本数为2，每个pool创建3个rbd，每个rbd 100G左右大小，在fio上对这些rbd进行读写，块大小设置为1M，iodepth 256，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，rbd创建成功，io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	块大小设置为4M，iodepth 256和512，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	iodepth 256时，io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
<p>
	iodepth 512时，io有断断续续的现象（10G网络），没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	块大小设置为16M，iodepth 64和128，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	iodepth 64时，io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
<p>
	iodepth 128时，io有断断续续的现象（10G网络），没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="925" name="client端使用大iodepth读写">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[169]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建3个pool,副本数为2，每个pool创建3个rbd，每个rbd 100G左右大小，在fio上对这些rbd进行读写，块大小设置为8k，iodepth 256，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，rbd创建成功，io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	调整iodepth 512，进行读写，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	调整iodepth 1024，进行读写，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="927" name="client端高并发读写">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[170]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建32个pool，在每个pool中创建64个rbd image，一共2048个image，每个image在100G-500G不等</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool成功，创建image成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	配置fio job文件，在同一个client上同时对256个image进行随机写和随机读（如果client端cpu不够的话，适当调整每个image读写的bs，iodepth），持续30分钟，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	读写成功，没有异常，没有数据一致性问题。</p>
<p>
	有可能达到cpu，物理带宽等极限。内存使用率正常，集群状态正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	接入8台client到集群，每个client端对不同的256个image进行随机读写（如果client端cpu不够的话，适当调整每个image读写的bs，iodepth），8台client一共对2048个image进行读写，持续30分钟，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	读写成功，没有异常，没有数据一致性问题。</p>
<p>
	有可能达到cpu，物理带宽等极限。内存使用率正常，集群状态正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="929" name="反复快速重启osd进程">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[171]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个pool，在pool中创建若干个image</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，image成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动fio对两个pool中的image进行读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写成功，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登录node1，将某一个osd进程的pid找出，kill -9，杀死该进程，然后立即启动该osd进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	反复快速重启osd进程成功，osd进程无异常</p>
<p>
	没有数据一致性问题，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node2，将某一个osd进程的pid找出，kill -9，杀死该进程，然后立即启动该osd进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	反复快速重启osd进程成功，osd进程无异常</p>
<p>
	没有数据一致性问题，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node3，将某一个osd进程的pid找出，kill -9，杀死该进程，然后立即启动该osd进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	反复快速重启osd进程成功，osd进程无异常</p>
<p>
	没有数据一致性问题，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="931" name="反复快速重启mon进程">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[172]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个pool，在pool中创建若干个image</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，image成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动fio对两个pool中的image进行读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写成功，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登录node1，将leader mon进程的pid找出，kill -9，杀死该进程，然后立即启动mon进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程kill，重启成功</p>
<p>
	io正常，没有数据一致性问题</p>
<p>
	集群仲裁正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定，集群状态OK后，登录node2，将mon进程的pid找出，kill -9，杀死该进程，然后立即启动mon进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程kill，重启成功</p>
<p>
	io正常，没有数据一致性问题</p>
<p>
	集群仲裁正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待io稳定，集群状态OK后，登录node3，将mon进程的pid找出，kill -9，杀死该进程，然后立即启动mon进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程kill，重启成功</p>
<p>
	io正常，没有数据一致性问题</p>
<p>
	集群仲裁正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="933" name="反复快速修改pool size">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[173]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，两个pool，pool1副本数1，pool2副本数2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pool1，pool2中创建若干个image</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建image成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动fio对pool1，pool2中的image进行随机读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	读写正常，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，修改pool1的size，由1改为2,2改为3,3改为2,2改为1，快速重复该步骤3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改副本数成功</p>
<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，在读写过程中，修改pool2的size，由2改为3,3改为1,1改为3，3改为2，快速重复该步骤3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改副本数成功</p>
<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="935" name="创建pool时使用极限pg数量">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[174]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool1，使用大数量pg，使得每个osd上的pg数量在500左右。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool成功，查看ceph osd df，每个osd上分布的pg数量比较平均，大约500左右</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在pool1中创建若干个rbd image，对这些images进行fio读写，带一致性校验，运行10分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建images成功，fio读写正常，数据一致性没有问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1的public网络中断，3分钟后，插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆，fio能够及时恢复，归零30秒内，没有数据一致性问题</p>
<p>
	线缆插回后，osd状态能够及时恢复为up，fio读写没有一致性问题，性能短暂下降后恢复</p>
<p>
	集群状态恢复为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除pool1，创建pool2，使用大数量pg，使得每个osd上的osd上的pg数量在3000左右。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool成功，查看ceph osd df，每个osd上分布的pg数量比较平均，大约3000左右</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在pool2中创建若干个images，重复步骤2、3。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	结果同步骤2、3的期望结果</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="1383" name="NBD" >
<node_order><![CDATA[9]]></node_order>
<details><![CDATA[]]></details>
<testsuite id="1384" name="网络故障注入" >
<node_order><![CDATA[1]]></node_order>
<details><![CDATA[]]></details>

<testcase internalid="1389" name="客户端与Public Switch间物理闪断">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[175]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<table class="simple" style="background: rgb(238, 238, 238); border-width: thin; border-style: solid; border-color: black; font-size: 12px; margin: 5px; width: 910px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif;">
	<tbody>
		<tr>
			<th class="bold" colspan="6" style="padding: 5px 3px 4px 5px; vertical-align: top; overflow: hidden; color: rgb(21, 66, 139); font-stretch: normal; font-size: 11px; font-family: tahoma, arial, verdana, sans-serif; border-style: solid; border-color: rgb(153, 187, 232); line-height: 15px; background: url(&quot;../images/white-top-bottom.gif&quot;) 0px -1px repeat-x rgb(205, 222, 243); text-align: left;">
				前提</th>
		</tr>
		<tr>
			<td colspan="6" style="vertical-align: top;">
				<div style="margin: 0px; padding: 0px;">
					1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
				<div style="margin: 0px; padding: 0px;">
					2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
				<div style="margin: 0px; padding: 0px;">
					3，副本数为2</div>
				<div style="margin: 0px; padding: 0px;">
					4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
			</td>
		</tr>
	</tbody>
</table>
<p>
	&nbsp;</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，短时间中断恢复（5秒）client端的public网络线缆，观察集群状态，io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写短时间归零后恢复（30秒以内归零），没有数据一致性问题</p>
<p>
	集群状态正常</p>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	使用ifdown、ifup的方式重复步骤2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1393" name="服务器前端与Public Switch间物理闪断（mon与osd共存）">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[176]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	手动快速拔插node1服务器前端与Public Switch间物理链路，5秒内插回；观察io状态，集群状态，重复该步骤3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io短暂归零，归零时间在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	手动快速拔插node2服务器前端与Public Switch间物理链路，5秒内插回；观察io状态，集群状态，重复该步骤3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io短暂归零，归零时间在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	用ifdown、ifup的方式重复步骤2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1397" name="服务器后端与Cluster Switch间物理闪断（mon与osd共存）">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[177]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<table class="simple" style="background: rgb(238, 238, 238); border-width: thin; border-style: solid; border-color: black; font-size: 12px; margin: 5px; width: 910px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif;">
	<tbody>
		<tr>
			<th class="bold" colspan="6" style="padding: 5px 3px 4px 5px; vertical-align: top; overflow: hidden; color: rgb(21, 66, 139); font-stretch: normal; font-size: 11px; font-family: tahoma, arial, verdana, sans-serif; border-style: solid; border-color: rgb(153, 187, 232); line-height: 15px; background: url(&quot;../images/white-top-bottom.gif&quot;) 0px -1px repeat-x rgb(205, 222, 243); text-align: left;">
				前提</th>
		</tr>
		<tr>
			<td colspan="6" style="vertical-align: top;">
				<div style="margin: 0px; padding: 0px;">
					1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
				<div style="margin: 0px; padding: 0px;">
					2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
				<div style="margin: 0px; padding: 0px;">
					3，副本数为2</div>
				<div style="margin: 0px; padding: 0px;">
					4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
			</td>
		</tr>
	</tbody>
</table>
<p>
	&nbsp;</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	手动快速拔插node1服务器后端与Cluster Switch间物理链路，5秒内插回；观察io状态，集群状态，重复该步骤3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io短暂归零，归零时间在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	手动快速拔插node3服务器后端与Cluster Switch间物理链路，5秒内插回；观察io状态，集群状态，重复该步骤3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io短暂归零，归零时间在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	用ifdown、ifup的方式重复步骤2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1403" name="客户端与Public Switch间长时间物理断开">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[178]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，长时间中断client端的public网络线缆，5分钟后插回线缆，观察集群状态，io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	断开连接后，io一直归零，插回线缆后，io继续下发，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	用ifdown、ifup的方式重复步骤2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1407" name="服务器前端与Public Switch间长时间物理中断（mon与osd共存）">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[179]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	手动拔出node1服务器前端与Public Switch间物理链路，60秒后插回；观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io短暂归零，归零时间在30秒以内，没有数据一致性问题</p>
<p>
	链路插回后，能够恢复连接，集群状态恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	手动拔出node2服务器前端与Public Switch间物理链路，60秒后插回；观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io短暂归零，归零时间在30秒以内，没有数据一致性问题</p>
<p>
	链路插回后，能够恢复连接，集群状态恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	用ifdown、ifup的方式重复步骤2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1412" name="服务器后端与Cluster Switch间长时间物理中断（mon与osd共存）">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[180]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	手动拔出node1服务器后端与Cluster Switch间物理链路，60秒后插回；观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io短暂归零，归零时间在30秒以内，没有数据一致性问题</p>
<p>
	链路插回后，能够恢复连接，集群状态恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	手动拔出node3服务器后端与Cluster Switch间物理链路，60秒后插回；观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io短暂归零，归零时间在30秒以内，没有数据一致性问题</p>
<p>
	链路插回后，能够恢复连接，集群状态恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	用ifdown、ifup的方式重复步骤2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1417" name="网络全部物理中断（mon与osd共存）">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[181]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1，node2，node3的Public网络线缆全部拔出，3分钟后插回，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	线缆拔出后，集群状态不能查看，fio 读写挂起</p>
<p>
	线缆插回后，fio读写继续下发，没有数据一致性问题，集群状态可以查看并且状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定，集群状态正常后，在读写过程中，将node1，node2，node3的Cluster网络线缆全部拔出，3分钟后插回，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	线缆拔出后，集群状态不能查看，fio 读写挂起</p>
<p>
	线缆插回后，fio读写继续下发，没有数据一致性问题，集群状态可以查看并且状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	用ifdown、ifup的方式重复步骤2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1422" name="MTU设置（mon与osd共存）">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[182]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在交换机上设置开启巨型帧，在读写过程中，在client端设置MTU为9000，观察io状态，集群状态，运行5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在client端设置MTU为1200，观察io状态，集群状态，运行5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在client端设置MTU为1500，在server端node1，node2，node3上，修改public网络的MTU为9000，运行5分钟，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在server端node1，node2，node3上，修改Cluster网络的MTU为9000，运行5分钟，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在server端node1，node2，node3上，修改Cluster网络的MTU为1200，运行5分钟，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在server端node1，node2，node3上，修改Public网络的MTU为1200，运行5分钟，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将server端和client端的所有网络MTU都设置为9000，运行5分钟，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1432" name="server端修改ip（mon与osd共存）">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[183]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1的public网络修改ip，两种情况修改成同网段的ip，修改成不同网段的ip，观察io情况，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，node1节点的osd进程都在crush tree上标记为down，leader mon切换，形成新的仲裁。</p>
<p>
	io短暂归零后恢复，归零时间30秒以内，没有数据一致性问题</p>
<p>
	若修改成同网段的ip，osd进程正常退出，修改成不同网段的ip，osd进程不退出</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将node1上的所有osd，mon进程关闭，将public网络修改回原来的ip地址后，重启node1上的所有osd，mon进程，观察io情况，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改回原来的ip成功，重启osd，mon进程成功，crush tree上重新标记为up，node1上的mon重新变成leader mon</p>
<p>
	io性能短暂降低后恢复到较高水平，集群开始平衡数据，最后完毕，集群状态为OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复到OK后，在node1上修改Cluster网络的ip，重复步骤2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复到OK后，在node2上重复步骤2,3,4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="1385" name="服务故障注入" >
<node_order><![CDATA[2]]></node_order>
<details><![CDATA[]]></details>

<testcase internalid="1439" name="osd脱离、加入集群">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[184]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将一个osd进程out出集群，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	out过程正常，该osd下的pg全部迁移到别的osd上</p>
<p>
	io读写正常，没有数据一致性，最后集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将该osd进程关闭，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	进程关闭正常，该进程所占用的内存释放</p>
<p>
	io读写正常，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	手动启动该osd进程，并手动加入集群，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，成功加入集群，最后pg迁移成功，集群状态正常</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7955" name="osd out/in 交替">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[539]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	启动fio对image进行读写带一致性校验，在读写过程中，将node1上的osd.0进行out</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd.0 out正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	15秒后，在osd.0还在out的过程中，将osd.0 in</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd.0 in正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	15秒后，在osd.0还在in的过程中，将osd.0 out</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd.0 out正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2,3进行5遍</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复OK后，在node2，node3上针对某一个osd，重复步骤1,2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1441" name="关闭osd进程">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[185]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	登录一个server节点，一次性关闭3-5个osd进程，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程关闭正常，所占用的内存释放，集群告警</p>
<p>
	io短暂归零，30秒以内归零时间，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，重启关闭的osd进程，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动正常，重新加入集群，pg开始迁移，最后集群状态为OK</p>
<p>
	io性能短暂下降后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1443" name="关闭mon进程">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[186]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1节点的leader mon进程关闭，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程关闭成功，集群leader mon变换，形成新的集群仲裁</p>
<p>
	fio读写没有影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1节点的leader mon进程开启，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程开启成功，该mon重新变为leader mon，形成新的集群仲裁</p>
<p>
	fio读写没有影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1和node2的mon进程关闭，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程关闭后，集群状态不能查看</p>
<p>
	原来下发的fio读写不受影响，新的io能读写</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1和node2的mon进程开启，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程开启成功，形成集群仲裁，集群状态正常</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1445" name="kill osd进程">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[187]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	登录一个server节点，一次性kill -9 3-5个osd进程，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程kill之后，所占用的内存释放，集群告警</p>
<p>
	io短暂归零，30秒以内归零时间，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，重启kill的osd进程，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动正常，重新加入集群，pg开始迁移，最后集群状态为OK</p>
<p>
	io性能短暂下降后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1447" name="kill mon进程">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[188]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1节点的leader mon进程kill -9，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程kill 后，集群leader mon变换，形成新的集群仲裁</p>
<p>
	fio读写没有影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1节点的leader mon进程开启，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程开启成功，该mon重新变为leader mon，形成新的集群仲裁</p>
<p>
	fio读写没有影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1和node2的mon进程kill -9，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程kill后，集群状态不能查看</p>
<p>
	原来下发的fio读写不受影响，新的io能读写</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1和node2的mon进程开启，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程开启成功，形成集群仲裁，集群状态正常</p>
<p>
	io读写没有问题，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1449" name="kill 所有进程">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[189]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1，node2，node3上的所有osd进程，mon进程kill -9&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有进程被kill，osd所占的内存释放</p>
<p>
	io读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1，node2，node3上所有的osd，mon进程开启，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon，osd进程启动成功，集群状态正常</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1451" name="删除osd进程">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[190]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，使用脚本删除node1上的nvme0n1所对应的osd进程，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写性能降低，没有数据一致性问题</p>
<p>
	该osd进程删除成功，osd上的pg迁移到其他osd上，该osd进程在crush tree上out，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1453" name="添加osd进程">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[191]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1上的nvme0n1磁盘使用脚本创建osd进程。观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	在nvme0n1磁盘上创建osd进程成功，该osd进程加入集群，集群状态为OK</p>
<p>
	fio读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1455" name="暂停集群osd">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[192]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[2]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，暂停集群，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，不能够读写image</p>
<p>
	集群状态正常，显示为暂停</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，解除集群暂停，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写恢复，能够读写image，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="1457" name="节点故障注入" >
<node_order><![CDATA[3]]></node_order>
<details><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></details>

<testcase internalid="1458" name="server端重启（mon与osd共存）">
	<node_order><![CDATA[1000]]></node_order>
	<externalid><![CDATA[193]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1节点reboot重启，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点上的osd在crush tree上标记为down，node1上的leader mon退出仲裁，集群形成新的仲裁</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node1节点启动后，重启node1上的所有osd进程，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，osd重新标记为up，node1上的mon重新恢复为leader mon，数据平衡后，集群状态恢复为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，在读写过程中，将所有server节点node1，node2，node3都reboot，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有节点reboot后，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	node1，node2，node3启动成功后，启动server上所有osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	server端启动osd，mon进程成功，集群状态为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1460" name="server端掉电（mon与osd共存）">
	<node_order><![CDATA[1001]]></node_order>
	<externalid><![CDATA[194]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1节点掉电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点上的osd在crush tree上标记为down，node1上的leader mon退出仲裁，集群形成新的仲裁</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node1节点上电启动后，重启node1上的所有osd进程，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，osd重新标记为up，node1上的mon重新恢复为leader mon，数据平衡后，集群状态恢复为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，在读写过程中，将所有server节点node1，node2，node3都掉电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有节点掉电后，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	node1，node2，node3上电启动成功后，启动server上所有osd，mon进程，，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	server端启动osd，mon进程成功，集群状态为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1462" name="server端关机/开机（mon与osd共存）">
	<node_order><![CDATA[1002]]></node_order>
	<externalid><![CDATA[195]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1节点关机，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点上的osd在crush tree上标记为down，node1上的leader mon退出仲裁，集群形成新的仲裁</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node1节点开机启动后，重启node1上的所有osd进程，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，osd重新标记为up，node1上的mon重新恢复为leader mon，数据平衡后，集群状态恢复为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，在读写过程中，将所有server节点node1，node2，node3都关机，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有节点关机后，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	node1，node2，node3开机启动成功后，启动server上所有osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	server端启动osd，mon进程成功，集群状态为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1464" name="server端panic（mon与osd共存）">
	<node_order><![CDATA[1003]]></node_order>
	<externalid><![CDATA[196]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1节点panic，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点上的osd在crush tree上标记为down，node1上的leader mon退出仲裁，集群形成新的仲裁</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node1节点启动后，重启node1上的所有osd进程，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，osd重新标记为up，node1上的mon重新恢复为leader mon，数据平衡后，集群状态恢复为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，在读写过程中，将所有server节点node1，node2，node3都panic，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有节点panic后，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	node1，node2，node3启动成功后，启动server上所有osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	server端启动osd，mon进程成功，集群状态为OK</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1466" name="server端非NTP同步（mon与osd共存）">
	<node_order><![CDATA[1004]]></node_order>
	<externalid><![CDATA[197]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登陆node1节点，取消NTP配置，使用命令date修改node1主机的时间，比现在的时间快5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	该节点的osd在crush tree上标记为down，集群告警mon始终不一致，超过0.05秒</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	10分钟后，在node1设置NTP，调整时间与node2，node3的时间一致，误差不超过0.05秒，重启node1节点的osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	NTP设置成功，node1，node2，node3的时间一致</p>
<p>
	osd，mon进程启动成功，osd重新标记为up，mon变为leader mon，集群无时钟漂移告警信息，数据读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，调整node1节点的时间，慢5分钟，重复步骤2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在node2上重复步骤2，3，4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1666" name="client端重启">
	<node_order><![CDATA[1005]]></node_order>
	<externalid><![CDATA[218]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建4个image（30G），分别nbd导出到client端，nbd0，nbd1，nbd2，nbd3，将nbd0，nbd1创建文件系统（ext4，ext3），挂载目录/root/nbd0, /root/nbd1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	导出nbd正常，创建文件系统正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在client端启动fio读写，对/root/nbd0写文件，对裸盘nbd2随机写，剩余两个nbd不读写，reboot client端，查看nbd状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	reboot后，系统可以正常关闭和启动</p>
<p>
	系统启动后，查看nbd映射状态正常，没有丢失</p>
<p>
	系统启动后，文件系统需要再次mount，且mount成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1670" name="client端掉电">
	<node_order><![CDATA[1006]]></node_order>
	<externalid><![CDATA[219]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建4个image（30G），分别nbd导出到client端，nbd0，nbd1，nbd2，nbd3，将nbd0，nbd1创建文件系统（ext4，ext3），挂载目录/root/nbd0, /root/nbd1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	导出nbd正常，创建文件系统正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在client端启动fio读写，对/root/nbd0写文件，对裸盘nbd2随机写，剩余两个nbd不读写，client端拔出电源异常掉电，1分钟后插回电源上电，查看nbd状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	掉电再上电后，client端启动正常</p>
<p>
	系统启动后，查看nbd映射状态正常，没有丢失</p>
<p>
	系统启动后，文件系统需要再次mount，且mount成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1672" name="client端关机/开机">
	<node_order><![CDATA[1007]]></node_order>
	<externalid><![CDATA[220]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建4个image（30G），分别nbd导出到client端，nbd0，nbd1，nbd2，nbd3，将nbd0，nbd1创建文件系统（ext4，ext3），挂载目录/root/nbd0, /root/nbd1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	导出nbd正常，创建文件系统正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在client端启动fio读写，对/root/nbd0写文件，对裸盘nbd2随机写，剩余两个nbd不读写，shutdown 关闭client端，1分钟后，对client端开机，查看nbd状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	client端开机后，client端启动正常</p>
<p>
	系统启动后，查看nbd映射状态正常，没有丢失</p>
<p>
	系统启动后，文件系统需要再次mount，且mount成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1674" name="client端panic">
	<node_order><![CDATA[1008]]></node_order>
	<externalid><![CDATA[221]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建4个image（30G），分别nbd导出到client端，nbd0，nbd1，nbd2，nbd3，将nbd0，nbd1创建文件系统（ext4，ext3），挂载目录/root/nbd0, /root/nbd1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	导出nbd正常，创建文件系统正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在client端启动fio读写，对/root/nbd0写文件，对裸盘nbd2随机写，剩余两个nbd不读写，将client端panic，查看nbd状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	系统可以正常关闭和启动</p>
<p>
	系统启动后，查看nbd映射状态正常，没有丢失</p>
<p>
	系统启动后，文件系统需要再次mount，且mount成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="1468" name="磁盘故障注入" >
<node_order><![CDATA[4]]></node_order>
<details><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></details>

<testcase internalid="1469" name="逻辑踢盘">
	<node_order><![CDATA[1000]]></node_order>
	<externalid><![CDATA[198]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，启动fio对nbd1进行裸盘读写，启动fio对ext4文件系统进行读写，读写都带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，fio读写成功，没有数据一致性问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在node1上逻辑删除磁盘2个，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	踢出磁盘后，该磁盘上的osd进程退出，在crush tree上标记为down</p>
<p>
	io短暂归零（30秒以内），没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将踢出的磁盘重新加载，启动osd，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd成功，加入集群成功，最后集群状态为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，在node2，node3上重复步骤1,2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="1517" name="压力和长时间测试" >
<node_order><![CDATA[5]]></node_order>
<details><![CDATA[]]></details>

<testcase internalid="1518" name="长时间测试">
	<node_order><![CDATA[1000]]></node_order>
	<externalid><![CDATA[199]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常.</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在总结了每种IO模型下的性能指标值后，调用脚本（包含5种IO模型）对集群进行长时间IO测试，测试时间为一个测试周期。此时不关注性能，不对产品做任何故障操作。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	验证产品在不同IO模型的切换下，主机的CPU，内存，磁盘利用率和基准测试保持一致，同时在长时间测试下，系统不会Panic，产品不会宕机，业务运行正常，IO不会中断，OSD和Mon不会Down，内存不会泄露，数据一致性正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="6489" name="PG/Crush故障注入" >
<node_order><![CDATA[6]]></node_order>
<details><![CDATA[<p>
	Crush的不同配置下的fio读写，故障注入</p>
<p>
	PG属性的修改，PG状态的改变下的故障注入</p>
]]></details>

<testcase internalid="6490" name="创建pool，使用不同数量的pg">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[375]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，副本数为2</div>
<div>
	3，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建poo1，pg数量：pg 32，pgp 32，在pool1中创建image1，并用nbd方式导出，启动fio读写，带一致性校验，运行5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool1，image1创建成功，fio读写正常，没有大范围的波动，没有数据一致性问题，osd进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建poo2，pg数量：pg 2048，pgp 2048，在pool2中创建image2，并用nbd方式导出，启动fio读写，带一致性校验，运行5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool2，image2创建成功。fio读写正常，没有大范围的波动，没有数据一致性问题，osd进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	创建pool3，pg数量：pg 729，pgp 729，在pool3中创建image3，并用nbd方式导出，启动fio读写，带一致性校验，运行5分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool3，image3创建成功，pg数量不是2的幂次方，fio读写正常，没有大范围波动，没有数据一致性问题，osd进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	创建pool4，pg数量：pg 500000，pgp 500000</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool4无法创建成功，提示每个osd上最多分布32768个pg，该参数是默认值</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6496" name="创建大量pool">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[376]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，副本数为2</div>
<div>
	3，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建30个pool，每个pool中pg 数量动态计算（合理pg数），每个pool中创建一个image， 大小50G</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool成功，创建image成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将image用nbd方式导出到client，fio对每个image进行读写，每个image都写入30G的数据进行预埋</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	对其中两个image进行fio随机写，带一致性校验，在读写过程中，kill -9一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	kill进程后，io归零30秒恢复，集群有告警，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将kill的osd进程，out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	out操作成功，集群有pg迁移，最后变为OK状态，在迁移初期有短暂的性能降低，然后恢复到较高的水平，fio没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	启动osd进程，并添加回集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd进程成功，加入集群成功，有pg的迁移，在迁移初期有短暂的性能降低，然后恢复到较高水平，fio没有数据一致性问题，迁移完成后，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在有FIO读写，带一致性校验的情况下，将其中一台osd主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	掉电后，io归零在30秒以内，集群有告警，fio无数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	6分钟后，将该主机上电，启动osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	5分钟后，集群自动将该主机上的osd进程out出集群，fio性能短暂下降，PG迁移完成后，集群恢复到OK状态。主机上电后，启动osd进程成功，加入集群成功，PG有迁移恢复，最后为OK状态，fio无数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6505" name="在线扩展pg数量">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[377]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	fio对两个nbd进行读写（一个文件系统，一个裸盘），带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，增加1，观察集群状态，io状态，重复5次，每次增加1。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，增加20，观察集群状态，io状态，重复5次，每次增加20</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，增加100，观察集群状态，io状态，重复3次，每次增加100</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，修改pgp数量，增加1，观察集群状态，io状态，重复5次，每次增加1。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，有数据迁移，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，修改pgp数量，增加20，观察集群状态，io状态，重复5次，每次增加20</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，有数据迁移，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，修改pgp数量，增加100，观察集群状态，io状态，重复3次，每次增加100</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群正常，有数据迁移，fio正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6514" name="在线减少pg数量">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[378]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，副本数为2</div>
<div>
	3，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端用fio读写两个nbd（一个文件系统，一个裸盘），带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，减少1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	不能减少pg，提示错误</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，减少20，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	不能减少pg，提示错误</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pg数量，减少100，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	不能减少pg，提示错误</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pgp数量，减少1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态有告警，提示pgp比pg少</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pgp数量，减少20，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态有告警，提示pgp比pg少</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，只修改pgp数量，减少100，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态有告警，提示pgp比pg少</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6523" name="在线增加副本数">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[379]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，副本数为2</div>
<div>
	3，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool1，副本数为1，创建pool2，副本数为2，创建image：pool1/m1 &nbsp;pool1/m2 &nbsp;pool2/n1 &nbsp;pool2/n2。将4个image nbd导出client端，pool1/m1 &nbsp;pool2/n1创建文件系统，其他裸盘，启动fio对这4个nbd读写，带一致性校验。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，image创建成功</p>
<p>
	nbd导出成功</p>
<p>
	fio读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，对pool1设置副本数为2，对pool2设置副本数为3，观察集群状态，观察io状态，查看pg分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	副本增加后，有pg的分配和数据的迁移，fio性能短暂下降然后恢复到较高水平，集群恢复为OK状态，PG数量有增加，分布在不同的osd上，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6527" name="在线减少副本数">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[380]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool1，pool2，副本数为3，创建image：pool1/m1 &nbsp;pool1/m2 &nbsp;pool2/n1 &nbsp;pool2/n2。将4个image nbd导出client端，pool1/m1 &nbsp;pool2/n1创建文件系统，其他裸盘，启动fio对这4个nbd读写，带一致性校验。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，image创建成功</p>
<p>
	nbd导出成功</p>
<p>
	fio读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，设置两个pool的副本数为2，查看集群状态，io状态，pg分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	减少副本数后，pg数量减少，集群有再平衡，性能有短暂降低后恢复到稳定的高水平，比原来三副本的性能要高，集群状态为OK，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群状态为OK时，在读写过程中，设置两个pool的副本数为1，查看集群状态，io状态，pg分布</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	减少副本数后，pg数量减少，集群有再平衡，性能有短暂降低后恢复到稳定的高水平，比原来两副本的性能要高，集群状态为OK，io没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6532" name="在线修改pool的crush rule">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[381]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，rule规则为1，在domain2上创建pool3，pool4，rule规则为2。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在fio读写过程中，手动修改pool1的crush rule规则，由rule 1设置为rule 2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain2的osd上，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待pool1的pg全部迁移到osddomain2的osd后，在读写过程中，手动修改pool1的crush rule规则，由rule 2设置为rule 1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain1的osd上，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在fio读写过程中，手动修改pool3的crush rule规则，由rule 2设置为rule 1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain1的osd上，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待pool3的pg全部迁移到osddomain1的osd后，在读写过程中，手动修改pool3的crush rule规则，由rule 1设置为rule 2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain2的osd上，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6539" name="PG迁移-kill 进程">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[382]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将osd.0,osd.1进程kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题，集群有告警，有PG的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动osd.0,osd.1进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，fio归零30秒以内，没有数据一致性问题，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio恢复到稳定状态后，将node1上的另外一个osd.2进程kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后启动osd.2进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，fio归零30秒以内，没有数据一致性问题，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待fio恢复到稳定状态后，在node2上kill -9一个osd进程，一分钟后启动该进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒内，没有数据一致性问题，有pg的迁移。</p>
<p>
	若是负责迁移的osd被kill，则集群会有pg卡住，io挂起，该osd恢复后，pg恢复正常，io恢复。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	待fio恢复到稳定状态后，在node3上kill -9一个osd进程，一分钟后启动该进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒内，没有数据一致性问题，有pg的迁移。</p>
<p>
	若是负责迁移的osd被kill，则集群会有pg卡住，io挂起，该osd恢复后，pg恢复正常，io恢复。</p>
<p>
	最后pg迁移完成，集群状态为OK。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6547" name="PG迁移-主机异常掉电">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[383]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将node1主机异常掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有一致性问题，集群有告警信息，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node1节点上电，启动该节点上的所有osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd启动成功，加入集群成功，性能短暂降低后恢复到较高水平并稳定，读写没有一致性问题，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio读写稳定后，将node2主机异常掉电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将node2节点上电，启动该节点上的所有osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd启动成功，加入集群成功，性能短暂降低后恢复到较高水平并稳定，读写没有一致性问题，有pg的迁移，最后迁移全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6553" name="PG迁移-网络中断">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[384]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断node1节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断node1节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在node2节点上重复操作步骤1和步骤2，短时间中断网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断node1节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断node1节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在node2节点上重复操作步骤4和步骤5，长时间中断网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	长时间中断后，有pg卡住，出现stuck inactive的状态，io挂起</p>
<p>
	连接恢复后，io恢复，没有数据一致性问题，pg状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的public网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的cluster网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，pg迁移正常</p>
<p>
	最后pg迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6563" name="PG迁移-IP变更">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[385]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，修改node1主机上public网络的ip（包含两种情况，修改成同网段的ip，修改成不同网段的ip）</p>
<p>
	观察集群状态，io状态</p>
<p>
	1分钟后，修改回原来的ip，启动该节点的osd进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node1主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，修改node1主机上的cluster网络ip，重复步骤1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node1主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，修改node2主机上的public、cluster网络ip，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，node2主机上osd进程正常退出，部分pg卡住，stuck inactive状态，io挂起</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
<p>
	最后迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6568" name="PG迁移-NTP时钟不同步">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[386]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，设置node1主机非NTP同步时钟，使用命令date修改时间，包含两种情况，比集群时间晚5分钟，比集群时间快5分钟。</p>
<p>
	观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	比集群时间快5分钟，该主机上的osd进程正常退出（有的不退出，需要重启才能重新加入集群），fio归零30秒以内，没有数据一致性问题，集群有告警</p>
<p>
	比集群时间慢5分钟，fio归零30秒以内，900秒后，该osd上的服务在crush上被标记为down并out（需要重启osd进程才能再次加入集群）</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	设置node1主机时间NTP时钟同步，和集群时间相差在0.05秒以内</p>
<p>
	重启该节点所有osd进程，加入集群</p>
<p>
	观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间成功，NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登陆node2，修改时间，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间后，该主机上的osd进程正常退出（部分不退出，需要重启），部分pg卡住，fio挂起</p>
<p>
	NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg迁移正常</p>
<p>
	最终pg迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6573" name="PG迁移-磁盘故障">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[387]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，登陆node1节点，将osd.0 osd.1所在的磁盘通过UI进行下电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	osd.0,osd.1进程正常退出，集群有告警信息</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，通过UI将两个磁盘上电，启动osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio稳定后，在node1上，使用命令踢盘的方式删除osd.0 osd.1的磁盘，再用命令扫出磁盘，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	踢盘时：</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	osd.0,osd.1进程正常退出，集群有告警信息</p>
<p>
	扫盘后：</p>
<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待fio稳定后，在node2上，使用命令和UI两种方式故障一个磁盘，重复步骤1到步骤3</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	磁盘故障时：</p>
<p>
	fio归零30秒以内，没有数据一致性问题；若是负责迁移的osd被终止，则集群会有pg卡住，io挂起，该osd恢复后，pg恢复正常，io恢复</p>
<p>
	osd进程正常退出，集群有告警信息</p>
<p>
	磁盘恢复后：</p>
<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg迁移正常，最后迁移全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6579" name="PG迁移-IO压力增加">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[388]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，新增加10条流的读写（或者增加client），iodepth 256，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群整体的iops，带宽有提升，osd进程正常，集群状态正常，pg迁移正常，性能稳定无较大的波动</p>
<p>
	最终pg迁移全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6582" name="PG迁移-管理平面操作">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[389]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，用脚本大批量创建，删除pool、image、快照并修改属性，用脚本反复查看集群状态、osd tree等信息，做大量管理平面的操作，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写无数据一致性问题</p>
<p>
	性能稍有衰减（iops衰减1k-2k），没有大范围波动</p>
<p>
	集群状态正常，osd进程正常</p>
<p>
	pg迁移过程最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6585" name="PG迁移-暂停集群">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[390]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，暂停osd，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	暂停成功，fio读写挂起</p>
<p>
	osd进程正常，迁移暂停，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，取消暂停，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	取消暂停成功，fio继续下发读写，没有一致性问题</p>
<p>
	osd进程正常，pg迁移正常，集群状态正常</p>
<p>
	pg迁移最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6589" name="PG迁移-添加进程">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[391]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，每个节点上若干个osd进程，其中node1是mon（只有一个mon）</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，在node1节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，在node2节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG迁移完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，在node2和node3上创建mon进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程创建成功，形成新的集群仲裁</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG迁移完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6593" name="PG迁移-删除进程">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[392]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，将该节点的osd.0 osd.1 out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，删除osd.0进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，有pg的迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node1节点上没有out的osd进程删除（osd.2）,观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node2上的一个osd进程删除，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，删除两个mon进程（leader mon和非leader mon），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功，集群形成新的仲裁</p>
<p>
	fio不归零，没有数据一致性问题</p>
<p>
	集群状态正常，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6599" name="PG迁移-rbd失效">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[393]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /><img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，在domain2上创建pool3，pool4。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验，在读写过程中，登录node1节点，将该节点的osd.0（osddomain1） osd.1（osddomain2） out出集群。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将osddomain2中的硬盘全部拔出，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘拔出后，相应的osd进程正常退出，osddomain2中的rbd读写挂起，迁移停止，集群有告警信息</p>
<p>
	osddomain1中的rbd读写不受影响，没有数据一致性问题，pg迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将osddomain2中的硬盘插回，启动osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd进程成功，加入集群成功，osddomain2中rbd恢复读写，没有一致性问题，迁移开始，最后迁移完成</p>
<p>
	osddomain1中的rbd读写不受影响，没有数据一致性问题，pg迁移正常，最后迁移完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将osddomain1中的磁盘全部拔出，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osddomain2中的rbd读写不受影响，没有数据一致性问题，pg迁移正常</p>
<p>
	磁盘拔出时：</p>
<p>
	osddomain1上的osd进程正常退出，集群有告警信息</p>
<p>
	rbd读写挂起，pg迁移暂停</p>
<p>
	磁盘插回后：</p>
<p>
	osddomain1上的osd进程启动成功，加入集群成功</p>
<p>
	rbd读写恢复，没有数据一致性问题，pg迁移开始</p>
<p>
	最终两个osddomain上的pg迁移全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6604" name="PG迁移-修改pool的crush rule">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[394]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，rule规则为1，在domain2上创建pool3，pool4，rule规则为2。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录node1节点，将该节点的osd.0（osddomain1） osd.1（osddomain2）out出集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	out操作成功，有pg的迁移</p>
<p>
	fio读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，手动修改pool1的crush rule规则，由rule2设置为rule 1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain1的osd上</p>
<p>
	最终pg迁移全部完成，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录node1节点，将该节点的osd.0 osd.1 重新加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	in操作成功，有pg的迁移</p>
<p>
	fio读写没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，手动修改pool1的crush rule规则，由rule 1设置为rule 2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain2的osd上</p>
<p>
	最终pg迁移全部完成，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中修改pool3的crush rule规则，由rule2设置为rule1，重复步骤1到步骤4</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rule 2设置为rule 1：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain1的osd上，集群状态OK</p>
<p>
	rule 1设置为rule 2：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain2的osd上</p>
<p>
	最终pg迁移全部完成，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6611" name="PG恢复-kill进程">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[395]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，将node1节点的osd.2进程kill -9，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒后恢复，没有数据一致性问题</p>
<p>
	集群有告警信息，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将osd.2进程启动，加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	进程启动成功，加入集群成功，集群状态正常</p>
<p>
	io性能短暂降低后恢复到较高水平并稳定，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node2主机，对一个osd进程kill -9，重复步骤1和步骤2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	kill进程后，fio归零30秒后恢复，没有数据一致性问题</p>
<p>
	若是负责迁移的osd被kill，则集群会有pg卡住，io挂起，该osd恢复后，pg恢复正常，io恢复</p>
<p>
	进程重新启动成功，加入集群成功，fio重新稳定后，没有数据一致性问题，pg恢复正常，最后恢复完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6616" name="PG恢复-主机异常掉电">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[396]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，将node2主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动node2主机，启动osd进程，加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，没有数据一致性问题</p>
<p>
	性能短暂下降后恢复到较高水平并稳定，没有数据一致性问题，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node1主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	node1主机上的osd进程正常退出，集群有告警信息，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	启动node1主机，启动osd进程（不启动osd.0,osd.1）,查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，没有数据一致性问题</p>
<p>
	性能短暂下降后恢复到较高水平并稳定，没有数据一致性问题，pg恢复正常，最后pg恢复完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6622" name="PG恢复-网络中断">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[397]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，短时间中断node1的public网络，拔出插回在5秒内，观察集群状态，io状态，重复3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	短时间中断后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，短时间中断node1的cluster网络，拔出插回在5秒内，观察集群状态，io状态，重复3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	短时间中断后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在node2上短时间中断public、cluster网络，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	短时间中断后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在node1上，长时间中断public网络，拔出插回在2分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	插回线缆后，连接恢复，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在node1上，长时间中断cluster网络，拔出插回间隔2分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，fio归零在30秒以内，没有数据一致性问题</p>
<p>
	插回线缆后，连接恢复，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在node2上长时间中断public、cluster网络，重复步骤4和步骤5</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	长时间中断后，有pg卡住，出现stuck inactive的状态，io挂起</p>
<p>
	连接恢复后，io恢复，没有数据一致性问题，pg状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将所有node的public网络长时间中断，拔出插回间隔2分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，fio读写挂起，集群有告警信息</p>
<p>
	插回线缆后，连接恢复，fio读写恢复，没有数据一致性问题，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	将所有node的cluster网络长时间中断，拔出插回间隔2分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，fio读写挂起，集群有告警信息</p>
<p>
	插回线缆后，连接恢复，fio读写恢复，没有数据一致性问题，集群状态正常，pg恢复正常，最后pg恢复完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6632" name="PG恢复-IP变更">
	<node_order><![CDATA[23]]></node_order>
	<externalid><![CDATA[398]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，修改node1主机上public网络的ip（包含两种情况，修改成同网段的ip，修改成不同网段的ip）</p>
<p>
	观察集群状态，io状态</p>
<p>
	1分钟后，修改回原来的ip，启动该节点的osd进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node1主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，修改node1主机上的cluster网络ip，重复步骤1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node1主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，修改node2主机上的public、cluster网络ip，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，node2主机上osd进程正常退出，部分pg卡住，stuck inactive状态，io挂起</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
<p>
	最后迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6637" name="PG恢复-NTP时钟不同步">
	<node_order><![CDATA[24]]></node_order>
	<externalid><![CDATA[399]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，设置node1主机非NTP同步时钟，使用命令date修改时间，包含两种情况，比集群时间晚5分钟，比集群时间快5分钟。</p>
<p>
	观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间后，该主机上的osd进程全部正常退出，fio归零30秒以内，没有数据一致性问题，集群有告警</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，设置node1主机时间NTP时钟同步，和集群时间相差在0.05秒以内</p>
<p>
	启动osd进程，加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间成功，NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登陆node2，修改时间，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间后，该主机上的osd进程正常退出（部分不退出，需要重启），部分pg卡住，fio挂起</p>
<p>
	NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg迁移正常</p>
<p>
	最终pg迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6642" name="PG恢复-磁盘故障">
	<node_order><![CDATA[25]]></node_order>
	<externalid><![CDATA[400]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，将node2上的一个磁盘通过UI进行下电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题；若是负责迁移的osd被终止，则集群会有pg卡住，io挂起</p>
<p>
	osd进程正常退出，集群有告警信息，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，通过UI对该磁盘上电，启动osd进程，加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	io性能短暂降低后恢复到较高水平并稳定，集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，在node2上使用命令方式删除另外一个磁盘，重复步骤1和2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除磁盘时：</p>
<p>
	fio归零30秒以内，没有数据一致性问题；若是负责迁移的osd被终止，则集群会有pg卡住，io挂起</p>
<p>
	osd进程正常退出，集群有告警信息，pg恢复正常</p>
<p>
	扫出磁盘后：</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	io性能在短暂降低后恢复到较高水平并稳定，集群状态正常，pg恢复正常，最后pg恢复全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6647" name="PG恢复-IO压力增加">
	<node_order><![CDATA[26]]></node_order>
	<externalid><![CDATA[401]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，新增加10条流的读写（或者增加client），iodepth 256，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群整体的iops，带宽有提升，osd进程正常，集群状态正常，pg恢复正常，性能稳定无较大的波动</p>
<p>
	最终pg恢复全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6650" name="PG恢复-管理平面操作">
	<node_order><![CDATA[27]]></node_order>
	<externalid><![CDATA[402]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，用脚本大批量创建，删除pool、rbd、快照并修改属性，用脚本反复查看集群状态、osd tree等信息，做大量管理平面的操作，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写无数据一致性问题</p>
<p>
	性能稍有衰减（iops衰减1k-2k），没有大范围波动</p>
<p>
	集群状态正常，osd进程正常</p>
<p>
	pg恢复最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6653" name="PG恢复-暂停集群">
	<node_order><![CDATA[28]]></node_order>
	<externalid><![CDATA[403]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，暂停osd，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	暂停成功，fio读写挂起</p>
<p>
	osd进程正常，pg恢复暂停，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，取消暂停，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	取消暂停成功，fio继续下发读写，没有一致性问题</p>
<p>
	osd进程正常，pg恢复正常，集群状态正常</p>
<p>
	pg最终全部恢复完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6657" name="PG恢复-添加进程">
	<node_order><![CDATA[29]]></node_order>
	<externalid><![CDATA[404]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，每个节点上若干个osd进程，其中node1是mon（只有一个mon）</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，在node1节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，在node2节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG恢复完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，在node2和node3上创建mon进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程创建成功，形成新的集群仲裁</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG恢复完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6661" name="PG恢复-删除进程">
	<node_order><![CDATA[30]]></node_order>
	<externalid><![CDATA[405]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node1节点，通过UI下电两个磁盘，对应的的osd.0 osd.1 手动out出集群。</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，删除osd.0进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，有pg的恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node1节点上没有out的osd进程删除（osd.2）,观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node2上的一个osd进程删除，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，删除两个mon进程（leader mon和非leader mon），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功，集群形成新的仲裁</p>
<p>
	fio不归零，没有数据一致性问题</p>
<p>
	集群状态正常，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6667" name="PG恢复-rbd失效">
	<node_order><![CDATA[31]]></node_order>
	<externalid><![CDATA[406]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，在domain2上创建pool3，pool4。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验，在读写过程中，登录node1节点，将osddomain1，osddomain2各通过UI下电一个硬盘，将相应的osd.0 osd.1手动out出集群</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在恢复的过程中，将osddomain1中的硬盘全部拔出，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osddomain1上的rbd读写挂起，osd进程正常退出，pg恢复暂停，集群有告警</p>
<p>
	osddomain2上的rbd读写不受影响，pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将osddomain1上的硬盘全部插回，启动osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，成功加入集群，osddomain1上的rbd恢复读写，没有一致性问题，pg恢复正常，最终恢复成功</p>
<p>
	osddomain2上的rbd读写不受影响，没有数据一致性问题，pg恢复正常，最终恢复完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在恢复过程中，将osddomain2中的硬盘全部拔出，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osddomain1中的rbd读写不受影响，没有数据一致性问题，pg恢复正常</p>
<p>
	磁盘拔出时：</p>
<p>
	osddomain2上的osd进程正常退出，集群有告警信息</p>
<p>
	rbd读写挂起，pg恢复暂停</p>
<p>
	磁盘插回后：</p>
<p>
	osddomain2上的osd进程启动成功，加入集群成功</p>
<p>
	rbd读写恢复，没有数据一致性问题，pg恢复开始</p>
<p>
	最终两个osddomain上的pg恢复全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6672" name="PG恢复-修改pool的crush rule">
	<node_order><![CDATA[32]]></node_order>
	<externalid><![CDATA[407]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个osd主机上有若干个osd进程，node1，node2，node3也是mon主机，3个mon。创建crush rule规则，如图，将osd分成两个osddomain，domain1和domain2，每个domain都在每个host上有osd分布。在domain1上创建pool1，pool2，crush rule 1，在domain2上创建pool3，pool4，crush rule 2。每个pool中若干个rbd，对每个rbd 8k随机写进行数据预埋，保证每个osd下大概使用100G左右的使用容量。启动fio对其中两个rbd（rbd1来自pool1，rbd3来自pool3）进行随机写，带fio校验，在读写过程中，登录node1节点，将osddomain1，osddomain2各通过UI下电一个硬盘，将相应的osd.0 osd.1手动out出集群</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，手动修改pool1的crush rule规则，由rule 1设置为rule 2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain2的osd上</p>
<p>
	pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待pool1的pg全部迁移到osddomain2的osd上后，在pg恢复过程中，手动修改pool1的crush rule规则，由rule 2设置为rule 1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain1的osd上</p>
<p>
	pg恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pg恢复过程中，手动设置pool3的crush rule规则，重复步骤1步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rule 2设置为rule 1：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain1的osd上</p>
<p>
	pg恢复正常</p>
<p>
	rule 1设置为rule 2：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain2的osd上</p>
<p>
	pg恢复正常，最终pg恢复全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6677" name="PG平衡-kill进程">
	<node_order><![CDATA[33]]></node_order>
	<externalid><![CDATA[408]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg平衡过程中，在node2上kill -9两个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将杀死的的osd进程启动，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂降低后恢复到较高水平并稳定，没有数据一致性问题</p>
<p>
	pg平衡正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio读写稳定后，登陆node3节点，将新创建的osd进程全部kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将node3的被kill的osd进程全部启动，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂降低后恢复到较高水平并稳定，没有数据一致性问题</p>
<p>
	pg平衡正常，最后平衡完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6683" name="PG平衡-主机异常掉电">
	<node_order><![CDATA[34]]></node_order>
	<externalid><![CDATA[409]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg平衡过程中，将node2主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动node2主机，启动osd进程，加入集群，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，没有数据一致性问题</p>
<p>
	性能短暂下降后恢复到较高水平并稳定，没有数据一致性问题，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，将node3主机异常掉电，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg卡住，出现stuck inactive的状态，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	启动node3主机，启动osd进程,查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功，没有数据一致性问题</p>
<p>
	性能短暂下降后恢复到较高水平并稳定，没有数据一致性问题，pg平衡正常，最后pg平衡完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6689" name="PG平衡-网络中断">
	<node_order><![CDATA[35]]></node_order>
	<externalid><![CDATA[410]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断node3节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断node3节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在node2节点上重复操作步骤1和步骤2，短时间中断网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断node3节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断node3节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在node2节点上重复操作步骤4和步骤5，长时间中断网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio挂起，pg卡住。</p>
<p>
	插回后，链路正常，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的public网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的cluster网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，pg平衡正常</p>
<p>
	最终pg平衡全部完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6699" name="PG平衡-IP变更">
	<node_order><![CDATA[36]]></node_order>
	<externalid><![CDATA[411]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，修改node3主机上public网络的ip（包含两种情况，修改成同网段的ip，修改成不同网段的ip）</p>
<p>
	观察集群状态，io状态</p>
<p>
	1分钟后，修改回原来的ip，启动该节点的osd进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node3主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，修改node3主机上的cluster网络ip，重复步骤1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，fio归零30秒内恢复，没有数据一致性问题，node3主机上osd进程正常退出。</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，修改node2主机上的public、cluster网络ip，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改ip后，node2主机上osd进程正常退出，有部分pg出现stuck inactive的状态，io挂起</p>
<p>
	修改回原来ip，启动osd成功，加入集群成功，fio性能短暂降低后恢复较高水平，没有数据一致性问题，集群状态正常</p>
<p>
	最后平衡全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6704" name="PG平衡-NTP时钟不同步">
	<node_order><![CDATA[37]]></node_order>
	<externalid><![CDATA[412]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，设置node3主机非NTP同步时钟，使用命令date修改时间，包含两种情况，比集群时间晚5分钟，比集群时间快5分钟。</p>
<p>
	观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	比集群时间快5分钟，该主机上的osd进程正常退出（有的不退出，需要重启才能重新加入集群），fio归零30秒以内，没有数据一致性问题，集群有告警</p>
<p>
	比集群时间慢5分钟，fio归零30秒以内，900秒后，该osd上的服务在crush上被标记为down并out（需要重启osd进程才能再次加入集群）</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，设置node3主机时间NTP时钟同步，和集群时间相差在0.05秒以内</p>
<p>
	启动osd进程，加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间成功，NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登陆node2，修改时间，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改时间后，该主机上的osd进程正常退出（部分不退出，需要重启），部分pg卡住，fio挂起</p>
<p>
	NTP时钟同步成功，误差在0.05秒以内</p>
<p>
	启动osd进程成功，加入集群成功</p>
<p>
	fio没有数据一致性问题，集群状态正常，pg迁移正常</p>
<p>
	最终pg迁移全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6709" name="PG平衡-磁盘故障">
	<node_order><![CDATA[38]]></node_order>
	<externalid><![CDATA[413]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，登陆node3节点，将新添加的3个osd进程所在的磁盘都通过UI进行下电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	osd进程正常退出，集群有告警信息</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将三个物理磁盘通过UI进行上电，启动osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待fio稳定后，在node3上，使用命令踢盘的方式删除新添加的osd进程的磁盘，再用命令扫出磁盘，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	踢盘时：</p>
<p>
	fio归零30秒以内，没有数据一致性问题</p>
<p>
	osd进程正常退出，集群有告警信息</p>
<p>
	扫盘后：</p>
<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio性能短暂下降后恢复到较高水平并稳定，没有一致性问题</p>
<p>
	集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待fio稳定后，在node2上，使用命令和UI下电两种方式故障一个磁盘，重复步骤1到步骤3</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	磁盘故障时：</p>
<p>
	osd进程正常退出，集群有告警信息</p>
<p>
	有部分pg卡住，fio挂起</p>
<p>
	磁盘恢复后：</p>
<p>
	osd进程启动成功，加入集群成功</p>
<p>
	fio读写恢复，没有一致性问题</p>
<p>
	集群状态正常，pg平衡正常，最后pg平衡全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6715" name="PG平衡-IO压力增加">
	<node_order><![CDATA[39]]></node_order>
	<externalid><![CDATA[414]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，新增加10条流的读写（或者增加client），iodepth 256，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群整体的iops，带宽有提升，osd进程正常，集群状态正常，pg平衡正常，性能稳定无较大的波动</p>
<p>
	最终pg平衡全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6718" name="PG平衡-管理平面操作">
	<node_order><![CDATA[40]]></node_order>
	<externalid><![CDATA[415]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，用脚本大批量创建，删除pool、rbd、快照并修改属性，用脚本反复查看集群状态、osd tree等信息，做大量管理平面的操作，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写无数据一致性问题</p>
<p>
	性能稍有衰减（iops衰减1k-2k），没有大范围波动</p>
<p>
	集群状态正常，osd进程正常</p>
<p>
	pg平衡过程最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6721" name="PG平衡-暂停集群">
	<node_order><![CDATA[41]]></node_order>
	<externalid><![CDATA[416]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，暂停osd，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	暂停成功，fio读写挂起</p>
<p>
	osd进程正常，平衡暂停，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，取消暂停，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	取消暂停成功，fio继续下发读写，没有一致性问题</p>
<p>
	osd进程正常，pg平衡正常，集群状态正常</p>
<p>
	pg平衡最终全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6725" name="PG平衡-添加进程">
	<node_order><![CDATA[42]]></node_order>
	<externalid><![CDATA[417]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，每个节点上若干个osd进程，其中node1是mon（只有一个mon）</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，在node1节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，在node2节点上的空闲硬盘上创建osd进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程创建成功，加入集群成功，集群状态正常</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG平衡完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，在node2和node3上创建mon进程，加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程创建成功，形成新的集群仲裁</p>
<p>
	fio读写没有一致性问题</p>
<p>
	PG平衡完成，集群为OK状态</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6729" name="PG平衡-删除进程">
	<node_order><![CDATA[43]]></node_order>
	<externalid><![CDATA[418]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读，在读写过程中，登录node3节点，新创建3个osd进程加入集群，有pg的平衡</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，登录node3节点，删除一个在平衡过程中的osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群有告警信息，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node1节点，删除一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群有告警信息，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node2节点，删除一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，没有数据一致性问题</p>
<p>
	集群有告警信息，pg平衡正常</p>
<p>
	最终pg平衡完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，删除两个mon进程（leader mon和非leader mon），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除进程成功，集群形成新的仲裁</p>
<p>
	fio不归零，没有数据一致性问题</p>
<p>
	集群状态正常，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6734" name="PG平衡-rbd失效">
	<node_order><![CDATA[44]]></node_order>
	<externalid><![CDATA[419]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，使用node1，node2，node3做mon主机，三个mon。配置crush rule，创建两个osddomain，osddomain1和osddomain2上的osd在node1和node2上都有分布，osddomain1上创建pool1，pool2，crush rule 1，osddomain2上创建pool3，pool4，crush rule 2。每个pool若干个image并用nbd导出到client端，对每个image 8k随机写，使得每个osd下使用100G的容量左右。对其中两个image（image1属于pool1，image3属于pool3）启动fio读写，带一致性校验，在写的过程中，在node3上新添加2个osd到osddomain1，新添加另外两个osd到osddomain2，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，将osddomain1中的磁盘全部拔出，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	磁盘拔出后，相应的osd进程正常退出，集群有告警信息</p>
<p>
	osddomain1中的rbd读写挂起，pg平衡暂停</p>
<p>
	osddomain2中的rbd读写不受影响，没有数据一致性问题，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将osddomain1中的磁盘全部插回，启动osd进程，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	磁盘插回后，启动osd进程成功，加入集群成功</p>
<p>
	osddomain1中的rbd读写恢复，没有数据一致性问题，pg平衡开始</p>
<p>
	osddomain2中的rbd读写不受影响，没有数据一致性问题，pg平衡正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在平衡过程中，将osddomain2中的磁盘全部拔出，重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osddomain1中的rbd读写不受影响，没有数据一致性问题，pg平衡正常</p>
<p>
	磁盘拔出时：</p>
<p>
	osddomain2上的osd进程正常退出，集群有告警信息</p>
<p>
	rbd读写挂起，pg平衡暂停</p>
<p>
	磁盘插回后：</p>
<p>
	osddomain2上的osd进程启动成功，加入集群成功</p>
<p>
	rbd读写恢复，没有数据一致性问题，pg平衡开始</p>
<p>
	最终两个osddomain上的pg平衡全部完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6739" name="PG平衡-修改pool的crush rule">
	<node_order><![CDATA[45]]></node_order>
	<externalid><![CDATA[420]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/crush%20rule.PNG" style="width: 552px; height: 405px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3为osd主机，每个主机上若干个osd进程，使用node1，node2，node3做mon主机，三个mon。配置crush rule，创建两个osddomain，osddomain1和osddomain2上的osd在node1和node2上都有分布，osddomain1上创建pool1，pool2，crush rule 1，osddomain2上创建pool3，pool4，crush rule 2。每个pool若干个image，对每个image 8k随机写，使得每个osd下使用100G的容量左右。对其中两个image（image1属于pool1，image3属于pool3）启动fio读写，带一致性校验，在写的过程中，在node3上新添加2个osd到osddomain1，新添加另外两个osd到osddomain2，有pg的重新平衡</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pg平衡过程中，手动修改pool1的crush rule规则，由rule 1设置为rule 2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain2的osd上</p>
<p>
	pg平衡过程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待pool1的pg全部迁移到osddomain2的osd上后，在pg平衡过程中，手动修改pool1的crush rule规则，由rule 2设置为rule 1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd1的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd3的io读写不受影响，没有数据一致性问题</p>
<p>
	pool1的pg全部迁移到osddomain1的osd上</p>
<p>
	pg平衡过程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pg平衡过程中，设置pool3的crush rule规则，重复步骤1到2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	由rule 2设置为rule 1：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain1的osd上</p>
<p>
	pg平衡过程正常</p>
<p>
	由rule 1设置为rule 2：</p>
<p>
	rbd3的io读写归零30秒以内，没有数据一致性问题</p>
<p>
	rbd1的io读写不受影响，没有数据一致性问题</p>
<p>
	pool3的pg全部迁移到osddomain2的osd上</p>
<p>
	pg平衡过程正常，最终pg平衡全部完成，集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="6794" name="边界值" >
<node_order><![CDATA[8]]></node_order>
<details><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
<p>
	主要关注在cpu，内存，网络带宽，pool，rbd等规格测试，以及接近物理极限时的集群响应</p>
]]></details>

<testcase internalid="6795" name="创建极限数量osd">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[421]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，同时这三个主机也是mon主机，3个mon。</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	三台主机均插满硬盘，按照每个硬盘创建5个osd的标准，3个mon，创建集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建集群成功，创建osd成功，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建pool，2副本，创建image并用nbd方式导出到client端，fio读写30分钟，带一致性校验，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写没有数据一致性问题，性能稳定，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，长时间中断node1的public网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down，选出新的leader mon</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，长时间中断node1的cluster网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，将主机node1异常掉电，然后再上电，启动所有osd进程，mon进程，观察集群，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1掉电后，io能够在30秒内恢复，该节点上的osd全部标记为down，集群仲裁选出新的leader mon</p>
<p>
	node1上电后，启动osd进程mon进程成功，osd全部标记为up，该mon重新变为leader mon，io短暂下降后恢复正常。</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6802" name="创建极限数量pool">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[422]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，数量为32个，副本数为2（pg根据osd数量计算）</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool创建成功，pg分布成功</p>
<p>
	ceph osd df</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在pool上面随机创建若干个image（选择id最小的pool，id最大的pool，id中间的pool），大小在10G到2T之间随机分配，将image用nbd方式导出到client端，启动fio对这些image读写，带一致性校验，运行半小时，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	image创建成功，fio读写没有问题，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，长时间中断node1的public网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down，选出新的leader mon</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，长时间中断node1的cluster网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down，选出新的leader mon</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，将主机node1异常掉电，然后再上电，启动所有osd进程，mon进程，观察集群，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1掉电后，io能够在30秒内恢复，该节点上的osd全部标记为down，集群仲裁选出新的leader mon。</p>
<p>
	node1上电后，启动osd进程mon进程成功，osd全部标记为up，该mon重新变为leader mon，io短暂下降后恢复正常。</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6809" name="创建极限数量image">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[423]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个pool，pool1，pool2，双副本，每个pool的pg根据osd数量计算，在每个pool中创建1024个image，一共2048个image ，随机对其中的10个image用nbd方式导出，进行读写（包含最先创建的image，最后创建的image，中间创建的image），读写带一致性校验，运行半小时，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建image成功，fio读写没有数据一致性问题，集群状态OK，无告警</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，长时间中断node1的public网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down，选出新的leader mon</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，长时间中断node1的cluster网络，3分钟后插回，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆后，io能够在30秒内恢复，该节点的osd标记为down</p>
<p>
	插回线缆后，node1的osd能够全部标记为up，io短暂降低后恢复</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io恢复稳定后，将主机node1异常掉电，然后再上电，启动所有osd进程，mon进程，观察集群，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1掉电后，io能够在30秒内恢复，该节点上的osd全部标记为down，集群仲裁选出新的leader mon</p>
<p>
	node1上电后，启动osd进程mon进程成功，osd全部标记为up，该mon重新变为leader mon，io短暂下降后恢复正常。</p>
<p>
	集群状态OK，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6815" name="物理cpu极限">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[424]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建若干个pool，每个pool中创建若干个image，将image用nbd导出到client端，启动fio,对这些image进行8k随机写，带校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool、images成功，fio读写没有问题，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	继续创建pool，images，增加client端，对新增加的images进行8k随机写，不带校验。直到server端的cpu利用率达到接近100%。运行半小时，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool、images成功，fio读写没有问题，server端cpu利用率达到100%后，运行fio写没有异常，os无异常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6819" name="物理带宽极限">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[425]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建30个pool，每个pool中创建1个image</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool、image创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在单个client端，nbd导出30个image，使用fio同时对30个image进行1M（或4M）大小的顺序写，带一致性校验，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	client端下发io正常，没有数据一致性问题</p>
<p>
	client物理链路达到理论带宽，性能无较大波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	增加一个client端，两个client端同时起多条流，进行1M（或4M）大小的顺序写，带一致性校验，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	两个client端下发io正常，没有数据一致性问题</p>
<p>
	client物理链路达到理论带宽，性能无较大波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	继续创建pool，images，继续增加client端，每个clietn端起多条流，进行1M（或4M）大小的顺序写，带一致性校验，将server端的物理链路带宽跑满为止，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有client端下发io正常，没有数据一致性问题</p>
<p>
	server物理链路达到理论带宽，性能无较大波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2到步骤4，将顺序写换成顺序读，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6825" name="物理内存极限">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[426]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2。每个主机的内存按osd标配来配置（每个osd进程配置2G内存预算）</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建30个pool，每个pool中创建1个image</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool、image创建成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将30个image用nbd方式导出，启动fio，对每个image进行8K随机读写，带一致性校验，观察内存，观察io状态，运行半小时</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	内存使用持续增长，达到一定程度后，系统会自动释放一定空间，不会将所有内存全部耗尽，不存在内存泄漏</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6829" name="物理容量极限">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[427]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机上一个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建osd时指定block的使用容量，每个osd使用10G左右的容量，创建集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群创建成功，查看osd的可用容量正确</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建image，nbd导出image，对该image，启动fio随机写，写的数据容量，超过pool的可用容量，观察io状态，观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool、image创建成功</p>
<p>
	fio随机写，写到默认的阈值后，fio无法继续再下发io，集群有告警。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6833" name="创建删除超大image">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[428]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，双副本，在该pool中创建2T的image，创建完成后nbd导出到client端，对该image进行读写，带一致性校验，运行15分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，image创建成功，io正常，没有数据一致性问题</p>
<p>
	集群状态为OK，cpu，内存使用范围正常，性能稳定没有明显波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	创建4T的image，创建完成后nbd导出到client端，对该image进行读写，带一致性校验，运行15分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	image创建成功，io正常，没有数据一致性问题</p>
<p>
	集群状态为OK，cpu，内存使用范围正常，性能稳定没有明显波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	创建10T的image，创建完成后nbd导出到client端，对该image进行读写，带一致性校验，运行15分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	image创建成功，io正常，没有数据一致性问题</p>
<p>
	集群状态为OK，cpu，内存使用范围正常，性能稳定没有明显波动</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	对2T,4T的image进行读写，10T的image停止读写，将10T的image删除，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	不能删除10T的image，提示已经被nbd映射</p>
<p>
	2T，4T的image读写不受影响</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	停止所有的image读写，unmap image，删除所有的image</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除image成功，集群pool使用容量有释放16T左右</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6839" name="创建删除小容量image">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[429]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，双副本，在该pool中创建image1，大小为1G，使用nbd导出后fio进行读写，带一致性校验，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，image创建成功，读写没有问题，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在pool中创建image2，大小为100M，用nbd方式导出，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	iamge创建成功</p>
<p>
	nbd导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pool中创建image3，大小为1M，用nbd方式导出，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	image创建成功</p>
<p>
	nbd导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将image1，image2，image3 unmap后删除，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除image成功，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6845" name="client端使用超大块读写">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[430]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建3个pool,副本数为2，每个pool创建3个image，每个image 100G左右大小，在client端用nbd导出，fio对这些image进行顺序读写，块大小设置为1M，iodepth 256，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，image创建成功，io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	块大小设置为4M，iodepth 256和512，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	iodepth 256时，io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
<p>
	iodepth 512时，io有断断续续的现象（10G网络），没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	块大小设置为16M，iodepth 64和128，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	iodepth 64时，io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
<p>
	iodepth 128时，io有断断续续的现象（10G网络），没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6850" name="client端使用大iodepth读写">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[431]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建3个pool,副本数为2，每个pool创建3个image，每个image 100G左右大小并用nbd导出到client端，在fio上对这些image进行随机读写，块大小设置为8k，iodepth 256，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，image创建成功，io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	调整iodepth 512，进行读写，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	调整iodepth 1024，进行读写，观察io状态，集群状态，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写没有问题，性能稳定，没有较大波动，没有osd进程异常，集群状态ok，cpu，内存使用正常范围</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6855" name="client端高并发读写">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[432]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建32个pool，在每个pool中创建64个rbd image，一共2048个image，每个image在100G-500G不等</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool成功，创建image成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	配置fio job文件，在同一个client上同时对256个image进行随机写和随机读（如果client端cpu不够的话，适当调整每个image读写的bs，iodepth），持续30分钟，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	读写成功，没有异常，没有数据一致性问题。</p>
<p>
	有可能达到cpu，物理带宽等极限。内存使用率正常，集群状态正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	接入8台client到集群，每个client端对不同的256个image进行随机读写（如果client端cpu不够的话，适当调整每个image读写的bs，iodepth），8台client一共对2048个image进行读写，持续30分钟，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	读写成功，没有异常，没有数据一致性问题。</p>
<p>
	有可能达到cpu，物理带宽等极限。内存使用率正常，集群状态正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6860" name="反复快速重启osd进程">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[433]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个pool，在pool中创建若干个image并用nbd导出到client端</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，image成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动fio对两个pool中的image进行读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写成功，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登录node1，将某一个osd进程的pid找出，kill -9，杀死该进程，然后立即启动该osd进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	反复快速重启osd进程成功，osd进程无异常</p>
<p>
	没有数据一致性问题，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node2，将某一个osd进程的pid找出，kill -9，杀死该进程，然后立即启动该osd进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	反复快速重启osd进程成功，osd进程无异常</p>
<p>
	没有数据一致性问题，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，登录node3，将某一个osd进程的pid找出，kill -9，杀死该进程，然后立即启动该osd进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	反复快速重启osd进程成功，osd进程无异常</p>
<p>
	没有数据一致性问题，集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6867" name="反复快速重启mon进程">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[434]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个pool，在pool中创建若干个image并用nbd方式导出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，image成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动fio对两个pool中的image进行读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写成功，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	登录node1，将leader mon进程的pid找出，kill -9，杀死该进程，然后立即启动mon进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程kill，重启成功</p>
<p>
	io正常，没有数据一致性问题</p>
<p>
	集群仲裁正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定，集群状态OK后，登录node2，将mon进程的pid找出，kill -9，杀死该进程，然后立即启动mon进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程kill，重启成功</p>
<p>
	io正常，没有数据一致性问题</p>
<p>
	集群仲裁正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待io稳定，集群状态OK后，登录node3，将mon进程的pid找出，kill -9，杀死该进程，然后立即启动mon进程，重复5次，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon进程kill，重启成功</p>
<p>
	io正常，没有数据一致性问题</p>
<p>
	集群仲裁正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6874" name="反复快速修改pool size">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[435]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，两个pool，pool1副本数1，pool2副本数2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在pool1，pool2中创建若干个image并用nbd方式导出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建image成功</p>
<p>
	nbd导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	启动fio对pool1，pool2中的image进行随机读写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	读写正常，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，修改pool1的size，由1改为2,2改为3,3改为2,2改为1，快速重复该步骤3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改副本数成功</p>
<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io稳定后，在读写过程中，修改pool2的size，由2改为3,3改为1,1改为3，3改为2，快速重复该步骤3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	修改副本数成功</p>
<p>
	fio读写正常，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6880" name="创建pool时使用极限pg数量">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[436]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool1，使用大数量pg，使得每个osd上的pg数量在500左右。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool成功，查看ceph osd df，每个osd上分布的pg数量比较平均，大约500左右</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在pool1中创建若干个image并用nbd方式导出，对这些images进行fio读写，带一致性校验，运行10分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建images成功，nbd导出成功，fio读写正常，数据一致性没有问题，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1的public网络中断，3分钟后，插回，观察集群状态，io状态。在node2，node3上重复该步骤</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出线缆，fio能够及时恢复，归零30秒内，没有数据一致性问题</p>
<p>
	线缆插回后，osd状态能够及时恢复为up，fio读写没有一致性问题，性能短暂下降后恢复</p>
<p>
	集群状态恢复为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除pool1，创建pool2，使用大数量pg，使得每个osd上的osd上的pg数量在3000左右。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool成功，查看ceph osd df，每个osd上分布的pg数量比较平均，大约3000左右</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在pool2中创建若干个images，重复步骤2、3。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	结果同步骤2、3的期望结果</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6887" name="snapshot边界值">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[437]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	<span style="font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 13.3333px; background-color: rgb(238, 238, 238);">node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</span></p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个pool：pool1,pool2，创建image：pool1/m1, pool2/n1,大小都为200G，nbd导出到client端</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，image创建成功，nbd导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，挂载到目录/root/nbd0，写一个30G的文件，同时对裸盘nbd1进行随机写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，写文件成功</p>
<p>
	裸盘写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	30G文件写成功后，对nbd0打快照，继续对/root/nbd0目录写同名文件，rate=200k（触发长时间的COW），在写的过程中，每隔10秒对nbd0打一次快照，总共创建128个快照。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	打快照正常，不影响nbd0的写</p>
<p>
	nbd1的读写不受影响</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待128个快照完成之后，中断node1节点的public网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd0，nbd1的读写归零30秒内然后恢复</p>
<p>
	node1节点的osd全部标记为down</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，恢复node1的public网线</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的osd重新标记为up，集群状态最终为OK</p>
<p>
	nbd0，nbd1的读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在node2和node3上重复步骤4,5</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将nbd0的其中3个快照用nbd方式导出，挂载目录，进行读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	快照导出正常，挂载目录正常</p>
<p>
	快照可以读，写快照报错，不允许</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6889" name="clone边界值">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[438]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	<span style="font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 13.3333px; background-color: rgb(238, 238, 238);">node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</span></p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建两个pool：pool1,pool2，创建image：pool1/m1, pool2/n1,大小都为200G，nbd导出到client端</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pool，image创建成功，nbd导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	对nbd0创建ext4文件系统，挂载到目录/root/nbd0，写一个30G的文件，同时对裸盘nbd1进行随机写，带一致性校验</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，写文件成功</p>
<p>
	裸盘写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	30G文件写成功后，对nbd0打快照，继续对/root/nbd0目录写同名文件，rate=200k（触发长时间的COW），在写的过程中，对nbd0打一次快照，保护快照，创建clone，隔10秒后，对该clone打快照，保护快照，创建clone，如此循环，总共5层父子关系。（创建clone要包含pool1和pool2）</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	快照，clone创建成功</p>
<p>
	nbd0，nbd1的读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将最后一个clone用nbd导出到client端(nbd2)，挂载目录，进行读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	clone导出正常</p>
<p>
	clone挂载目录成功，可以进行读写</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将5个clone全部进行flatten，在flatten过程中，中断node1的public网络</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd0，nbd1，nbd2的读写归零30秒以内</p>
<p>
	node1上的osd标记为down，flatten过程继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1的public网络恢复，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd0，nbd1，nbd2的读写正常</p>
<p>
	node1的osd全部标记为up，flatten最终成功完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6891" name="nbd map边界值">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[439]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	<span style="font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 13.3333px; background-color: rgb(238, 238, 238);">node1，node2，node3三个osd主机，每个osd主机若干个osd进程，同时这三个主机也是mon主机，3个mon。集群状态正常，副本数设置为2</span></p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	创建pool，创建256个image，在一个client端将这256个image全部用nbd方式导出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建pool，image成功</p>
<p>
	在client端导出256个image成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	对其中的10个nbd创建文件系统（包含nbd0，nbd255），并进行读写文件</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件系统创建成功，读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	对nbd随机挑选10个进行裸盘读写（包含nbd0，nbd255）</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd裸盘读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="8104" name="热插拔/硬盘漫游（graceful）" >
<node_order><![CDATA[10]]></node_order>
<details><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p2.PNG" style="width: 494px; height: 294px;" /></p>
]]></details>
<testsuite id="10088" name="故障域为Tahoe" >
<node_order><![CDATA[0]]></node_order>
<details><![CDATA[<p>
	之前的用例故障域为节点，现在初始化的时候如果是两台tahoe，则默认故障域为tahoe</p>
<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/p1.PNG" style="width: 482px; height: 286px;" /></p>
]]></details>

<testcase internalid="10125" name="单tahoe多节点多盘插回原节点(5分钟内)">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[608]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原节点(有些插回原槽位，有些插回节点内其他槽位)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	选择同一tahoe下的其他盘点击关闭</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	关闭后直接点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘显示不为空，硬盘总数相应增加，io读写正常，OSD进程状态为up，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	数据迁移完成，io读写正常，集群状态正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-9多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10136" name="单tahoe多节点多盘插回原节点(5-10分钟迁移过程中)">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[609]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个</div>
<div>
	&nbsp;</div>
<div>
	osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在单个tahoe下多个节点选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将拔出的硬盘插回原节点(有些插回原槽位，有些插回节点内其他槽位)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，之前的osd数据迁移继续进行，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常,之前out的osd被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10148" name="单tahoe多节点多盘插回原节点(10分钟后迁移已完成)">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[610]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个</div>
<div>
	&nbsp;</div>
<div>
	osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	个数和状态正确，OSD进程正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在单个tahoe多个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd从集群删除，系统状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原节点(有些插回原槽位，有些插回节点内其他槽位)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10160" name="单tahoe多节点多盘插回其他节点(5分钟内)">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[611]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个</div>
<div>
	&nbsp;</div>
<div>
	osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回同一tahoe(有些插回原节点，有些插回同一tahoe内其他节点，包括节点间互换位置)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	选择同一tahoe下的其他盘点击关闭</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	关闭后直接点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘显示不为空，硬盘总数相应增加，io读写正常，OSD进程状态为up，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-9多次(轮流)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10172" name="单tahoe多节点多盘插回其他节点(5-10分钟迁移过程中)">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[612]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个</div>
<div>
	&nbsp;</div>
<div>
	osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在单个tahoe下多个节点选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将拔出的硬盘插回同一tahoe(有些插回原节点，有些插回同一tahoe内其他节点，包括节点间互换位置)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，之前的osd数据迁移继续进行，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常,之前out的osd被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次(轮流)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10183" name="单tahoe多节点多盘插回其他节点(10分钟后迁移已完成)">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[613]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个</div>
<div>
	&nbsp;</div>
<div>
	osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在单个tahoe多个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd从集群删除，系统状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回同一tahoe(有些插回原节点，有些插回同一tahoe内其他节点，包括节点间互换位置)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次(轮流)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面的结果相同</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10254" name="单tahoe多节点多盘原节点反复拔插(5分钟内)">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[617]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个</div>
<div>
	&nbsp;</div>
<div>
	osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原节点(有些插回原槽位，有些插回节点内其他槽位)(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态变为为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将刚才插回的硬盘再次下电，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原节点(有些插回原槽位，有些插回节点内其他槽位)(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	重复步骤7-10 5次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10278" name="单tahoe多节点多盘原节点反复拔插(5-10分钟迁移过程中)">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[618]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将拔出的硬盘插回原节点(有些插回原槽位，有些插回节点内其他槽位)(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，之前的osd数据迁移继续进行，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	再次将刚才插回的硬盘下电，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，刚新建的OSD进程状态为down，无数据迁移，系统性能基本无变化，之前的osd数据迁移继续进行，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<div>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降，之前的osd数据迁移完成后被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将拔出的硬盘插回原节点(有些插回原槽位，有些插回节点内其他槽位)(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，之前的osd数据迁移继续进行，迁移完成后被删除，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	重复步骤8-12 5次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，out的osd被删除，集群状态正常，io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10294" name="单tahoe多节点多盘原节点反复拔插(10分钟后迁移已完成)">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[619]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	集群状态正常，无数据迁移，OSD进程正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成后osd被删除，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原节点(有些插回原槽位，有些插回节点内其他槽位)(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	再次将刚才插回的硬盘下电，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，刚新建的OSD进程状态为down，无数据迁移，系统性能基本无变化，之前的osd数据迁移继续进行，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降，之前的osd数据迁移完成后被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移完成后osd被删除，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原节点(有些插回原槽位，有些插回节点内其他槽位)(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[16]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[17]]></step_number>
	<actions><![CDATA[<p>
	重复步骤10-16</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10313" name="单tahoe多节点多盘同一tahoe内反复拔插(5分钟内)">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[620]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回同一tahoe(有些插回原节点，有些插回同一tahoe内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将刚才插回的硬盘再次下电，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘再次插回同一tahoe(有些插回原节点，有些插回同一tahoe内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	重复步骤7-10 5次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10327" name="单tahoe多节点多盘同一tahoe内反复拔插(5-10分钟迁移过程中)">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[621]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<div>
	数据迁移过程中将拔出的硬盘插回原tahoe(有些插回原节点，有些插回同一tahoe内其他节点)</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，之前的osd数据迁移继续进行，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	再次将刚才插回的硬盘下电，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，刚新建的OSD进程状态为down，无数据迁移，系统性能基本无变化，之前的osd数据迁移继续进行，迁移完成后osd被系统删除，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，之前的osd数据迁移继续进行，迁移完成后osd被系统删除，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降，之前的osd数据迁移完成后被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将拔出的硬盘插回原tahoe(有些插回原节点，有些插回同一tahoe内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，之前的osd数据迁移继续进行，迁移完成后被删除，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	重复步骤8-12 5次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，out的osd被删除，集群状态正常，io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10343" name="单tahoe多节点多盘同一tahoe内反复拔插(10分钟后迁移已完成)">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[622]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成后osd进程被系统删除，系统状态正常，io读写正确</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回同一tahoe(有些插回原节点，有些插回同一tahoe内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	再次将刚才插回的硬盘下电，并通过ceph -s 和 ceph osd tree查看集群状态</p>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，刚新建的OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成后osd进程被系统删除，系统状态正常，io读写正确</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘再插回同一tahoe(有些插回原节点，有些插回同一tahoe内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	重复步骤8-12 5次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[16]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，out的osd被删除，集群状态正常，io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10363" name="多tahoe多节点多盘轮流拔插(5分钟内)">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[624]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe A的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原tahoe A(有些插回原节点，有些插回tahoe A内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态变为为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移完成后集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在另外一台tahoe B上的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原tahoe B(有些插回原节点，有些插回tahoe B内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态变为为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<div>
	等待数据迁移完成</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-12</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10378" name="多tahoe多节点多盘轮流拔插(5-10分钟迁移过程中)">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[625]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	集群状态正常，无数据迁移，OSD进程正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe A的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将拔出的硬盘插回原tahoe A(有些插回原节点，有些插回tahoe A内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，之前的osd数据迁移继续进行，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，之前的osd数据迁移完成后被集群删除，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在另外一台tahoe B上的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将拔出的硬盘插回原tahoe B(有些插回原节点，有些插回tahoe B内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，之前的osd数据迁移继续进行，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，之前的osd数据迁移完成后被集群删除，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-14</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10396" name="多tahoe多节点多盘轮流拔插(10分钟后迁移已完成)">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[626]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一台tahoe A的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移完成后对应的osd被删除，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将拔出的硬盘插回原tahoe A(有些插回原节点，有些插回tahoe A内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<div>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移正常，数据分布均匀，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<div>
	在另外一台tahoe B上的多个节点的硬盘概观上选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘并记住每个硬盘来自哪个节点哪个槽位，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成后对应的osd被删除，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将拔出的硬盘插回原tahoe B(有些插回原节点，有些插回tahoe B内其他节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[16]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移正常，数据分布均匀，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[17]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-16</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="12476" name="多tahoe多节点多盘逐个拔插(5分钟内 磁盘清理场景)">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[654]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，</div>
<div>
	&nbsp;</div>
<div>
	每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	选择单台tahoe，从tahoe内逐个拔出硬盘(不按节点顺序拔插)，5分钟内插回</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	io读写正常，插回后osd状态恢复正常，系统性能基本无变化，日志和告警有相应事件</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	单个tahoe操作完后等数据迁移完成集群恢复正常状态后操作同集群的其他tahoe</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面的结果相同</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	所有磁盘被拔插一次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10195" name="多tahoe多节点多盘来回拔插(5分钟内)">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[614]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个</div>
<div>
	&nbsp;</div>
<div>
	osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A的多个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插入同一集群的tahoe B设备</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，UI提示客户这些盘属于另外一台tahoe A，让客户选择是继续加入集群还是重新插，日志和告警有相应事件&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在UI上选择继续加入</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	新建的OSD为UP，新加入的硬盘有大量数据迁移，原OSD在5分钟后out并且开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移完后之前的osd被集群删除，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	再在tahoe B多个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态(包括刚才从tahoe A插过来的盘)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插入同一集群的tahoe A设备</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，UI提示客户这些盘属于另外一台tahoe B，让客户选择是继续加入集群还是重新插，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	在UI上选择部分盘重新插，部分盘继续加入集群</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将部分盘重新插回tahoe B</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，id不变</p>
<p>
	选择继续加入的硬盘有大量的数据迁移，日志和告警有相应事件，之前B上对应的osd 5分钟后out并且开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[16]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移正常完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[17]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-16 3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10213" name="多tahoe多节点多盘来回拔插(5-10分钟迁移过程中)">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[615]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个</div>
<div>
	&nbsp;</div>
<div>
	osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A的多个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应的OSD变为out，开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	&nbsp;在迁移过程中将拔出的硬盘插入同一集群的tahoe B设备</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，UI提示客户这些盘属于另外一台tahoe A，让客户选择是继续加入集群还是重新插，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在UI上选择继续加入</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	新建的OSD为UP，新加入的硬盘有大量数据迁移，原OSD数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移完后之前的osd被集群删除，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	再在tahoe B多个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态(包括刚才从tahoe A插过来的盘)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应的OSD变为out，开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<div>
	将拔出的硬盘插入同一集群的tahoe A设备</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	&nbsp;</div>
<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，UI提示客户这些盘属于另外一台tahoe B，让客户选择是继续加入集群还是重新插，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	在UI上选择部分硬盘重新插，部分硬盘继续加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	将重新插的盘插回tahoe B</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[16]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，所有加入的硬盘都有大量的数据迁移，日志和告警有相应事件，之前的osd数据迁移继续</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[17]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移正常完成后之前的osd被集群删除，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[18]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-17 3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10232" name="多tahoe多节点多盘来回拔插(10分钟后迁移已完成)">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[616]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个</div>
<div>
	&nbsp;</div>
<div>
	osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A的多个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	5分钟后观察集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应的OSD变为out，开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移完成后out的osd被集群删除，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插入同一集群的tahoe B设备</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群 勾选新盘直接加入也不会直接加入)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，UI提示客户这些盘属于另外一台tahoe A，让客户选择是继续加入集群还是重新插，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在UI上选择继续加入</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	新建的OSD为UP，新加入的硬盘有大量数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<div>
	等待数据迁移完成</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移完后之前的osd被集群删除，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	再在tahoe B多个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态(包括刚才从tahoe A插过来的盘)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<div>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<div>
	5分钟后观察集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应的OSD变为out，开始数据迁移</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移完成后out的osd被集群删除，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插入同一集群的tahoe A设备</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[16]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群 勾选新盘直接加入也不会直接加入)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，UI提示客户这些盘属于另外一台tahoe B，让客户选择是继续加入集群还是重新插，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[17]]></step_number>
	<actions><![CDATA[<p>
	在UI上选择部分硬盘重新插，部分继续加入集群</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[18]]></step_number>
	<actions><![CDATA[<div>
	将选择重插的盘重新插回tahoe B</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[19]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，所有加入的硬盘有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[20]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移正常完成后之前的osd被集群删除，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[21]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-20 3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10415" name="多tahoe多节点多新盘同时加入">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[627]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将多个新盘同时插入多个tahoe多个节点的空余槽位(包括同时插入单个节点和同时插入多个节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在多台tahoe同时点击扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2-4多次</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面的结果相同</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10433" name="多tahoe多节点多新盘轮流加入">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[629]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将多个新盘插入tahoe A的多个节点的空余槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	点击扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将多个新盘插入tahoe B的多个节点的空余槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	数据迁移完成，io读写正常，集群状态正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2-6</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10442" name="多tahoe多节点多新盘轮流替换">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[630]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A多个节点的硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将新的硬盘插入刚拔出的槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新的OSD进程状态为up，id变化，新加入的硬盘有大量的数据迁移，日志和告警有相应事件，之前的osd进程5分钟后out并且开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常,之前的osd进程被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<div>
	在tahoe B多个节点的硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	将新的硬盘插入刚拔出的槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<div>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新的OSD进程状态为up，id变化，新加入的硬盘有大量的数据迁移，日志和告警有相应事件，之前的osd进程5分钟后out并且开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常,之前的osd进程被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10456" name="多tahoe多节点加入之前使用过的盘">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[631]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	另外准备6块之前设备上使用过的硬盘(硬盘分区label和正在使用的硬盘一样,集群id不同)</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对目录/root/nbd0停业务后进行文件备份到本地</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将准备好的6块盘插入两台tahoe设备，每个节点1块盘，然后点击扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	系统能识别到盘，盘在UI上能正确显示，并且提示这些盘来自其他集群，是否继续加入集群</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	选择否</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	没有相应的OSD创建，集群状态正常，IO读写正常，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将这些盘下电后拔插一下，再次点击扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	系统能识别到盘，盘在UI上能正确显示，并且提示这些盘来自其他集群，是否继续加入集群</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	选择是</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	相应的OSD成功创建，并且有大量的数据迁移到新加入的盘，集群状态正常，IO读写正常，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	将备份的nbd0的数据与当前的数据做一致性比较</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中将这些盘中的2个再次下电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应的osd变为down，上面的数据迁移暂停，其他osd数据迁移继续，集群状态正常，io读写正常，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	5分钟内再次点击扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应的osd变为up，上面的数据迁移继续，集群状态正常，IO读写正常，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，IO读写正常，数据迁移到新的盘并且均衡</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10467" name="单tahoe多节点多盘拔出后设备链路断开(5分钟内)">
	<node_order><![CDATA[23]]></node_order>
	<externalid><![CDATA[632]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A的每个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内短时间中断该tahoe 所有节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	5分钟内短时间中断该tahoe 所有节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	fio读写正常，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	5分钟内长时间中断该tahoe 所有节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	5分钟内长时间中断该tahoe 所有节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原tahoe</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他tahoe重复步骤3-10(可以拔多块盘，插回去的时候可以在tahoe内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10503" name="单tahoe多节点多盘拔出后设备链路断开(5-10分钟迁移过程中)">
	<node_order><![CDATA[24]]></node_order>
	<externalid><![CDATA[633]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A的每个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out了，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断该tahoe所有节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断该tahoe 所有节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断该节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断该节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写正常，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新创建的OSD进程状态为up，id不变，新加入的硬盘有大量的数据迁移，日志和告警有相应事件，之前的osd仍为out，数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据顺利完成迁移，之前out的osd被删除，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他tahoe重复步骤3-12(可以拔多块盘，插回去的时候可以在tahoe内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面的结果相同</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10519" name="单tahoe多盘拔插后设备链路断开(10分钟后迁移已完成)">
	<node_order><![CDATA[25]]></node_order>
	<externalid><![CDATA[634]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A每个节点的硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程out，数据开始迁移，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成后插回之前拔出的硬盘，点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，之前的osd进程被删除，插入后新创建osd并且开始数据迁移，硬盘总数加1，OSD进程状态为up，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断该tahoe 所有节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断该tahoe 所有节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将该tahoe所有节点的public网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将该tahoe所有节点的cluster网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他tahoe 重复步骤3-10(可以拔多块盘，插回去的时候可以在tahoe内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10532" name="单tahoe多节点多盘拔出后设备掉电上电(5分钟内)">
	<node_order><![CDATA[26]]></node_order>
	<externalid><![CDATA[635]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A的硬盘概观的每个节点选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内设备整体掉电，5分钟后上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，数据一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	起来后通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	之前被拔盘所对应的osd仍然为down ？ 无数据迁移，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<div>
	将拔出的硬盘插回原节点</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在tahoe B重复步骤3-8(插回去的时候可以在tahoe内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10546" name="单tahoe多节点多盘拔出后设备掉电上电(5-10分钟迁移过程中)">
	<node_order><![CDATA[27]]></node_order>
	<externalid><![CDATA[636]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对</div>
<div>
	nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A硬盘概观每个节点选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd 进程变为out，开始数据迁移，io读写正常，数据一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中设备掉电，5分钟后上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，数据一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	起来后通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	之前被拔盘所对应的osd仍然为out，数据迁移继续，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新创建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件，之前的osd仍为out，数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	迁移完成后out的osd被删除，io读写正常，数据分布均匀</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<div>
	在数据迁移完成后在tahoe B重复步骤3-10(可以拔多块盘，插回去的时候可以在tahoe内随意选择槽位)</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10559" name="单tahoe多节点多盘拔插后设备掉电上电(10分钟后迁移已完成)">
	<node_order><![CDATA[28]]></node_order>
	<externalid><![CDATA[637]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A硬盘概观的每个节点选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	osd进程out，数据迁移开始，io读写正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成后out的osd被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	插回之前拔出的硬盘，点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	io读写正常，插入后新创建osd并且开始数据迁移，硬盘总数加1，OSD进程状态为up，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中设备整体掉电上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	起来后通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，io读写正常，数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在tahoe B重复步骤3-9(可以拔多块盘，插回去的时候可以在tahoe内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10585" name="停io后单tahoe多节点多盘拔插(10分钟后)">
	<node_order><![CDATA[29]]></node_order>
	<externalid><![CDATA[638]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，停止io读写</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A硬盘概观的每个节点选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，OSD进程状态为up？osd进程不存在了，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，OSD进程状态为up，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程仍然为up，没有数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	10分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程仍然为up，没有数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回tahoe A(可以换节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，OSD进程重新启动，状态为up，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	对tahoe A的其他盘重复步骤3-9</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面的结果相同</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	对tahoe B操作步骤3-10</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10612" name="停io后多tahoe多节点多盘拔插(10分钟后)">
	<node_order><![CDATA[30]]></node_order>
	<externalid><![CDATA[639]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，停止io读写</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在tahoe A 和tahoe B的硬盘概观的选择多个节点的多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，OSD进程状态为up，osd进程不存在了，无数据迁移，系统性能基本无变化</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减1，OSD进程状态为up，无数据迁移，系统性能基本无变化，日志和告警有相应事件，集群状态为ok</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程为up， 集群状态为ok</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	10分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程仍然为up，开始检查盘是否插回</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原tahoe(盘可以在原tahoe内随意插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数增加，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件，无数据不一致，集群状态恢复正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	启动io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10636" name="停io后多tahoe多节点插入新盘">
	<node_order><![CDATA[31]]></node_order>
	<externalid><![CDATA[640]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，停止io读写</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将新盘插入tahoe A和tahoeB的空余槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在插入盘的节点 点击扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	启动io读写，等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2-4多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10643" name="停io后多tahoe多节点插入之前使用过的盘">
	<node_order><![CDATA[32]]></node_order>
	<externalid><![CDATA[641]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量</div>
<div>
	6，停止io读写</div>
<div>
	7，准备几块其他集群使用过的盘</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	集群状态正常，无数据迁移，OSD进程正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将之前使用过的盘插入tahoe A和tahoe B的空余槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在插入盘的节点 点击扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	提示该盘为其他集群的盘，让客户确认是否继续加入该集群</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	点击确认后</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	启动io读写，等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2-4多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10652" name="集群整体下电后tahoe内多盘交换位置后上电(硬盘tahoe内漫游)">
	<node_order><![CDATA[33]]></node_order>
	<externalid><![CDATA[642]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	停业务后对目录/root/nbd0下的文件备份</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将设备tahoe A和tahoe B下电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	下电后在每个tahoe内打乱磁盘位置后，设备上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	设备上电成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	检查集群状态，OSD状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，OSD状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	比较备份文件和原文件的数据一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重新启动io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，集群状态正常，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10668" name="集群整体下电后多tahoe多节点插入新盘和之前使用过的盘后上电">
	<node_order><![CDATA[34]]></node_order>
	<externalid><![CDATA[644]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	停业务后对目录/root/nbd0下的文件备份</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将设备tahoe A和tahoe B下电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	下电后在tahoe A 和tahoe B的每个节点插入新盘和之前使用过的盘并做好记录，然后上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	设备上电成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	上电后开始io读写，然后去节点信息里面查看插入的盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对于新盘直接加入集群，对于之前使用过的老盘，UI提示客户是老盘，确认是否加入集群，io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	选择部分老盘加入集群，部分不选择加入</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	加入的盘创建对应的osd，并且开始数据迁移，选择不加入的盘不创建对于的osd，io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，数据分布均匀，io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10686" name="集群整体下电后多tahoe多节点互换盘后上电">
	<node_order><![CDATA[35]]></node_order>
	<externalid><![CDATA[646]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	停业务后对目录/root/nbd0下的文件备份</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将设备tahoe A和tahoe B下电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	下电后在tahoe A和tahoe B之间打乱部分磁盘位置并且记录后，设备上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	设备上电成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	检查集群状态，OSD状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	打乱顺序的盘对应的osd为down，未打乱顺序的osd为up，集群状态为error？</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	进入节点页面查看</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	打乱顺序的盘UI提示客户这些盘来自其他tahoe，让客户选择是否继续加入集群还是插到正确的tahoe</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	客户选择插入正确的tahoe 并且到设备上拔插到正确tahoe</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	重新扫描后之前down的osd变为up，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	启动io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，集群状态正常，日志和告警有相应事件，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="12458" name="带io下多tahoe多节点多盘跨故障域拔插(5分钟内)">
	<node_order><![CDATA[36]]></node_order>
	<externalid><![CDATA[652]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，</div>
<div>
	&nbsp;</div>
<div>
	每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	停止对nbd0 的读写，对nbd0上的文件进行备份</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	通过命令ceph pg dump 查询某个pg对应的副本pg在哪些盘上(覆盖三副本pool以及ec pool)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在io过程中将这些盘同时拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io中断，集群状态变为error，但os不会crush</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	将这些盘插回集群，比较nbd0和备份数据</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复正常，备份数据一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	重新启动io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	对其他pg对应的盘重复步骤4-7 5次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite>
<testcase internalid="8105" name="单节点单盘插回原槽位(5分钟内)">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[543]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	个数和状态正确，OSD进程正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	选择同一节点下的另外一个硬盘点击关闭</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<div>
	关闭后直接点击重新扫描</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘显示不为空，硬盘总数相应增加，io读写正常，OSD进程状态为up，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-10多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9108" name="单节点单盘插回原槽位(5-10分钟迁移过程中)">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[578]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	取消新盘插入直接加入集群选项</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在某个节点下选择1个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	7分钟后数据迁移过程中将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置硬盘显示为数据过期的盘，点击盘 选择关闭SSD</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	关闭后拔出硬盘，勾选上新盘插入后直接加入集群选项</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	再把盘插入槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9398" name="单节点单盘插回原槽位(10分钟后迁移已完成)">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[582]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	个数和状态正确，OSD进程正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在某个节点下选择1个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	10分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成，osd进程从crush tree删除后 将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8127" name="单节点单盘插入新槽位(5分钟内)">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[545]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	查看硬盘个数和状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回其他槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	&nbsp;点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	选择同一节点下的另外一个硬盘点击关闭</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	关闭后直接点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘显示不为空，硬盘总数不变，io读写正常，OSD进程状态为up，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-9多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8137" name="单节点单盘插入新槽位(5-10分钟迁移过程中)">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[546]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	7分钟后在数据迁移过程中将拔出的硬盘插回其他槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9549" name="单节点单盘插入新槽位(10分钟后迁移已完成)">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[589]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进</div>
<div>
	行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	10分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成，osd进程从crush tree删除后 将拔出的硬盘插入新槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	数据迁移完成，io读写正常，集群状态正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="12270" name="多节点多盘来回拔插(5分内)">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[647]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择节点A的多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插入节点C的槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后在其他节点对应位置的硬盘有显示，并且提示是来自节点A的盘，选择继续加入集群后，硬盘总数增加，io读写正常，新创建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	之前节点A的osd数据迁移完成后被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	将刚才插入C节点的盘下电后再插回到A节点</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	C节点上的盘对应的osd 变为down</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后插回A的盘对应位置的硬盘有显示，并且提示是来自节点C的盘</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	部分盘选择继续加入集群后，部分盘选择重新插</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	将选择重新插的盘插回C后，C节点硬盘总数增加，io读写正常，重新启动的OSD进程状态为up</p>
<p>
	选择继续加入集群的盘在A节点新建OSD，并且有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	C节点拔出盘对应的osd被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-11 3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="12272" name="多节点多盘来回拔插(5-10分钟迁移过程中)">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[648]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择节点A的多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	OSD进程变为down out，开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	迁移过程中将拔出的硬盘插入节点C的槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后在其他节点对应位置的硬盘有显示，并且提示是来自节点A的盘，选择继续加入集群后，硬盘总数增加，io读写正常，新创建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	之前节点A的osd数据迁移完成后被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	将刚才插入C节点的盘下电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	C节点上的盘对应的osd 变为down</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	C节点进程变为down out， 开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	将刚才下电的SSD再插回到A节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后插回A的盘对应位置的硬盘有显示，并且提示是来自节点C的盘</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	部分盘选择重新插拔，部分盘选择继续加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	将选择重新插拔的盘插回C节点，C节点的硬盘总数增加，io读写正常，新创建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件，之前的OSD数据迁移继续</p>
<p>
	选择继续加入集群的盘在A节点创建OSD，并且有大量的数据迁移，日志和告警有相应事件，之前的OSD数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应的osd被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-14 3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="12274" name="多节点多盘来回拔插(10分钟后迁移已完成)">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[649]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	点击总览信息里面的单个节点</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择节点A的多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减少，io读写正常，OSD进程状态为down，无数据迁移</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	OSD 进程down，out 并且开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插入节点C的槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群 勾选新盘直接加入集群也不会直接加入)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后在其他节点对应位置的硬盘有显示，并且提示是来自节点A的盘，选择继续加入集群后，硬盘总数增加，io读写正常，新创建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	将刚才插入C节点的盘下电</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减少，io读写正常，OSD进程状态为down，无数据迁移</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	OSD 进程down，out 并且开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	C节点拔出盘对应的osd被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	将刚下电的SSD插回A节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群 勾选新盘直接加入集群也不会直接加入)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后插回A的盘对应位置的硬盘有显示，并且提示是来自节点C的盘</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	将部分盘选择继续加入集群，部分盘选择重新插</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	选择继续加入集群的盘，A节点硬盘总数增加，io读写正常，新创建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
<p>
	选择重新插的盘插回C后重新创建OSD，新建的OSD有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[16]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[17]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-16 3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8151" name="单节点多盘插回原槽位(5分钟内)">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[548]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在某个节点上选择多个硬盘逐个点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将所有拔出的硬盘插回原槽位(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	迁移完成后在其他节点重复步骤3-7多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8161" name="单节点多盘插回原槽位(5-10分钟迁移过程中)">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[549]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在某个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	7分钟后在数据迁移过程中将拔出的硬盘插回原槽位(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9409" name="单节点多盘插回原槽位(10分钟后迁移已完成)">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[583]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在某个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	10分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等数据迁移完成，并且对应的osd从crush tree删除后将拔出的硬盘插回原槽位(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	数据迁移完成，io读写正常，集群状态正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8多次</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面的结果相同</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8171" name="单节点多盘交换位置插回槽位(5分钟内)(硬盘节点内漫游)">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[550]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	在单个节点选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘打乱顺序后插回槽位(包括两两互换位置的情况)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	OSD进程正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<div>
	点击重新扫描</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-7 多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8179" name="单节点多盘交换位置插回槽位(5-10分钟迁移过程中)(硬盘节点内漫游)">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[551]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	个数和状态正确，OSD进程正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在单个节点选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	7分钟后在数据迁移过程中将拔出的硬盘打乱顺序后插回槽位(包括两两互换位置的情况)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，id发生变化，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8 多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9420" name="单节点多盘交换位置插回槽位(10分钟后迁移已完成)(硬盘节点内漫游)">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[584]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在单个节点选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	10分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成并且osd进程从crush tree删除后将拔出的硬盘打乱顺序后插回槽位(包括两两互换位置的情况)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，id发生变化，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	数据迁移完成，io读写正常，集群状态正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-8 多次</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面的结果相同</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8230" name="单节点双盘拔出单盘插回">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[554]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	个数和状态正确，OSD进程正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在单个节点选择1个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程1状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	10分钟后观察集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程1 out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	10分钟后将同一节点的另外一个硬盘关闭</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程2状态为down，数据迁移正常进行，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将第二次拔出的硬盘插入第一次拔盘的槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，新加入的硬盘不会有大量的数据迁移，之前的OSD进程数据迁移完成后从crush tree移除，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9587" name="单节点双盘拔出双盘插回">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[590]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在单个节点选择1个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程1状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5-10分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程1 out，系统正在进行数据迁移，系统性能略有下降</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	将同一节点的另外一个硬盘关闭</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程2状态为down，数据迁移正常进行，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将第二次拔出的硬盘插入第一次拔盘的槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移正常，扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，新加入的硬盘不会有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	将第一次拔出的硬盘插入到第二次拔出硬盘的槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移正常，扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新的OSD进程状态为up，新加入的硬盘有大量的数据迁移，之前的OSD进程数据迁移完成后从crush tree移除，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<div>
	等待数据迁移完成</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8266" name="单节点单盘反复拔插(5分钟内)">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[555]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原槽位(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	对该盘重复步骤2-6 5次&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面对应结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8437" name="单节点单盘反复拔插(5-10分钟迁移过程中)">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[562]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<div>
	5分钟后观察集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降 &nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	7分钟后在数据迁移过程中将拔出的硬盘插回其他槽位(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id发生变化，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中对同一块盘重复步骤3-7 5次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面对应的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9431" name="单节点单盘反复拔插(10分钟后迁移已完成)">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[585]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态(包括同时拔出)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降 &nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成并且osd进程从crush tree删除后将拔出的硬盘插回其他槽位(包括同时插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id发生变化，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中对同一块盘重复步骤3-7 5次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面对应的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8304" name="单节点多盘轮流拔插(5分钟内)">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[557]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	对该节点的其他多个盘轮流重复步骤3-6 &nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面对应结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8306" name="单节点多盘轮流拔插(5-10分钟迁移过程中)">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[558]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降 &nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	7分钟后在数据迁移过程中将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中对同一节点的其他盘轮流重复步骤3-6&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的对应结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-7多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9442" name="单节点多盘轮流拔插(10分钟后迁移已完成)">
	<node_order><![CDATA[23]]></node_order>
	<externalid><![CDATA[586]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	10分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成并且osd进程在crush tree上删除后将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<div>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中对同一节点的其他盘轮流重复步骤3-6&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的对应结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-7多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8324" name="多节点多盘轮流拔插(5分钟内)">
	<node_order><![CDATA[24]]></node_order>
	<externalid><![CDATA[559]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化</div>
<div>
	，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	对该节点的其他多个盘轮流重复步骤3-6 &nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面对应结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在集群没有数据迁移的情况下对其他节点重复步骤3-8</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面对应的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8458" name="多节点多盘轮流拔插(5-10分钟迁移过程中)">
	<node_order><![CDATA[25]]></node_order>
	<externalid><![CDATA[563]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<div>
	在硬盘概观选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降 &nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	7分钟后将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中对同一节点的其他盘轮流重复步骤3-6 多次&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的对应结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	等数据迁移完成后在其他节点重复步骤3-7&nbsp;</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面对应的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9454" name="多节点多盘轮流拔插(10分钟后迁移已完成)">
	<node_order><![CDATA[26]]></node_order>
	<externalid><![CDATA[587]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	10分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	io读写正常，对应的osd进程out，系统正在进行数据迁移，系统性能略有下降 &nbsp;</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成并且osd进程从crush tree删除后将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中对同一节点的其他盘轮流重复步骤3-6 多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的对应结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<div>
	等数据迁移完成后在其他节点重复步骤3-7&nbsp;</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面对应的结果相同</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	数据迁移完成，io读写正常，集群状态正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="12469" name="多节点多盘逐个拔插(5分钟内 磁盘清理场景)">
	<node_order><![CDATA[27]]></node_order>
	<externalid><![CDATA[653]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，</div>
<div>
	&nbsp;</div>
<div>
	每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	选择单个节点，从节点内逐个拔出硬盘，5分钟内插回</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，插回后osd状态恢复正常，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	单个节点操作完后等数据迁移完成集群状态恢复为ok后再操作其他节点</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	所有磁盘被拔插一次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8146" name="多节点新盘同时加入">
	<node_order><![CDATA[28]]></node_order>
	<externalid><![CDATA[547]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将多个新盘同时插入多个节点的空余槽位(包括同时插入同一节点和同时插入多个节点)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群）</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2-4多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10417" name="多节点新盘轮流加入">
	<node_order><![CDATA[29]]></node_order>
	<externalid><![CDATA[628]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	集群状态正常，无数据迁移，OSD进程正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将1个新盘插入节点A的空余槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将第2个新盘插入节点B的空余槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	将第3个新盘插入节点C的空余槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<div>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8190" name="单节点单盘新盘替换">
	<node_order><![CDATA[30]]></node_order>
	<externalid><![CDATA[552]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将新的硬盘插入刚拔出的槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描(覆盖新盘直接加入集群和手动加入集群 )</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新的OSD进程状态为up，id变化，新加入的硬盘有大量的数据迁移，日志和告警有相应事件，之前的osd5分钟后out并且开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常,之前的osd进程被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	重复步骤3-7多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9501" name="单节点所有盘新盘替换">
	<node_order><![CDATA[31]]></node_order>
	<externalid><![CDATA[588]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	查看硬盘个数和状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在一个节点上将上面的所有硬盘下电，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，超过5分钟的硬盘有数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将多个新盘插入拔出硬盘的节点，点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常,之前的osd进程被删除</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在其他其他节点重复步骤3-6</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9289" name="每个节点加入之前使用过的硬盘">
	<node_order><![CDATA[32]]></node_order>
	<externalid><![CDATA[579]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	&nbsp;</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	另外准备6块之前设备上使用过的硬盘(硬盘分区label和正在使用的硬盘一样,集群id不同)</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	对目录/root/nbd0停业务后进行文件备份到本地</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将准备好的6块盘插入设备，每个节点2块盘，然后点击重新扫描(覆盖新盘直接加入集群和手动加入集群)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	系统能识别到盘，盘在ＵＩ上能正确显示，相应的OSD成功创建，集群状态正常，IO读写正常，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将备份的nbd0的数据与当前的数据做一致性比较</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移到新的盘并且均衡</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9695" name="新盘加入后节点链路断开">
	<node_order><![CDATA[33]]></node_order>
	<externalid><![CDATA[597]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在某个节点下新插入新盘</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断该节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断该节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断该节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断该节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的public网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的cluster网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据顺利完成迁移，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他节点重复步骤3-11</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8502" name="新盘加入后节点重启">
	<node_order><![CDATA[34]]></node_order>
	<externalid><![CDATA[564]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在某个节点下插入新盘</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中将该盘所在节点重启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移暂停，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	等节点重启完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	节点重启完成后数据迁移继续进行，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中再次将该节点重启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移暂停，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	等节点再次重启完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	节点重启完成后数据迁移继续进行，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	迁移完成后在其他节点重复步骤3-13</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9709" name="新盘加入后设备掉电上电">
	<node_order><![CDATA[35]]></node_order>
	<externalid><![CDATA[598]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在每个节点下插入新盘</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中将设备掉电，5分钟后上电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	上电后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移继续，io继续下发，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移过程中将某个节点重启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	该节点数据迁移暂停，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	等节点重启完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	节点重启完成后数据迁移继续进行，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<div>
	等待数据迁移完成</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	迁移完成后在其他节点重复步骤3-9</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9721" name="单节点盘拔出后节点链路断开(5分钟内)">
	<node_order><![CDATA[36]]></node_order>
	<externalid><![CDATA[599]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在某个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内短时间中断该节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	5分钟内短时间中断该节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	5分钟内长时间中断该节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	5分钟内长时间中断该节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	5分钟内将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他节点重复步骤3-10(可以拔多块盘，插回去的时候可以在节点内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8357" name="单节点盘拔出后节点链路断开(5-10分钟后迁移过程中)">
	<node_order><![CDATA[37]]></node_order>
	<externalid><![CDATA[561]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在某个节点下选择多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数相应减少，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	io读写正常，对应的osd进程out了，系统正在进行数据迁移，系统性能略有下降</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断该节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<div>
	在迁移过程中，短时间中断该节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断该节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断该节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的public网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<div>
	在迁移过程中，将所有节点的cluster网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，数据迁移正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[14]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[15]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新创建的OSD进程状态为up，id不变，新加入的硬盘有大量的数据迁移，日志和告警有相应事件，之前的osd仍为out，数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[16]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据顺利完成迁移，之前out的osd被集群删除，集群状态正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[17]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他节点重复步骤3-16(可以拔多块盘，插回去的时候可以在节点内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9738" name="单节点盘拔插后节点链路断开(10分钟后迁移已完成)">
	<node_order><![CDATA[38]]></node_order>
	<externalid><![CDATA[600]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	个数和状态正确，OSD进程正常</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移正常，io读写正常，osd进程out</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成后插回之前拔出的硬盘，点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，之前的osd进程被删除，插入后新创建osd并且开始数据迁移，硬盘总数加1，OSD进程状态为up，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断该节点的public网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，短时间中断该节点的cluster网络（拔出插回在5秒内），观察集群状态，io状态，重复操作3次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路连接正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断该节点的public网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，长时间中断该节点的cluster网络（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<div>
	在迁移过程中，将所有节点的public网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<div>
	fio读写挂起，待链路恢复后，fio读写继续下发，没有一致性问题，集群状态正常，数据迁移正常</div>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[12]]></step_number>
	<actions><![CDATA[<p>
	在迁移过程中，将所有节点的cluster网络长时间中断（拔出插回间隔1分钟），观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio归零在30秒以内，插回后，链路正常，没有一致性问题，集群状态正常，数据迁移正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[13]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他节点重复步骤3-12(可以拔多块盘，插回去的时候可以在节点内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9601" name="单节点盘拔出后节点重启(5分钟内)">
	<node_order><![CDATA[39]]></node_order>
	<externalid><![CDATA[591]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	重启节点</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	起来后通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	之前被拔盘所对应的osd仍然为down ？ 无数据迁移，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原槽位(同时覆盖5分钟后再插回，看是否走out 迁移流程)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他节点重复步骤3-8(可以拔多块盘，插回去的时候可以在节点内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9623" name="单节点盘拔出后节点重启(5-10分钟后迁移过程中)">
	<node_order><![CDATA[40]]></node_order>
	<externalid><![CDATA[593]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd 进程变为out后被删除，开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重启节点</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	起来后通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移继续，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新创建的OSD进程状态为up，id不变，新加入的硬盘有大量的数据迁移，日志和告警有相应事件，数据迁移继续</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他节点重复步骤3-8(可以拔多块盘，插回去的时候可以在节点内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9647" name="单节点盘拔插后节点重启(10分钟后迁移已完成)">
	<node_order><![CDATA[41]]></node_order>
	<externalid><![CDATA[595]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，osd进程out后被删除，数据开始迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成后插回之前拔出的硬盘，点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，插入后新创建osd并且开始数据迁移，硬盘总数加1，OSD进程状态为up，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中重启节点</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，数据迁移暂停</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	起来后通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，io读写正常，数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他节点重复步骤3-8(可以拔多块盘，插回去的时候可以在节点内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9612" name="单节点盘拔出后设备掉电上电(5分钟内)">
	<node_order><![CDATA[42]]></node_order>
	<externalid><![CDATA[592]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟内设备整体掉电，5分钟后上电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	起来后通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	之前被拔盘所对应的osd仍然为down ？ 无数据迁移，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，OSD进程状态为up，id不变，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他节点重复步骤3-8(可以拔多块盘，插回去的时候可以在节点内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面的结果相同</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9635" name="单节点盘拔出后设备掉电上电(5-10分钟后迁移过程中)">
	<node_order><![CDATA[43]]></node_order>
	<externalid><![CDATA[594]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd 进程变为out后被删除，开始数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中设备掉电，5分钟后上电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	起来后通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移继续，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，io读写正常，新创建的OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件，数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	在数据迁移完成后在其他节点重复步骤3-8(可以拔多块盘，插回去的时候可以在节点内随意选择槽位)</p>
]]></actions>
	<expectedresults><![CDATA[<div>
	与上面的结果相同</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9658" name="单节点单盘拔插后设备掉电上电(10分钟后迁移已完成)">
	<node_order><![CDATA[44]]></node_order>
	<externalid><![CDATA[596]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<div>
	查看硬盘个数和状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，io读写正常，OSD进程状态为down，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后观察集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，osd进程out后被删除，数据开始迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等待数据迁移完成后插回之前拔出的硬盘，点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，插入后新创建osd并且开始数据迁移，硬盘总数加1，OSD进程状态为up，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	数据迁移过程中设备整体掉电上电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	起来后通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，io读写正常，数据迁移继续</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<div>
	在数据迁移完成后在其他节点重复步骤3-8(可以拔多块盘，插回去的时候可以在节点内随意选择槽位)</div>
<div>
	&nbsp;</div>
<p>
	&nbsp;</p>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9773" name="停io后单节点单盘拔插(10分钟后)">
	<node_order><![CDATA[45]]></node_order>
	<externalid><![CDATA[602]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，写完后停IO</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择一个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，OSD进程状态为up？osd进程不存在了，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，OSD进程状态为up，无数据迁移，系统性能基本无变化，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程仍然为up，没有数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	10分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程仍然为up，没有数据迁移</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回节点(可以换槽位)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数加1，OSD进程重新启动，状态为up，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[10]]></step_number>
	<actions><![CDATA[<p>
	对同一节点的其他盘重复步骤3-9</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[11]]></step_number>
	<actions><![CDATA[<p>
	对其他节点操作步骤3-10</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9786" name="停io后多节点多盘拔插(10分钟后)">
	<node_order><![CDATA[46]]></node_order>
	<externalid><![CDATA[603]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，停止io读写</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在硬盘概观选择多个节点的多个硬盘点击关闭，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭成功，OSD进程状态为up，osd进程不存在了，无数据迁移，系统性能基本无变化</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	关闭后去物理设备上拔出对应的硬盘，并通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘显示为空，硬盘总数减1，OSD进程状态为up，无数据迁移，系统性能基本无变化，日志和告警有相应事件，集群状态为error？</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	5分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程为up， 集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	10分钟后检查集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程为up，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	将拔出的硬盘插回原节点(盘可以在节点内随意插回)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	点击重新扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数增加，OSD进程启动并且状态为up，新加入的硬盘没有大量的数据迁移，系统性能基本无变化，日志和告警有相应事件，无数据不一致，集群状态恢复正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	启动io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9946" name="停io后多节点插入新盘">
	<node_order><![CDATA[47]]></node_order>
	<externalid><![CDATA[606]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，写完数据后停io</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将新盘插入空余槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	点击扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	扫描后对应位置的硬盘重新显示，硬盘总数相应增加，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	启动io读写，等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2-4多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9975" name="停io后多节点插入之前使用过的盘">
	<node_order><![CDATA[48]]></node_order>
	<externalid><![CDATA[607]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，停止io读写</div>
<div>
	7， 准备几块其他集群使用过的盘</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	通过ceph -s 和 ceph osd tree查看集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，无数据迁移，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将之前使用过的盘插入空余槽位</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	点击扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	提示该盘为其他集群的盘，让客户确认是否继续加入该集群</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	点击确认后</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对应位置的硬盘重新显示，硬盘总数相应增加，io读写正常，新建OSD进程状态为up，新加入的硬盘有大量的数据迁移，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	启动io读写，等待数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，io读写正常，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重复步骤2-4多次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9105" name="集群整体下电后节点内多盘交换位置后上电(硬盘节点内漫游)">
	<node_order><![CDATA[49]]></node_order>
	<externalid><![CDATA[577]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	停业务后对目录/root/nbd0下的文件备份</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将设备下电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	下电后在每个节点内打乱磁盘位置后，设备上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	设备上电成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	检查集群状态，OSD状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，OSD状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	比较备份文件和原文件的数据一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重新启动io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，集群状态正常，日志和告警有相应事件</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10660" name="集群整体下电后多节点插入新盘和之前使用过的盘后上电">
	<node_order><![CDATA[50]]></node_order>
	<externalid><![CDATA[643]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节</div>
<div>
	&nbsp;</div>
<div>
	点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<div>
	停业务后对目录/root/nbd0下的文件备份</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将设备下电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	下电后在每个节点插入新盘和之前使用过的盘并做好记录，然后上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	设备上电成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	上电后开始io读写，然后去节点信息里面查看插入的盘</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	对于新盘直接加入集群，对于之前使用过的老盘，UI提示客户是老盘，确认是否加入集群，io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	选择部分老盘加入集群，部分不选择加入</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	加入的盘创建对应的osd，并且开始数据迁移，选择不加入的盘不创建对于的osd，io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	等数据迁移完成</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	数据迁移完成，数据分布均匀，io读写正常，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="10676" name="集群整体下电后多节点互换盘后上电">
	<node_order><![CDATA[51]]></node_order>
	<externalid><![CDATA[645]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	停业务后对目录/root/nbd0下的文件备份</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将设备下电</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	下电后在节点间打乱磁盘位置并做好记录后，设备上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	设备上电成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<div>
	检查集群状态，OSD状态</div>
<div>
	&nbsp;</div>
]]></actions>
	<expectedresults><![CDATA[<p>
	打乱顺序的盘对应的osd为down，未打乱顺序的osd为up，集群状态为error？</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	进入节点页面查看</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	打乱顺序的盘UI提示客户这些盘来自其他节点，让客户选择是否继续加入集群还是插到正确的节点</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	客户选择插入正确的节点并且到设备上拔插到正确节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	点击扫描</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	重新扫描后之前down的osd变为up，集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	启动io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，集群状态正常，日志和告警有相应事件，无数据不一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="12446" name="带io下多节点多盘跨故障域拔插(5分钟内)">
	<node_order><![CDATA[52]]></node_order>
	<externalid><![CDATA[651]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[]]></summary>
	<preconditions><![CDATA[<div>
	1，node1，node2，node3,node4,node5,node6 一共6个主机节点,前3个节点来自一台tahoe，后3个节点来自一台tahoe，每个节点上若干个osd进程，其中node1是leader mon</div>
<div>
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div>
	3，副本数为2</div>
<div>
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div>
	5，对若干个nbd 进行8k随机写数据预埋，保证每个osd下大概使用100G左右的使用容量。</div>
<div>
	6，将nbd0（100G）创建ext4文件系统，挂载到目录/root/nbd0, 启动fio对/root/nbd0写30G大小的文件，带fio校验，启动fio对nbd1裸盘进行随机写，带fio校验，启动fio对nbd2裸盘进行随机读</div>
<div>
	&nbsp;</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[2]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>1</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	点击总览信息里面的单个节点</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	查看硬盘个数和状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	个数和状态正确，OSD进程正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	停止对nbd0 的读写，对nbd0上的文件进行备份</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	通过命令ceph pg dump 查询某个pg对应的副本pg在哪些盘上(覆盖三副本pool以及ec pool)</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在io过程中将这些盘同时拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io中断，集群状态变为error，但os不会crush</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	将这些盘插回集群，比较nbd0和备份数据</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复正常，备份数据一致</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	重新启动io读写</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	对其他pg对应的盘重复步骤4-7 5次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	与上面的结果相同</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite></testsuite><testsuite id="1386" name="Snapshot" >
<node_order><![CDATA[10]]></node_order>
<details><![CDATA[<p>
	快照和clone都是COW，如果只是创建快照和clone，不会对object进行复制</p>
<p>
	在对原image进行写时，会将有快照标签的object全部拷贝（COW），为了保证COW的持续时间长，对原image的写限制带宽，如200K/s，fio参数rate=200k，在COW的过程中，进行故障注入。</p>
<p>
	clone在flatten时，会将快照的object完整拷贝出来，在拷贝的过程中，进行故障注入。</p>
<p>
	快照的备份，则是在快照导出和导入时进行故障注入</p>
]]></details>
<testsuite id="1521" name="全量快照rollback" >
<node_order><![CDATA[1]]></node_order>
<details><![CDATA[]]></details>

<testcase internalid="1524" name="关闭osd进程（mon与osd共存）">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[200]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件），对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1节点的三个osd进程关闭，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	进程关闭后，crush tree上将osd进程标记为down，更新osdmap</p>
<p>
	io短暂归零后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1节点的osd进程开启，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	进程开启成功，crush tree上标记为up，最后集群状态为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，在node2,node3上重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，将nbd0和nbd1的文件系统的io停止，nbd2，nbd3的读写继续。umount文件系统，unmap nbd0，nbd1，将nbd0和nbd1的快照回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount文件系统成功，unmap nbd成功，回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将nbd0和nbd1重新map，mount文件系统，diff -s比较两个文件20g.file，检查文件一致性，检查io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd map，mount文件系统成功，两个文件系统内的文件20g.file一致，没有出错</p>
<p>
	nbd2，nbd3的io没有影响，没有数据一致性问题，集群状态正常</p>
<p>
	&nbsp;</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1531" name="kill osd进程（mon与osd共存）">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[201]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1节点的三个osd进程kill -9，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	进程kill -9后，crush tree上将osd进程标记为down，更新osdmap</p>
<p>
	io短暂归零后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1节点的osd进程开启，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	进程开启成功，crush tree上标记为up，最后集群状态为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，在node2,node3上重复步骤1和步骤2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，将nbd0和nbd1的文件系统的io停止，nbd2，nbd3的读写继续。umount文件系统，unmap nbd0，nbd1，将nbd0和nbd1的快照回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount文件系统成功，unmap nbd成功，回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将nbd0和nbd1重新map，mount文件系统，diff -s比较两个文件20g.file，检查文件一致性，检查io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd map，mount文件系统成功，两个文件系统内的文件20g.file一致，没有出错</p>
<p>
	nbd2，nbd3的io没有影响，没有数据一致性问题，集群状态正常</p>
<div>
	&nbsp;</div>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1533" name="kill 所有进程（mon与osd共存）">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[202]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1，node2，node3上的所有osd，mon进程全部kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	kill -9所有进程后，osd进程所占用内存释放，集群不可用，无法查看集群状态</p>
<p>
	client端读写fio全部挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将所有的server节点的mon，osd进程都重启，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	所有进程启动成功，集群状态正常</p>
<p>
	io恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	读写运行5分钟后，nbd0，nbd1文件系统的io中断，nbd2，nbd3裸设备的io继续，umount文件系统，unmap nbd0，nbd1，对nbd0，nbd1，回滚。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount文件系统成功，unmap nbd成功，回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将nbd0，nbd1重新map，重新mount文件系统，diff -s比较两个文件系统的文件20g.file是否一致</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd map，mount文件系统成功</p>
<p>
	两个文件比较一致，nbd2，nbd3读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1535" name="服务器前端与Public Switch间长时间物理中断（mon与osd共存）">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[203]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，中断node1节点的public网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的osd在crush tree上标记为down，leader mon切换。</p>
<p>
	io短暂归零后恢复（归零30秒以内），没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1节点的public网络恢复，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	插回线缆后，连接恢复，osd重新标记为up，重新变为leader mon，集群状态最后为OK</p>
<p>
	io性能短时间降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群状态恢复ok后，在node2节点上重复步骤1和2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	运行io 5分钟后，将文件系统的io中断，nbd2，nbd3的io继续下发，umount文件系统，unmap nbd0，nbd1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	对nbd0，nbd1进行快照回滚，然后map，mount文件系统，比较两个文件系统里面的文件20g.file</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	回滚成功</p>
<p>
	map，mount文件系统成功，文件比较一致</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1537" name="服务器后端与Cluster Switch间长时间物理中断（mon与osd共存）">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[204]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，中断node1节点的cluster网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的osd在crush tree上标记为down，leader mon不切换。</p>
<p>
	io短暂归零后恢复（归零30秒以内），没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1节点的cluster网络恢复，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	插回线缆后，连接恢复，osd重新标记为up，集群状态最后为OK</p>
<p>
	io性能短时间降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群状态恢复ok后，在node2节点上重复步骤1和2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	运行io 5分钟后，将文件系统的io中断，nbd2，nbd3的io继续下发，umount文件系统，unmap nbd0，nbd1</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	对nbd0，nbd1进行快照回滚，然后map，mount文件系统，比较两个文件系统里面的文件20g.file</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	回滚成功</p>
<p>
	map，mount文件系统成功，文件比较一致</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1539" name="网络全部物理中断（mon与osd共存）">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[205]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1，node2，node3的public，cluster网络全部中断</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	fio读写全部挂起，集群不能查看状态，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，恢复所有节点的网络，观察集群状态，查看io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	网络恢复后，集群状态正常</p>
<p>
	fio恢复读写，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	运行5分钟后，中断文件系统读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统，unmap nbd0，nbd1，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap正常，集群状态正常</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将nbd0，nbd1的快照回滚，然后重新map nbd0，nbd1，mount文件系统，比较20g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	快照回滚成功，重新map，mount文件系统成功</p>
<p>
	文件一致，nbd2，nbd3的读写没有问题</p>
<p>
	集群状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1566" name="server端重启（mon与osd共存）">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[206]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中重启node1节点，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点重启后，该节点上的osd在crush tree上标记为down，leader mon切换</p>
<p>
	io短暂归零（归零时间30秒以内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node1节点重启后，启动该节点上的所有osd，mon进程，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动mon，osd进程成功，osd在crush tree上重新标记为up，mon变为leader mon，最后集群状态为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，将node1，node2，node3节点全部重启，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	server端都重启后，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待节点重启后，启动所有节点上的osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，集群状态ok</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1568" name="server端panic （mon与osd共存）">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[207]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中panic node1节点，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点重启后，该节点上的osd在crush tree上标记为down，leader mon切换</p>
<p>
	io短暂归零（归零时间30秒以内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node1节点重启后，启动该节点上的所有osd，mon进程，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动mon，osd进程成功，osd在crush tree上重新标记为up，mon变为leader mon，最后集群状态为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，将node1，node2，node3节点全部panic，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	server端都重启后，集群不可用</p>
<p>
	fio读写挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待节点重启后，启动所有节点上的osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，集群状态ok</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1582" name="逻辑踢盘（mon与osd共存）">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[208]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在node1上逻辑删除磁盘2个，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	踢出磁盘后，该磁盘上的osd进程退出，在crush tree上标记为down</p>
<p>
	io短暂归零（30秒以内），没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将踢出的磁盘重新加载，启动osd，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd成功，加入集群成功，最后集群状态为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复为OK后，在node2，node3上重复步骤1,2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1737" name="osd脱离、加入集群（mon与osd共存）">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[231]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node2上的两个osd out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	out过程中，有pg迁移，io状态正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	out过程完成后，将这两个osd进程关闭，1分钟后，重启该osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd关闭，重启成功，crush tree上看到是up状态，out状态</p>
<p>
	io正常无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将这两个osd重新加入集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	in的过程中有pg迁移，最后集群为ok状态</p>
<p>
	io正常无问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1739" name="删除osd（mon与osd共存）">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[232]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node3上的一个osd进程用脚本删除，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd上的pg迁移到其他pg，最后集群状态为OK</p>
<p>
	io读写没有问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1741" name="添加osd（mon与osd共存）">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[233]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node3上添加一个osd，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	部分pg迁移到该osd上，最后集群状态为OK</p>
<p>
	io读写没有问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1743" name="暂停集群（mon与osd共存）">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[234]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将整个集群暂停，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群暂停后，集群不能提供读写，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，将集群暂停取消，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可以提供读写服务，io继续下发，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1773" name="server端掉电（mon与osd共存）">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[235]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中将node1，node2节点掉电，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	掉电后，集群不可用，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node1，node2重新上电后，重启node1，node2上的所有osd、mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1，node2上的osd，mon启动成功，集群状态为OK</p>
<p>
	io恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1775" name="server端关机/开机（mon与osd共存）">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[236]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中将node2，node3 shutdown关机，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关机后，集群不可用，io挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node2，node3开机后，重启node2，node3上的所有osd、mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node2，node3上的osd，mon启动成功，集群状态为OK</p>
<p>
	io恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1777" name="server端非NTP同步（mon与osd共存）">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[237]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node2节点取消NTP时钟同步，调整node2的时间比集群时间快5分钟，观察集群状态5分钟，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node2上的osd在crush tree上标记为down，mon退出集群仲裁</p>
<p>
	io短暂归零后（归零30秒以内），恢复正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node2启动NTP同步时间，与集群时间相差0.05秒以内，重启该节点的所有osd、mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	NTP时钟同步成功</p>
<p>
	osd标记为up，mon加入仲裁，集群状态OK</p>
<p>
	io没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node3节点取消NTP时钟同步，调整node3的时间比集群时间慢5分钟，观察集群状态5分钟，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群有告警，时钟偏移</p>
<p>
	io正常没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	node3启动NTP同步时间，与集群时间相差0.05秒以内，重启该节点的所有osd、mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	NTP时钟同步成功</p>
<p>
	osd标记为up，mon加入仲裁，集群状态OK</p>
<p>
	io没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1796" name="PG数量扩展（mon与osd共存）">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[238]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将pool1，pool2的pg，pgp数量增加为原来的两倍，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pg，pgp数量扩展成功</p>
<p>
	集群状态为OK，io没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待集群状态为OK后，将pool1，pool2的pg，pgp数量增加为原来的4倍，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pg，pgp数量扩展成功</p>
<p>
	集群状态为OK，io没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1798" name="副本数调整（mon与osd共存）">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[239]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）和nbd1（pool2）都创建成ext4的文件系统，分别挂载到/nbd0 和/nbd1目录下，在/nbd0 目录下写一个20G大小的文件20g.file，完成后拷贝到/nbd1目录下</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，拷贝完成后，对nbd0和nbd1做快照</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对两个文件系统/nbd0 /nbd1进行写文件操作<span style="font-size: 13.3333px;">（一个写同名文件，rate=200k，保证COW持续的时间长，一个写不同名称的文件）</span><span style="font-size: 12px;">，对裸设备nbd2（pool1），nbd3（pool2）进行裸设备随机读写</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，调整pool1，pool2的副本数为3，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	副本数调整成功</p>
<p>
	集群状态正常，io正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	运行3分钟后，调整pool1，pool2的副本数为1，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	副本数调整成功</p>
<p>
	集群状态正常，io正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	运行3分钟后，调整pool1，pool2的副本数为2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	副本数调整成功</p>
<p>
	集群状态正常，io正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	io运行5分钟后，中断文件系统的读写，nbd2，nbd3的裸盘读写继续下发，umount文件系统nbd0，nbd1，unmap nbd0，nbd1，对nbd0，nbd1进行回滚，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	umount，unmap成功</p>
<p>
	nbd0，nbd1回滚成功</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	重新map nbd0，nbd1，mount文件系统，比较文件20g.file，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	map，mount成功</p>
<p>
	文件比较一致</p>
<p>
	nbd2，nbd3的读写不受影响，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0，nbd1的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="1522" name="增量快照导出导入" >
<node_order><![CDATA[2]]></node_order>
<details><![CDATA[]]></details>

<testcase internalid="1589" name="关闭osd进程（mon与osd共存）">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[209]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node2节点上的两个osd进程关闭，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭的osd在crush tree上标记为down</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导出正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后将关闭的osd进程开启，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，在crush tree上标记为up</p>
<p>
	io读写性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，将node3节点上的两个osd进程关闭，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关闭的osd在crush tree上标记为down</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后将关闭的osd进程开启，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，在crush tree上标记为up</p>
<p>
	io读写性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1596" name="kill osd进程（mon与osd共存）">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[210]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node2节点上的两个osd进程kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	kill的osd在crush tree上标记为down</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导出正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后将kill的osd进程开启，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，在crush tree上标记为up</p>
<p>
	io读写性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool2中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool2/image</p>
<p>
	在导入过程中，将node3节点上的两个osd进程kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	kill的osd在crush tree上标记为down</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后将kill的osd进程开启，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，在crush tree上标记为up</p>
<p>
	io读写性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool2/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1598" name="kill 所有进程（mon与osd共存）">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[211]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node1，node2，node3上所有osd，mon进程kill -9，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态不可查看，集群不可用</p>
<p>
	io挂起</p>
<p>
	导出快照停止，最后超时失败</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，启动node1，node2，node3上所有的osd，mon进程，观察集群状态，io状态</p>
<p>
	在client端重新将nbd0的快照s1导出到/diff</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，集群状态正常</p>
<p>
	io恢复，没有数据一致性问题</p>
<p>
	快照导出正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，将node1，node2，node3节点所有osd，mon进程kill -9 ，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态不可查看，集群不可用</p>
<p>
	io挂起</p>
<p>
	导入快照停止，最后超时失败</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，启动node1，node2，node3上所有的osd，mon进程，观察集群状态，io状态</p>
<p>
	在client端重新导入/diff到pool1/image</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，集群状态正常</p>
<p>
	io恢复，没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1600" name="服务器前端与Public Switch间长时间物理中断（mon与osd共存）">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[212]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node1节点的public网络中断，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的osd在crush tree上全部标记为down，leader mon切换</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后恢复node1节点的public网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	链接恢复，node1上的osd全部标记为up，node1的mon恢复为leader mon</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，将node3节点的public网路中断，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node3节点的osd在crush tree上标记为down，集群leader mon不变</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后恢复node3节点的public网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	连接恢复，node3节点的osd全部标记为up，该节点的mon重新加入集群仲裁，最后集群状态为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1602" name="服务器后端与Cluster Switch间长时间物理中断（mon与osd共存）">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[213]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node1节点的cluster网络中断，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的osd在crush tree上全部标记为down，leader mon切换</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后恢复node1节点的cluster网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	链接恢复，node1上的osd全部标记为up，node1的mon恢复为leader mon</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，将node3节点的cluster网路中断，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node3节点的osd在crush tree上标记为down，集群leader mon不变</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后恢复node3节点的cluster网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	连接恢复，node3节点的osd全部标记为up，该节点的mon重新加入集群仲裁，最后集群状态为OK</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1604" name="网络全部物理中断（mon与osd共存）">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[214]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node1，node2，node3节点的public，cluster网络全部中断，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用，fio读写挂起</p>
<p>
	快照导出挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后恢复node1，node2，node3节点的public，cluster网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，fio读写恢复，没有数据一致性问题</p>
<p>
	快照导出正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，将node1，node2，node3节点的public，cluster网络全部中断，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用，fio读写挂起</p>
<p>
	快照导入挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后恢复node1，node2，node3节点的public，cluster网络，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态正常，fio读写恢复，没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1606" name="server端重启（mon与osd共存）">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[215]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node1节点reboot，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的osd在crush tree上全部标记为down，leader mon切换</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node1节点重启成功后，启动所有该节点的osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1上的osd全部标记为up，node1的mon恢复为leader mon</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，将node3节点reboot，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node3节点的osd在crush tree上标记为down，集群leader mon不变</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	node3节点重启成功后，启动所有该节点的osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node3上的osd全部标记为up，node3的mon重新加入集群仲裁</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1608" name="server端panic（mon与osd共存）">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[216]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node1节点panic，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的osd在crush tree上全部标记为down，leader mon切换</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node1节点重启成功后，启动所有该节点的osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1上的osd全部标记为up，node1的mon恢复为leader mon</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool2中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool2/image</p>
<p>
	在导入过程中，将node3节点panic，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node3节点的osd在crush tree上标记为down，集群leader mon不变</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	node3节点重启成功后，启动所有该节点的osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node3上的osd全部标记为up，node3的mon重新加入集群仲裁</p>
<p>
	io性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool2/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1610" name="逻辑踢盘（mon与osd共存）">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[217]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，在node1节点上逻辑踢出两个SSD盘，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd在crush tree上标记为down</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导出正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后将两个SSD盘重新加载，重新启动osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，在crush tree上标记为up</p>
<p>
	io读写性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool2中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool2/image</p>
<p>
	在导入过程中，将node3节点上逻辑踢出两个SSD，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	被踢盘的osd在crush tree上标记为down</p>
<p>
	io短暂归零后恢复（io归零30秒以内），没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后将两个SSD盘重新加载，重新启动osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd进程启动成功，在crush tree上标记为up</p>
<p>
	io读写性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool2/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1811" name="Client端与Public Switch间长时间物理中断（mon与osd共存）">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[240]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将client节点的public网络中断，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	client端不能查看集群状态</p>
<p>
	client端上的io读写挂起</p>
<p>
	client端的快照导出挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，恢复client端的public网络，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	client端可以查看集群状态</p>
<p>
	client端的io读写继续下发</p>
<p>
	client端的快照导出继续，最后成功完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pool2中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool2/image</p>
<p>
	在导入过程中，将client节点的public网路中断，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	client端不能查看集群状态</p>
<p>
	client端上的io读写挂起</p>
<p>
	client端的快照导出挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，恢复client端的public网络，查看集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	client端可以查看集群状态</p>
<p>
	client端的io读写继续下发</p>
<p>
	client端的快照导入继续，最后成功完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool2/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1818" name="osd脱离、加入集群（mon与osd共存）">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[241]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="margin: 0px; padding: 0px; font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="margin: 0px; padding: 0px; font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node1节点的两个osd进程out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd out过程正常，有pg的迁移，最后集群为OK状态</p>
<p>
	io读写没有数据一致性问题</p>
<p>
	快照导出正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在pool2中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool2/image</p>
<p>
	在导入过程中，将node2节点的两个osd进程out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd out过程正常，有pg的迁移，最后集群为OK状态</p>
<p>
	io读写没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将pool2/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2 状态为out的osd，重新加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd加入集群成功，pg迁移，最后集群状态为OK</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1820" name="删除osd（mon与osd共存）">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[242]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="margin: 0px; padding: 0px; font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="margin: 0px; padding: 0px; font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，使用脚本删除node3上的一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除osd成功，有pg的迁移，最终集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，使用脚本删除node1上的一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除osd成功，有pg的迁移，最终集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	快照导入成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1822" name="添加osd（mon与osd共存）">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[243]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="margin: 0px; padding: 0px; font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="margin: 0px; padding: 0px; font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，使用脚本在node3上添加一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	添加osd成功，有pg的迁移，最终集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	集群恢复为OK后，在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，使用脚本在node1上添加一个osd进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	添加osd成功，有pg的迁移，最终集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	快照导入成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1824" name="暂停集群（mon与osd共存）">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[244]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="margin: 0px; padding: 0px; font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="margin: 0px; padding: 0px; font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，暂停集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群暂停后，集群不能提供读写服务</p>
<p>
	io挂起</p>
<p>
	快照导出过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，取消集群暂停，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	取消暂停后，集群可以提供读写服务</p>
<p>
	io继续下发，没有数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，暂停集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群暂停后，集群不能提供读写服务</p>
<p>
	io挂起</p>
<p>
	快照导入过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，取消集群暂停，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	取消暂停后，集群可以提供读写服务</p>
<p>
	io继续下发，没有数据一致性问题</p>
<p>
	快照导入成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1826" name="server端掉电（mon与osd共存）">
	<node_order><![CDATA[14]]></node_order>
	<externalid><![CDATA[245]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="margin: 0px; padding: 0px; font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="margin: 0px; padding: 0px; font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node1，node2掉电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	掉电后，集群不能提供读写服务</p>
<p>
	io挂起</p>
<p>
	快照导出过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node1，node2上电，重启该节点上的osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd，mon正常，集群状态ok</p>
<p>
	io继续下发，无数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pool2中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool2/image</p>
<p>
	在导入过程中，将node2，node3掉电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	掉电后，集群不能提供读写服务</p>
<p>
	io挂起</p>
<p>
	快照导入过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将node2，node3上电，重启该节点上的osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd，mon正常，集群状态ok</p>
<p>
	io继续下发，无数据一致性问题</p>
<p>
	快照导入成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool2/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1828" name="server端关机/开机（mon与osd共存）">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[246]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="margin: 0px; padding: 0px; font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="margin: 0px; padding: 0px; font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将node1，node3关机，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关机后，集群不能提供读写服务</p>
<p>
	io挂起</p>
<p>
	快照导出过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将node1，node3开机，重启该节点上的osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd，mon正常，集群状态ok</p>
<p>
	io继续下发，无数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在pool2中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool2/image</p>
<p>
	在导入过程中，将node2，node3关机，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	关机后，集群不能提供读写服务</p>
<p>
	io挂起</p>
<p>
	快照导入过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将node2，node3开机，重启该节点上的osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	启动osd，mon正常，集群状态ok</p>
<p>
	io继续下发，无数据一致性问题</p>
<p>
	快照导入成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool2/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1830" name="server端非NTP同步（mon与osd共存）">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[247]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="margin: 0px; padding: 0px; font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="margin: 0px; padding: 0px; font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，取消node1节点的NTP服务，调整node1节点的时间，比集群时间快5分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的osd被标记为down，leader mon切换</p>
<p>
	io短暂归零后恢复，没有数据一致性问题</p>
<p>
	快照导出正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，启动node1的NTP服务，调整时间，与集群时间相差0.05秒以内，重启node1节点的所有osd、mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon启动正常，集群状态为OK</p>
<p>
	io读写正常，无数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群状态为OK后，在pool2中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool2/image</p>
<p>
	在导入过程中，取消node2节点的NTP服务，调整node2节点的时间，比集群时间慢5分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群有告警，时钟偏移</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	快照导入正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，启动node2的NTP服务，调整时间，与集群时间相差0.05秒以内，重启node2节点的所有osd、mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon启动正常，集群状态为OK</p>
<p>
	io读写正常，无数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	将pool2/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1832" name="PG数量扩展（mon与osd共存）">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[248]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="margin: 0px; padding: 0px; font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="margin: 0px; padding: 0px; font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将pool1，pool2的pg数量扩展为原来的两倍，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pg数量增加，有数据迁移，最后集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待集群状态为OK后，在pool2中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool2/image</p>
<p>
	在导入过程中，将pool1，pool2的pg数量扩展为原来的4倍，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pg数量增加，有数据迁移，最后集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	快照导入成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将pool2/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1834" name="副本数调整（mon与osd共存）">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[249]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="margin: 0px; padding: 0px; font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="margin: 0px; padding: 0px; font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在client端将nbd0的快照s1导出到/diff</p>
<p>
	rbd export-diff pool1/nbd0@s1 /diff</p>
<p>
	在导出过程中，将pool1，pool2的副本数调整到3，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg的重新配对，数据迁移，最后集群状态为OK</p>
<p>
	io读写无问题，没有数据一致性问题</p>
<p>
	快照导出成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待集群状态为OK后，在pool1中创建image，大小任意，将/diff导入到该image中</p>
<p>
	rbd import-diff /diff pool1/image</p>
<p>
	在导入过程中，将pool1，pool2的副本数量调整到2，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	有pg的重新配对，最后集群状态为OK</p>
<p>
	io读写无问题，没有数据一致性问题</p>
<p>
	快照导入成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将pool1/image 用nbd map导出，mount文件系统/image，将nbd0快照回滚，重新挂载，比较/image、/nbd0里面的文件30g.file的一致性，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	文件比较一致</p>
<p>
	其他io读写不受影响，没有数据一致性问题</p>
<p>
	集群状态OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite><testsuite id="1523" name="快照克隆" >
<node_order><![CDATA[3]]></node_order>
<details><![CDATA[]]></details>

<testcase internalid="1683" name="关闭osd进程（mon与osd共存）">
	<node_order><![CDATA[1000]]></node_order>
	<externalid><![CDATA[222]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node1上的三个osd进程关闭</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd flatten过程正常</p>
<p>
	关闭进程后，io短暂归零后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将关闭的osd进程开启，查看osd状态，flatten状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd启动成功，在crush tree上标记为up，集群最后恢复为OK</p>
<p>
	flatten过程正常，最后成功</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1689" name="kill osd进程（mon与osd共存）">
	<node_order><![CDATA[1001]]></node_order>
	<externalid><![CDATA[223]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node2上的三个osd进程kill -9</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd flatten过程正常</p>
<p>
	进程kill -9后，io短暂归零后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将关闭的osd进程开启，查看osd状态，flatten状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd启动成功，在crush tree上标记为up，集群最后恢复为OK</p>
<p>
	flatten过程正常，最后成功</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1691" name="kill 所有进程（mon与osd共存）">
	<node_order><![CDATA[1002]]></node_order>
	<externalid><![CDATA[224]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node1，node2，node3上的所有osd、mon进程kill -9</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写挂起，集群不可用，无法查看状态</p>
<p>
	flatten过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，启动所有node1，node2，node3上的osd、mon进程，观察集群状态，flatten状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动成功，集群状态正常</p>
<p>
	fio读写恢复，没有数据一致性问题</p>
<p>
	flatten过程可以重新继续，最后成功完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1693" name="服务器前端与Public Switch间长时间物理中断（mon与osd共存）">
	<node_order><![CDATA[1003]]></node_order>
	<externalid><![CDATA[225]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node1的public网络长时间中断</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd flatten过程正常</p>
<p>
	node1的public网络中断后，io短暂归零后恢复，没有数据一致性问题</p>
<p>
	node1上的osd全部标记为down，leader mon切换</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node1节点的public网络恢复，查看osd状态，flatten状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1的所有osd在crush tree上标记为up，node1的mon重新变为leader mon。集群最后恢复为OK</p>
<p>
	flatten过程正常，最后成功</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1695" name="服务器后端与Cluster Switch间长时间物理中断（mon与osd共存）">
	<node_order><![CDATA[1004]]></node_order>
	<externalid><![CDATA[226]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node2的cluster网络长时间中断</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd flatten过程正常</p>
<p>
	node2的cluster网络中断后，io短暂归零后恢复，没有数据一致性问题</p>
<p>
	node2上的osd全部标记为down，leader mon不切换</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将node2节点的cluster网络恢复，查看osd状态，flatten状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node2的所有osd在crush tree上标记为up，node2的mon重新加入集群仲裁。集群最后恢复为OK</p>
<p>
	flatten过程正常，最后成功</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1697" name="网络全部物理中断（mon与osd共存）">
	<node_order><![CDATA[1005]]></node_order>
	<externalid><![CDATA[227]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node1，node2，node3的public、cluster网络长时间中断</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用，不能查看状态</p>
<p>
	fio读写挂起</p>
<p>
	flatten过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，恢复node1，node2，node3的public，cluster网络，查看集群状态，io状态，flatten状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	网络恢复后，集群恢复为OK状态</p>
<p>
	io读写继续下发，没有数据一致性问题</p>
<p>
	flatten过程继续，最后成功完成</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1699" name="server端重启（mon与osd共存）">
	<node_order><![CDATA[1006]]></node_order>
	<externalid><![CDATA[228]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node1 reboot重启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd flatten过程正常</p>
<p>
	node1 reboot后，io短暂归零后恢复，没有数据一致性问题</p>
<p>
	node1上的osd全部标记为down，leader mon切换</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node1启动成功后，将osd、mon进程开启，查看osd状态，flatten状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1的所有osd在crush tree上标记为up，node1的mon重新变为leader mon。集群最后恢复为OK</p>
<p>
	flatten过程正常，最后成功</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1701" name="server端panic（mon与osd共存）">
	<node_order><![CDATA[1007]]></node_order>
	<externalid><![CDATA[229]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件，（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node3 panic重启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	rbd flatten过程正常</p>
<p>
	node3 panic后，io短暂归零后恢复，没有数据一致性问题</p>
<p>
	node3上的osd全部标记为down，leader mon不切换</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node3启动成功后，将osd、mon进程开启，查看osd状态，flatten状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node3所有osd在crush tree上标记为up，node3的mon重新加入集群仲裁。集群最后恢复为OK</p>
<p>
	flatten过程正常，最后成功</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1703" name="逻辑踢盘（mon与osd共存）">
	<node_order><![CDATA[1008]]></node_order>
	<externalid><![CDATA[230]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node2节点上逻辑踢出两个SSD，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	踢出SSD的osd被标记为down</p>
<p>
	踢出SSD时，io短暂归零后恢复（30秒内），没有数据一致性问题</p>
<p>
	flatten过程正常，最后成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将踢出的SSD重新加载回来，启动osd进程，观察io状态，集群状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	重新加载成功，启动osd成功，在crush tree上重新标记为up，集群状态最后为OK</p>
<p>
	io读写没有问题，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1891" name="osd脱离、加入集群（mon与osd共存）">
	<node_order><![CDATA[1009]]></node_order>
	<externalid><![CDATA[250]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node2节点的两个osd out出集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	out时，有pg的迁移，最后集群恢复为OK</p>
<p>
	io性能短暂降低，没有数据一致性问题</p>
<p>
	flatten过程正常，最后成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将node2上的osd重新加入集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd加入集群成功，有pg的迁移</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1893" name="删除osd（mon与osd共存）">
	<node_order><![CDATA[1010]]></node_order>
	<externalid><![CDATA[251]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，使用脚本将node1节点一个osd删除，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd删除成功，集群状态为OK</p>
<p>
	io读写正常，无数据一致性问题</p>
<p>
	flatten过程成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1895" name="添加osd（mon与osd共存）">
	<node_order><![CDATA[1011]]></node_order>
	<externalid><![CDATA[252]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，使用脚本在node1节点添加一个osd，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd添加成功，集群状态为OK</p>
<p>
	io读写正常，无数据一致性问题</p>
<p>
	flatten过程成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1897" name="暂停集群（mon与osd共存）">
	<node_order><![CDATA[1012]]></node_order>
	<externalid><![CDATA[253]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，暂停集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群暂停，不能提供读写服务</p>
<p>
	io读写挂起</p>
<p>
	flatten过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，取消暂停集群，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群可以提供读写服务</p>
<p>
	io读写恢复</p>
<p>
	flatten过程成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1899" name="server端掉电（mon与osd共存）">
	<node_order><![CDATA[1013]]></node_order>
	<externalid><![CDATA[254]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node3，node1掉电，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	flatten过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node1，node3上电，重启所有osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon启动成功，集群状态为OK</p>
<p>
	io读写恢复，无一致性问题</p>
<p>
	flatten过程成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1901" name="server端关机/开机（mon与osd共存）">
	<node_order><![CDATA[1014]]></node_order>
	<externalid><![CDATA[255]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将node3，node2关机，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
<p>
	io读写挂起</p>
<p>
	flatten过程挂起</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	node2，node3开机，重启所有osd，mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon启动成功，集群状态为OK</p>
<p>
	io读写恢复，无一致性问题</p>
<p>
	flatten过程成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1903" name="server端非NTP同步（mon与osd共存）">
	<node_order><![CDATA[1015]]></node_order>
	<externalid><![CDATA[256]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，取消node2节点的NTP服务，调整node2节点的时间，比集群时间慢5分钟，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群有告警，时钟偏移</p>
<p>
	io读写正常，没有数据一致性问题</p>
<p>
	flatten过程成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	5分钟后，启动node2的NTP服务，调整时间，与集群时间相差0.05秒以内，重启node2节点的所有osd、mon进程，观察集群状态，io状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	mon、osd启动正常，集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	重复步骤1到步骤5，在步骤2调整时间时，换成调整node1的时间，比集群快5分钟，步骤3重启node1的所有osd，mon</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1905" name="PG数量扩展（mon与osd共存）">
	<node_order><![CDATA[1016]]></node_order>
	<externalid><![CDATA[257]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将pool1，pool2的pg数量扩展到原来的4倍。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	pg扩展成功，集群状态最后为OK</p>
<p>
	flatten成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="1907" name="副本数调整（mon与osd共存）">
	<node_order><![CDATA[1017]]></node_order>
	<externalid><![CDATA[258]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	1，node1，node2，node3一共3个主机节点，这3个节点都为osd，mon节点，每个节点上若干个osd进程，其中node1是leader mon</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	2，集群状态正常，已经创建了rbd pool和rbd image，已经将rbd image在client端通过NBD的方式导出</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	3，副本数为2</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	4，客户端、服务器前端与Public Switch连接正常，服务器后端与Cluster Switch连接正常</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	5，在client端将nbd0（pool1）创建成ext4的文件系统，分别挂载到/nbd0目录下，在/nbd0 目录下写一个30G大小的文件30g.file</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	6，文件写完成后，对nbd0做快照pool1/nbd0@s1</div>
<div style="margin: 0px; padding: 0px; font-family: &quot;Trebuchet MS&quot;, Arial, Verdana, sans-serif; font-size: 12px; background-color: rgb(238, 238, 238);">
	7，对裸设备nbd1（pool1），nbd2（pool2）进行裸设备随机读写，对/nbd0进行随机写文件（<span style="font-size: 13.3333px;">写同名文件，rate=200k，保证COW持续的时间长</span><span style="font-size: 12px;">），带一致性校验</span></div>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将快照pool1/nbd0@s1分别clone到pool1和pool2中：</p>
<p>
	rbd snap protest pool1/nbd0@s1</p>
<p>
	rbd clone pool1/nbd0@s1 pool1/nbd0s1c1</p>
<p>
	rbd clone pool1/nbd0@s1 pool2/nbd0s1c2</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	创建clone成功，在pool1和pool2中可以查看到clone后的image</p>
<p>
	io读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	flatten pool1/nbd0s1c1,pool2/nbd0s1c2:</p>
<p>
	rbd flatten pool1/nbd0s1c1</p>
<p>
	rbd flatten pool2/nbd0s1c2</p>
<p>
	在flatten过程中，将pool1，pool2的副本数调整到3。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	副本数调整成功，集群状态最后为OK</p>
<p>
	flatten成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将pool1/nbd0s1c1, pool2/nbd0s1c2用nbd导出，挂载到文件系统/root/c1 , /root/c2, 比较两个文件系统里面30g.file文件的一致性</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	nbd导出成功，挂载文件系统成功</p>
<p>
	文件比较一致</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	删除nbd0的快照</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	删除快照成功</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite></testsuite><testsuite id="3776" name="Tahoe硬件" >
<node_order><![CDATA[12]]></node_order>
<details><![CDATA[<p>
	针对Tahoe硬件的特点设计tahoe相关的测试用例，涉及硬件相关，或者硬件和软件交互的用例</p>
]]></details>

<testcase internalid="6968" name="BMC-节点reset">
	<node_order><![CDATA[0]]></node_order>
	<externalid><![CDATA[440]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录node1（leader mon）的BMC，将node1节点reset</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io归零30秒以内恢复，没有数据一致性问题</p>
<p>
	node1节点的osd全部标记为down，node1的mon down，集群形成新的仲裁。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node1启动后，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，加入集群</p>
<p>
	io读写正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，登录node2，node3的BMC，将这两个节点同时reset</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	node2，node3启动后，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，加入集群，集群状态正常</p>
<p>
	io恢复读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，将node1，node2，node3同时reset</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	node1，node2，node3启动后，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，集群状态正常</p>
<p>
	io恢复读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6970" name="BMC-节点掉电">
	<node_order><![CDATA[1]]></node_order>
	<externalid><![CDATA[441]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录node1（leader mon）的BMC，将node1节点立即掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io归零30秒以内恢复，没有数据一致性问题</p>
<p>
	node1节点的osd全部标记为down，node1的mon down，集群形成新的仲裁。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node1上电，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，加入集群</p>
<p>
	io读写正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，登录node2，node3的BMC，将这两个节点同时立即掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，加入集群</p>
<p>
	io读写正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	node2，node3上电，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，加入集群，集群状态正常</p>
<p>
	io恢复读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，将node1，node2，node3同时立即掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	node1，node2，node3上电，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，集群状态正常</p>
<p>
	io恢复读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6972" name="BMC-节点正常关机">
	<node_order><![CDATA[2]]></node_order>
	<externalid><![CDATA[442]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录node1（leader mon）的BMC，将node1节点关机</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io归零30秒以内恢复，没有数据一致性问题</p>
<p>
	node1节点的osd全部标记为down，node1的mon down，集群形成新的仲裁。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node1开机，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，加入集群</p>
<p>
	io读写正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，登录node2，node3的BMC，将这两个节点同时关机</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	node2，node3开机，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，加入集群，集群状态正常</p>
<p>
	io恢复读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，将node1，node2，node3同时关机</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	node1，node2，node3开机，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，集群状态正常</p>
<p>
	io恢复读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6974" name="BMC-节点重启">
	<node_order><![CDATA[3]]></node_order>
	<externalid><![CDATA[443]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录node1（leader mon）的BMC，将node1节点硬重启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io归零30秒以内恢复，没有数据一致性问题</p>
<p>
	node1节点的osd全部标记为down，node1的mon down，集群形成新的仲裁。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	node1启动后，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，加入集群</p>
<p>
	io读写正常。</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，登录node2，node3的BMC，将这两个节点同时硬重启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	node2，node3启动后，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，加入集群，集群状态正常</p>
<p>
	io恢复读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复后，将node1，node2，node3同时硬重启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	node1，node2，node3启动后，启动所有osd，mon进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	osd，mon进程启动正常，集群状态正常</p>
<p>
	io恢复读写，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6976" name="tahoe整机掉电">
	<node_order><![CDATA[4]]></node_order>
	<externalid><![CDATA[444]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中将tahoe的电源拔出做整体下电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，将电源插回上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	系统启动正常，osd，mon启动正常，集群恢复正常</p>
<p>
	io继续下发，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6982" name="交换机掉电">
	<node_order><![CDATA[5]]></node_order>
	<externalid><![CDATA[447]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将public交换机掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，将public交换机上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	交换机恢复后，集群状态正常</p>
<p>
	io恢复，没有读写一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群正常后，将cluster交换机掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，将cluster交换机上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	交换机恢复后，集群状态正常</p>
<p>
	io恢复，没有读写一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群正常后，将public，cluster交换机一起掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，将public，cluster交换机上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	交换机恢复后，集群状态正常</p>
<p>
	io恢复，没有读写一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6978" name="机柜掉电（tahoe + switch）">
	<node_order><![CDATA[6]]></node_order>
	<externalid><![CDATA[445]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe和cluster交换机整体掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io挂起，集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，将tahoe和cluster交换机一起上电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	上电完成后，集群状态恢复正常</p>
<p>
	io恢复，没有读写一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="6980" name="client端掉电（client + switch）">
	<node_order><![CDATA[7]]></node_order>
	<externalid><![CDATA[446]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将client和public交换机一起掉电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群不可用</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	3分钟后，将client和public交换机上电，client端重新下发io</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	集群状态恢复正常</p>
<p>
	client端下发io正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7041" name="单节点SSD拔插（graceful）">
	<node_order><![CDATA[8]]></node_order>
	<externalid><![CDATA[448]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录UI界面，在node1上选择一个SSD进行下电，并拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	UI下电操作成功，拔出后，该磁盘的前面板灯熄灭，硬盘区域没有红灯</p>
<p>
	该SSD上的osd进程关闭，集群将该osd标记为down</p>
<p>
	io归零30秒以内然后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	待io恢复后，登录UI界面，在node1上，再随机选择3个SSD，一起下电，然后拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	UI下电操作成功，拔出后，该磁盘的前面板的灯熄灭，硬盘区域没有红灯</p>
<p>
	SSD上的osd进程关闭，集群将osd标记为down</p>
<p>
	io归零30秒以内然后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待io恢复后，登录UI界面，在node1上，将剩余所有的SSD，全部下电，并拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	UI下电操作成功，拔出后，该磁盘的前面板的灯熄灭，硬盘区域没有红灯</p>
<p>
	SSD上的osd进程关闭，集群将osd标记为down</p>
<p>
	io归零30秒以内然后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	待io恢复后，登录UI界面，在node1上，将所有SSD插入进行上电，UI上点击重新扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	插入磁盘后，前面板对应槽位的灯点亮为绿色，硬盘区域没有红灯</p>
<p>
	osd进程自动启动，加入集群，有数据迁移和恢复，最终集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在node2，node3上分别重复步骤1到4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	在集群A,B,C节点都正常的情况下，停止io读写，重复步骤1到5</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7043" name="多节点SSD拔插（graceful）">
	<node_order><![CDATA[9]]></node_order>
	<externalid><![CDATA[449]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，登录UI界面，在node1上随机下电3个SSD，然后在node2上随机下电3个SSD。将下电的SSD拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出的磁盘，前面板灯熄灭，硬盘区域没有红灯</p>
<p>
	io有可能挂起，pg有卡住</p>
<p>
	集群有pg故障</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将node1，node2节点下电的SSD全部插入上电，UI上点击重新扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	插入的磁盘，前面板灯点亮为绿色，硬盘区域没有红灯</p>
<p>
	osd进程自动启动</p>
<p>
	集群状态恢复正常</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	io恢复后，在node1，node2，node3上各随机下电2个SSD，并将下电的SSD拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出的磁盘，前面板灯熄灭，硬盘区域没有红灯</p>
<p>
	io有可能挂起，pg有卡住</p>
<p>
	集群有pg故障</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将node1，node2，node3节点下电的SSD全部插入上电，UI上点击重新扫描SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	插入的磁盘，前面板灯点亮为绿色，硬盘区域没有红灯</p>
<p>
	osd进程自动启动</p>
<p>
	集群状态恢复正常</p>
<p>
	io读写恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在集群A,B,C节点多正常的情况下，停止io读写，重复步骤1到4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7074" name="新SSD拔插（graceful）">
	<node_order><![CDATA[10]]></node_order>
	<externalid><![CDATA[454]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在tahoe A节点的空槽位上插入一个新的SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写不受影响，os正常</p>
<p>
	插入新的SSD后，面板前方的槽位灯常亮绿色，硬盘区域没有红灯</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在UI界面上点击重新扫描SSD，在os上查看lsblk，lspci磁盘状态</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	点击重新扫描SSD后，io读写不受影响</p>
<p>
	在os上lspci可以看该磁盘的控制器，lsblk可以看到磁盘，磁盘状态正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	UI界面上对这个新的SSD进行remove操作</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	os上看不到该盘的磁盘控制器</p>
<p>
	lsblk看不到该盘</p>
<p>
	io读写不受影响</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	将这个新的SSD拔出，在A节点，B节点，C节点的空槽位插入，重复步骤1到3，重复15次。</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在集群A,B,C节点都正常的情况下，停止io读写，重复步骤1到4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7061" name="系统盘满盘">
	<node_order><![CDATA[11]]></node_order>
	<externalid><![CDATA[452]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在node1节点上，创建目录/root/big，使用fio在该目录写一个超大的文件，使得/目录下的总容量使用达到80%。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	写超大文件成功，node1节点正常</p>
<p>
	fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	加大io读写压力，持续运行tahoe 30分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	系统正常，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	不将超大文件删除，后续测试保持这个文件，在/目录使用80%的情况下做其他的故障注入测试</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7200" name="Public网络换端口">
	<node_order><![CDATA[12]]></node_order>
	<externalid><![CDATA[455]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在io读写过程中，登录node1节点，将public 网络端口eth4 用命令ifdown断开，删除eth4的ip配置</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的所有osd标记为down，mon 退出集群仲裁</p>
<p>
	fio归零30秒以内然后恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	随机挑选另外的public网络端口，例如eth3，配置eth3为public网络，ip地址使用原来eth4的ip地址，配置完ip并激活端口，将public网络的线缆插入到eth3端口</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	配置ip成功</p>
<p>
	插入线缆后，node1上的所有osd，mon加入集群，集群恢复正常</p>
<p>
	fio读写正常没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复正常后，在node2，node3上重复步骤1到2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7202" name="Cluster网络换端口">
	<node_order><![CDATA[13]]></node_order>
	<externalid><![CDATA[456]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在io读写过程中，登录node1节点，将cluster 网络端口eth0 用命令ifdown断开，删除eth0的ip配置</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的所有osd标记为down，mon正常</p>
<p>
	fio归零30秒以内然后恢复</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	挑选另外的cluster网络端口eth1，配置eth1为cluster网络，ip地址使用原来eth0的ip地址，配置完ip并激活端口，将cluster网络的线缆插入到eth1端口</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复正常后，在node2，node3上重复步骤1到2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7206" name="高负载">
	<node_order><![CDATA[15]]></node_order>
	<externalid><![CDATA[458]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，在每个tahoe节点上配置BMC，可以在另外的主机上串口登录，并将结果保存。</p>
<p>
	3，配置coredump，文件保存在/data/coredump</p>
<p>
	4，测试前手动将所有的osd，mon stop，用命令ceph-osd 或者ceph-mon手动再启动一遍，避免进程在down掉之后自动重启</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	配置3个client，每个client端nbd map 4个不同的image，两个image创建文件系统，两个为裸盘，启动fio对这个12个image进行读写（server端CPU空闲20%），运行一晚上（每天晚上运行）</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	os无异常，集群状态正常，没有osd ,mon down</p>
<p>
	fio读写无一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7208" name="USB接口稳定性">
	<node_order><![CDATA[16]]></node_order>
	<externalid><![CDATA[459]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，使用键盘，鼠标，U盘（正规厂家，里面拷贝有数据），在node1节点的两个usb上随机插入和拔出，共50次，查看节点是否有异常</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	插入键盘，鼠标，U盘，再拔出，不会导致节点异常</p>
<p>
	fio读写正常，没有一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在node2，node3上重复步骤1</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7210" name="串口稳定性">
	<node_order><![CDATA[17]]></node_order>
	<externalid><![CDATA[460]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在node1上通过串口接入系统，然后拔出串口。重复15次。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	串口接入正常，os正常</p>
<p>
	fio读写没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在node2，node3上重复步骤1</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7212" name="VGA接口稳定性">
	<node_order><![CDATA[18]]></node_order>
	<externalid><![CDATA[461]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将VGA显示器接入node1节点，显示成功后，拔出，重复15次</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	接入成功，os无问题</p>
<p>
	fio读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在node2，node3上重复步骤1</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7214" name="面板硬盘状态灯">
	<node_order><![CDATA[19]]></node_order>
	<externalid><![CDATA[462]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	检查前面板的SSD状态灯是否正常</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	状态灯正常，在有io的时候，闪烁为绿色</p>
<p>
	亮灯的个数与SSD的个数匹配</p>
<p>
	亮灯的位置和SSD的槽位匹配</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在UI界面上随机对node1的三个SSD硬盘进行下电</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	三个SSD的状态灯绿色，常亮不闪烁</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将这三个SSD拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出后，对应的槽位的灯熄灭</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，插回三个SSD，UI界面进行上电，启动osd进程</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	插回SSD后，状态灯变为常亮绿色</p>
<p>
	启动osd恢复读写后，该SSD的灯为绿色闪烁</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	待集群状态正常后，在node2，node3上分别重复步骤2到4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8805" name="单节点SSD拔插（surprise）">
	<node_order><![CDATA[20]]></node_order>
	<externalid><![CDATA[572]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在有io的情况下，在tahoe的A节点上一次随机拔出一个或者多个SSD盘，直到A节点的所有SSD磁盘被拔出。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘区域没有亮红灯</p>
<p>
	拔出的磁盘的前面板灯熄灭，没有拔出的磁盘的前面板灯绿色</p>
<p>
	（可能会出现A节点os重启，或者死机，软件方面不支持surprise拔出，只关注硬件是否正常）</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将tahoe的A节点上一次随机插入一个或者多个SSD盘，直到A节点的所有SSD盘插回</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	插回时，硬盘区域没有亮红灯</p>
<p>
	插入磁盘后，该槽位的面板灯亮绿色</p>
<p>
	（可能会出现A节点os重启，或者死机，软件方面不支持surprise拔出，只关注硬件是否正常）</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将A节点重启</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	重启成功，A节点的所有osd，mon自动加入集群</p>
<p>
	集群状态为OK，io读写正常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在B节点，C节点上分别重复步骤1到3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在集群A,B,C节点都正常的情况下，停止io读写，集群没有任何io的情况下，重复步骤1到4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8824" name="多节点SSD拔插（surprise）">
	<node_order><![CDATA[21]]></node_order>
	<externalid><![CDATA[573]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在有io读写的过程中，在tahoe A，B，C节点上随机一次拔出一个或者多个SSD，直到tahoe上所有的SSD都被拔出一遍</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘区域没有亮红灯</p>
<p>
	拔出的磁盘的前面板灯熄灭，没有拔出的磁盘的前面板灯绿色</p>
<p>
	（可能会出现被拔节点os重启，或者死机，软件方面不支持surprise拔出，只关注硬件是否正常）</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将tahoe A，B，C节点上的SSD插回，一次插入一个或者多个SSD，插回的槽位为原来的槽位，直到所有的SSD都插回，也可以拔出后就插回。</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	硬盘区域没有亮红灯</p>
<p>
	拔出的磁盘的前面板灯熄灭，插回的磁盘的前面板灯绿色</p>
<p>
	（可能会出现插入SSD节点os重启，或者死机，软件方面不支持surprise插入，只关注硬件是否正常）</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	将A，B，C节点都重启，待集群A,B,C节点恢复为OK后，停止io读写，重复步骤1到2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8834" name="新SSD拔插（surprise）">
	<node_order><![CDATA[22]]></node_order>
	<externalid><![CDATA[574]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，在A节点的空槽位上插入一个新的SSD</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	os正常，io读写正常</p>
<p>
	硬盘区域没有亮红灯</p>
<p>
	插入后，该槽位的前面板灯亮绿色</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	将A节点的新SSD直接拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	os正常，io读写正常</p>
<p>
	硬盘区域没有亮红灯</p>
<p>
	拔出后，该槽位的前面板灯熄灭</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在A，B，C节点的其他空槽位上重复步骤1,2，重复15次</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在集群A,B,C节点都正常的情况下，停止io读写，重复步骤1到3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="7941" name="1U模块更换">
	<node_order><![CDATA[23]]></node_order>
	<externalid><![CDATA[537]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将tahoe A节点在BMC上poweroff oderly，poweroff之后，将A节点从后端抽出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	A节点poweroff正常</p>
<p>
	A节点的的osd标记为down，mon退出集群仲裁</p>
<p>
	fio短暂归零（30秒以内）恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将A节点从后端插回，在BMC上重新power on</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	A节点上电成功</p>
<p>
	A节点的osd，mon启动成功，并且自动加入集群</p>
<p>
	fio读写性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
<p>
	集群有数据迁移，最后恢复为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	待集群恢复OK后，在读写过程中，将tahoe A节点在BMC上poweroff immediately，poweroff之后，将A节点从后端抽出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	A节点poweroff正常</p>
<p>
	A节点的osd标记为down，mon退出集群仲裁</p>
<p>
	fio短暂归零（30秒以内）恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，将A节点从后端插回，在BMC上重新power on</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	A节点上电成功</p>
<p>
	A节点的osd，mon启动成功，并且自动加入集群</p>
<p>
	fio读写性能短暂降低后恢复到较高水平，没有数据一致性问题</p>
<p>
	集群有数据迁移，最后恢复为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在tahoe B，C节点上重复步骤1到4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8852" name="1U模块拔插">
	<node_order><![CDATA[24]]></node_order>
	<externalid><![CDATA[575]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，手动将A节点的服务模块抽出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	A节点的osd标记为down，mon退出集群仲裁</p>
<p>
	io读写短暂归零（30秒内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	1分钟后，插回A节点</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	A节点系统重启，没有出现磁盘故障</p>
<p>
	osd，mon进程启动成功，加入集群成功，集群状态恢复为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在节点B，C上重复步骤1,2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="8855" name="1U模块reset">
	<node_order><![CDATA[25]]></node_order>
	<externalid><![CDATA[576]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半）</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，点击A节点后端的reset按钮进行硬件reset复位</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	A节点开始重启，该节点的所有osd标记为down，mon退出集群仲裁</p>
<p>
	io短暂归零（30秒以内）后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	A节点恢复后，观察集群</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	A节点系统恢复后，磁盘状态正常</p>
<p>
	osd，mon进程自动启动，加入集群成功，集群状态为OK</p>
<p>
	io读写正常，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在B，C节点上重复步骤1,2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9331" name="public网络bond">
	<node_order><![CDATA[26]]></node_order>
	<externalid><![CDATA[580]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半），在public网络方面，每个节点eth2，eth3，eth4，eth5作为bond配置public网络，并且4个口都接入public网络交换机。在cluster网络方面，每个节点的eth0，eth1作为bond配置cluster网络，并且2个口都接入cluster网络交换机</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	启动fio读写，带一致性校验，运行10分钟</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io性能稳定，没有数据一致性问题</p>
<p>
	public网络中4个端口的网络流量平均且稳定</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1上的eth2线缆拔出，2分钟后，再依次拔出eth3，eth4的线缆，最后只留eth5的线缆连接</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	拔出eth2，eth3，eth4的线缆后，io不归零，所有到node1的读写io全部都由eth5端口下发，没有数据一致性问题</p>
<p>
	集群状态为OK，没有osd，mon被报down，没有心跳异常</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1上的eth2线缆插回，2分钟后，再依次插回eth3，eth4的线缆，最后node1上public网络有4条线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	插回eth2，eth3，eth4的线缆后，io不归零，所有到node1上的读写io全部均匀有eth2，eth3，eth4，eth5端口下发，每条线缆负载均衡且稳定，没有数据一致性问题</p>
<p>
	集群状态为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	在node2，node3上重复步骤2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	使用ifconfig down，ifconfig up的方式对端口关闭和开启，重复步骤2,3,4</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	将node1上的所有public网络线缆全部拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io归零30秒以内恢复，没有数据一致性问题</p>
<p>
	node1上的osd全部标记为down，node1的mon退出集群仲裁</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将node1上的所有public网络线缆全部插回</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io性能短暂下降后恢复到较高水平，没有数据一致性问题</p>
<p>
	node1上的osd全部标记为up，node1的mon加入集群仲裁，集群状态最后为OK</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	在node2，node3上重复步骤6,7</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[9]]></step_number>
	<actions><![CDATA[<p>
	用ifconfig down，ifconfig up的方式对端口关闭和开启，重复步骤6,7,8</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>

<testcase internalid="9337" name="cluster网络bond">
	<node_order><![CDATA[27]]></node_order>
	<externalid><![CDATA[581]]></externalid>
	<version><![CDATA[1]]></version>
	<summary><![CDATA[<p>
	<img alt="" src="/testlink/third_party/kcfinder/upload_area/images/rbd-nbd%E7%BB%84%E7%BD%91%E5%9B%BE.PNG" style="width: 431px; height: 274px;" /></p>
]]></summary>
	<preconditions><![CDATA[<p>
	1，使用tahoe机器创建集群，三个节点都是osd，mon节点，每个节点上若干个osd（满配或插入一半），在public网络方面，每个节点eth2，eth3，eth4，eth5作为bond配置public网络，并且4个口都接入public网络交换机。在cluster网络方面，每个节点的eth0，eth1作为bond配置cluster网络，并且2个口都接入cluster网络交换机</p>
<p>
	2，创建两个pool，每个pool双副本，每个pool中创建2个image，并用nbd map导出到client端</p>
<p>
	3，在client端对nbd创建文件系统ext4，挂载目录，启动fio对文件系统和裸盘进行读写（有读有写），带一致性校验</p>
]]></preconditions>
	<execution_type><![CDATA[1]]></execution_type>
	<importance><![CDATA[3]]></importance>
	<estimated_exec_duration></estimated_exec_duration>
	<status>7</status>
	<is_open>1</is_open>
	<active>1</active>
<steps>
<step>
	<step_number><![CDATA[1]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1上的eth0的线缆拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，性能稳定，没有数据一致性问题</p>
<p>
	node1后端cluster网络的负载全部切换到eth1的线缆中下发</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[2]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将eth0的线缆插回，node1节点的cluster网络有两条线缆</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	io读写正常，性能稳定，没有数据一致性问题</p>
<p>
	node1后端cluster网络的负载在eth0，eth1上均匀分布且稳定</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[3]]></step_number>
	<actions><![CDATA[<p>
	在node2，node3上重复步骤1，2</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[4]]></step_number>
	<actions><![CDATA[<p>
	使用ifconfig down，ifconfig up的方式关闭开启端口，重复步骤1,2,3</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[5]]></step_number>
	<actions><![CDATA[<p>
	在读写过程中，将node1上的所有cluster网络线缆拔出</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的所有osd标记为down，node1上的mon不受影响</p>
<p>
	io读写30秒内归零后恢复，没有数据一致性问题</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[6]]></step_number>
	<actions><![CDATA[<p>
	2分钟后，将node1上的所有cluster网络线缆插回</p>
]]></actions>
	<expectedresults><![CDATA[<p>
	node1节点的所有osd标记为up，最后集群为OK</p>
<p>
	io读写性能短暂下降后恢复，没有数据一致性问题</p>
<p>
	node1后端的cluster网络的负载均衡且稳定</p>
]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[7]]></step_number>
	<actions><![CDATA[<p>
	在node2，node3上重复步骤5,6</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>

<step>
	<step_number><![CDATA[8]]></step_number>
	<actions><![CDATA[<p>
	使用ifconfig down，ifconfig up的方式关闭开启端口，重复步骤5,6,7</p>
]]></actions>
	<expectedresults><![CDATA[]]></expectedresults>
	<execution_type><![CDATA[1]]></execution_type>
</step>
</steps>
</testcase>
</testsuite></testsuite>
</data>
